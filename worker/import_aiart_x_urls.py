#!/usr/bin/env python3
"""
ä» aiart_pics_x_urls.json å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“

å·¥ä½œæµç¨‹:
1. è¯»å– cache/aiart_pics_x_urls.json (å·²çˆ¬å–çš„ x_url æ•°æ®)
2. ç­›é€‰æœ‰ x_url çš„è®°å½•
3. ä» Twitter è·å–å›¾ç‰‡å’Œå†…å®¹
4. ä½¿ç”¨ AI åˆ†ç±»
5. å†™å…¥æ•°æ®åº“

ç”¨æ³•:
  python import_aiart_x_urls.py                    # å¯¼å…¥æ‰€æœ‰æœ‰ x_url çš„è®°å½•
  python import_aiart_x_urls.py --limit 10         # é™åˆ¶æ•°é‡
  python import_aiart_x_urls.py --dry-run          # é¢„è§ˆæ¨¡å¼
  python import_aiart_x_urls.py --reset            # é‡ç½®è¿›åº¦
"""

import os
import sys
import json
import argparse
from typing import Optional, Dict, Any
from datetime import datetime, timezone
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv

    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"âœ“ å·²åŠ è½½: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"âœ“ å·²åŠ è½½: {env_file}")
except ImportError:
    pass

# å¯¼å…¥ä¸»æ¨¡å—
from main import Database, AI_MODEL
from prompt_utils import process_tweet_for_import

# ========== é…ç½® ==========
DATABASE_URL = os.environ.get("DATABASE_URL", "")

# æ•°æ®æ–‡ä»¶
CACHE_DIR = Path(__file__).parent / "cache"
X_URLS_FILE = CACHE_DIR / "aiart_pics_x_urls.json"
PROGRESS_FILE = CACHE_DIR / "aiart_x_urls_import_progress.json"

# å¤±è´¥è®°å½•
FAILED_OUTPUT_DIR = Path(__file__).parent / "failed_imports"


def load_x_urls_data() -> Optional[Dict]:
    """åŠ è½½ x_urls æ•°æ®"""
    if not X_URLS_FILE.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {X_URLS_FILE}")
        return None

    with open(X_URLS_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    print(f"ğŸ“¦ å·²åŠ è½½: {X_URLS_FILE}")
    print(f"   æ€»è®°å½•: {data.get('total', 0)}")
    print(f"   æœ‰ x_url: {data.get('with_x_url', 0)}")

    return data


def load_progress() -> Dict:
    """åŠ è½½å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {"processed_slugs": [], "last_updated": None}


def save_progress(progress: Dict):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    progress["last_updated"] = datetime.now(timezone.utc).isoformat()

    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")


def clear_progress():
    """æ¸…é™¤å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        PROGRESS_FILE.unlink()
        print("ğŸ—‘ï¸ å·²æ¸…é™¤å¤„ç†è¿›åº¦")


def save_failed_items(failed_items: list, timestamp: str) -> Optional[Path]:
    """ä¿å­˜å¤±è´¥è®°å½•"""
    if not failed_items:
        return None

    FAILED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    filepath = FAILED_OUTPUT_DIR / f"aiart_x_urls_failed_{timestamp}.json"

    output = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "total": len(failed_items),
        "items": failed_items
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    return filepath


def process_item(db: Database, item: Dict, dry_run: bool = False) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ¡ç›®

    æµç¨‹: ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•° process_tweet_for_import
    """
    x_url = item.get("x_url", "")

    if not x_url:
        return {"success": False, "method": "skipped", "error": "No x_url", "twitter_failed": False}

    # ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    result = process_tweet_for_import(
        db=db,
        tweet_url=x_url,
        import_source="aiart_pics",
        ai_model=AI_MODEL,
        dry_run=dry_run
    )

    return result


def run_import(limit: int = None, dry_run: bool = False,
               resume: bool = True, reset_progress: bool = False):
    """è¿è¡Œå¯¼å…¥æµç¨‹"""
    print("=" * 70)
    print("ğŸ“¦ AIART.PICS X URLs å¯¼å…¥")
    print("=" * 70)
    print(f"æ•°æ®æº: {X_URLS_FILE}")
    print(f"é¢„è§ˆæ¨¡å¼: {dry_run}")
    print(f"æ–­ç‚¹ç»­ä¼ : {resume}")
    if limit:
        print(f"é™åˆ¶æ•°é‡: {limit}")
    print("=" * 70)

    # é‡ç½®è¿›åº¦
    if reset_progress:
        clear_progress()

    # æ£€æŸ¥é…ç½®
    if not DATABASE_URL:
        print("âŒ ç¼ºå°‘ DATABASE_URL ç¯å¢ƒå˜é‡")
        sys.exit(1)

    # åŠ è½½æ•°æ®
    data = load_x_urls_data()
    if not data:
        sys.exit(1)

    items = data.get("items", [])

    # åªå¤„ç†æœ‰ x_url çš„
    items = [item for item in items if item.get("x_url")]
    print(f"ğŸ“Š æœ‰ x_url çš„è®°å½•: {len(items)}")

    # åŠ è½½è¿›åº¦
    progress = load_progress()
    processed_slugs = set(progress.get("processed_slugs", []))

    if resume and processed_slugs:
        original_count = len(items)
        items = [item for item in items if item.get("slug") not in processed_slugs]
        skipped = original_count - len(items)
        if skipped > 0:
            print(f"ğŸ“Š å·²å¤„ç†ï¼ˆè·³è¿‡ï¼‰: {skipped}")
            print(f"   ä¸Šæ¬¡æ›´æ–°: {progress.get('last_updated', 'N/A')}")

    # é™åˆ¶æ•°é‡
    if limit:
        items = items[:limit]

    total = len(items)
    print(f"\nğŸ”„ å‡†å¤‡å¤„ç† {total} æ¡è®°å½•...\n")

    if total == 0:
        print("âœ… æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•")
        return

    # è¿æ¥æ•°æ®åº“
    db = Database(DATABASE_URL)

    try:
        db.connect()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        sys.exit(1)

    # ç»Ÿè®¡
    stats = {
        "total": total,
        "success": 0,
        "skipped": 0,
        "failed": 0,
        "twitter_failed": 0,
    }

    failed_items = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    try:
        for i, item in enumerate(items, 1):
            slug = item.get("slug", "?")
            x_url = item.get("x_url", "")

            progress_pct = (i / total) * 100
            print(f"[{i}/{total}] ({progress_pct:.1f}%) {slug[:50]}")
            if x_url:
                print(f"   ğŸ”— {x_url}")

            result = process_item(db, item, dry_run=dry_run)

            if result.get("twitter_failed"):
                stats["twitter_failed"] += 1
                failed_items.append({
                    "slug": slug,
                    "x_url": x_url,
                    "error": result.get("error", "Unknown")
                })

            if result["success"]:
                stats["success"] += 1
                print(f"   âœ… æˆåŠŸå…¥åº“")
            else:
                if result["method"] == "skipped":
                    stats["skipped"] += 1
                    print(f"   â­ï¸ è·³è¿‡: {result['error']}")
                elif result["method"] == "twitter_failed":
                    print(f"   âŒ Twitter å¤±è´¥: {result['error']}")
                else:
                    stats["failed"] += 1
                    print(f"   âŒ å¤±è´¥: {result['error']}")

            # ä¿å­˜è¿›åº¦
            if not dry_run:
                processed_slugs.add(slug)
                if i % 10 == 0 or i == total:
                    save_progress({"processed_slugs": list(processed_slugs)})

            print()

        # ä¿å­˜å¤±è´¥è®°å½•
        failed_file = None
        if failed_items and not dry_run:
            failed_file = save_failed_items(failed_items, timestamp)

        # è¾“å‡ºç»Ÿè®¡
        print("=" * 70)
        print("ğŸ“Š å¯¼å…¥å®Œæˆ - ç»Ÿè®¡æ±‡æ€»")
        print("=" * 70)
        print(f"æ€»è®¡: {stats['total']}")
        print(f"âœ… æˆåŠŸ: {stats['success']}")
        print(f"â­ï¸ è·³è¿‡: {stats['skipped']}")
        print(f"âŒ å¤±è´¥: {stats['failed']}")
        print(f"âš ï¸ Twitter å¤±è´¥: {stats['twitter_failed']}")

        if failed_file:
            print(f"\nğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜: {failed_file}")

        print("=" * 70)

    finally:
        db.close()


def main():
    parser = argparse.ArgumentParser(
        description="ä» aiart_pics_x_urls.json å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å…¥æ‰€æœ‰æœ‰ x_url çš„è®°å½•
  python import_aiart_x_urls.py

  # é™åˆ¶å¯¼å…¥æ•°é‡
  python import_aiart_x_urls.py --limit 10

  # é¢„è§ˆæ¨¡å¼
  python import_aiart_x_urls.py --dry-run --limit 5

  # é‡ç½®è¿›åº¦
  python import_aiart_x_urls.py --reset
        """
    )

    parser.add_argument("--limit", "-l", type=int, help="é™åˆ¶å¯¼å…¥æ•°é‡")
    parser.add_argument("--dry-run", "-d", action="store_true", help="é¢„è§ˆæ¨¡å¼")
    parser.add_argument("--no-resume", action="store_true", help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®è¿›åº¦")

    args = parser.parse_args()

    run_import(
        limit=args.limit,
        dry_run=args.dry_run,
        resume=not args.no_resume,
        reset_progress=args.reset
    )


if __name__ == "__main__":
    main()
