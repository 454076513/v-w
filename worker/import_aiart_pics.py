#!/usr/bin/env python3
"""
AIART.PICS æç¤ºè¯å¯¼å…¥è„šæœ¬

é€šè¿‡ aiart.pics API è·å–æ•°æ®å¹¶å¯¼å…¥åˆ°æ•°æ®åº“ã€‚

å·¥ä½œæµç¨‹:
1. é€šè¿‡ API (/api/prompts) è·å–æç¤ºè¯åˆ—è¡¨
2. ä½¿ç”¨ API è¿”å›çš„æ•°æ®ï¼ˆåŒ…å«æç¤ºè¯ã€å›¾ç‰‡ã€ä½œè€…ã€æ ‡ç­¾ï¼‰
3. å¦‚éœ€è¦åˆ™è¿›è¡Œ AI åˆ†ç±»
4. å†™å…¥æ•°æ®åº“

ç¯å¢ƒå˜é‡:
  DATABASE_URL - PostgreSQL è¿æ¥å­—ç¬¦ä¸² (å¿…éœ€)
  AI_MODEL     - AI æ¨¡å‹ (é»˜è®¤: openai)

ç”¨æ³•:
  python import_aiart_pics.py                    # å¯¼å…¥æ•°æ®
  python import_aiart_pics.py --limit 10         # é™åˆ¶å¯¼å…¥æ•°é‡
  python import_aiart_pics.py --dry-run          # é¢„è§ˆæ¨¡å¼
  python import_aiart_pics.py --pages 5          # åªè·å–å‰ 5 é¡µ
  python import_aiart_pics.py --reset            # é‡ç½®è¿›åº¦
"""

import os
import sys
import json
import asyncio
import argparse
import requests
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv

    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"âœ“ å·²åŠ è½½: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"âœ“ å·²åŠ è½½: {env_file}")
except ImportError:
    pass

# å¯¼å…¥ä¸»æ¨¡å—
from main import Database, AI_MODEL
from prompt_utils import process_tweet_for_import

# ========== é…ç½® ==========
BASE_URL = "https://aiart.pics"
DATABASE_URL = os.environ.get("DATABASE_URL", "")

# æ•°æ®æ–‡ä»¶
CACHE_DIR = Path(__file__).parent / "cache"
PROGRESS_FILE = CACHE_DIR / "aiart_pics_import_progress.json"

# å¤±è´¥è®°å½•
FAILED_OUTPUT_DIR = Path(__file__).parent / "failed_imports"


def fetch_prompts_from_api(limit: int = 50, offset: int = 0) -> List[Dict]:
    """é€šè¿‡ API è·å–æç¤ºè¯åˆ—è¡¨"""
    url = f"{BASE_URL}/api/prompts?limit={limit}&offset={offset}"

    response = requests.get(url, timeout=30)
    if response.status_code != 200:
        return []
    return response.json().get("prompts", [])


def extract_data_from_api_item(item: Dict) -> Optional[Dict]:
    """ä» API è¿”å›çš„ item ä¸­æå–éœ€è¦çš„æ•°æ®"""
    origin_url = item.get("originUrl", "")
    if not origin_url:
        return None

    # åˆå¹¶ prompts æ•°ç»„ä¸ºå•ä¸ªå­—ç¬¦ä¸²
    prompts = item.get("prompts", [])
    prompt_text = "\n".join(prompts) if prompts else ""

    # æå–æ ‡é¢˜ (ä¼˜å…ˆè‹±æ–‡)
    title_obj = item.get("title", {})
    title = title_obj.get("en") or title_obj.get("zh") or ""

    # æå–å›¾ç‰‡ URL
    images = []
    img_base = "https://img1.aiart.pics/"
    for img in item.get("images", []):
        path = img.get("path", "")
        if path:
            images.append(f"{img_base}{path}")

    # æå–ä½œè€…
    author_obj = item.get("author", {})
    author = author_obj.get("username") or author_obj.get("name") or ""

    # æå–æ ‡ç­¾
    tags = item.get("tags", [])

    return {
        "x_url": origin_url.replace("twitter.com", "x.com"),
        "prompt": prompt_text,
        "title": title,
        "images": images,
        "author": author,
        "tags": tags,
        "id": item.get("id", ""),
    }


def load_progress() -> Dict:
    """åŠ è½½å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {"processed_slugs": [], "last_updated": None}


def save_progress(progress: Dict):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    progress["last_updated"] = datetime.now(timezone.utc).isoformat()

    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")


def clear_progress():
    """æ¸…é™¤å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        PROGRESS_FILE.unlink()
        print("ğŸ—‘ï¸ å·²æ¸…é™¤å¤„ç†è¿›åº¦")


def save_failed_items(failed_items: List[Dict], timestamp: str) -> Optional[Path]:
    """ä¿å­˜å¤±è´¥è®°å½•"""
    if not failed_items:
        return None

    FAILED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    filepath = FAILED_OUTPUT_DIR / f"aiart_pics_failed_{timestamp}.json"

    output = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "total": len(failed_items),
        "items": failed_items
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    return filepath


def process_api_item(db: Database, api_data: Dict, dry_run: bool = False) -> Dict[str, Any]:
    """
    å¤„ç† API è¿”å›çš„å•ä¸ªæ¡ç›®

    ç­–ç•¥:
    - ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•° process_tweet_for_import
    - å›¾ç‰‡: å¿…é¡»ä» Twitter è·å–é«˜æ¸…å›¾

    è¿”å›: {"success": bool, "method": str, "error": str or None, "twitter_failed": bool}
    """
    x_url = api_data.get("x_url", "")
    raw_prompt = api_data.get("prompt", "")
    api_author = api_data.get("author", "")

    if not x_url:
        return {"success": False, "method": "skipped", "error": "No x_url", "twitter_failed": False}

    if not raw_prompt:
        return {"success": False, "method": "skipped", "error": "No prompt", "twitter_failed": False}

    # ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    result = process_tweet_for_import(
        db=db,
        tweet_url=x_url,
        raw_text=raw_prompt,
        author=api_author or None,
        import_source="aiart_pics",
        ai_model=AI_MODEL,
        dry_run=dry_run
    )

    return result


async def run_import_async(limit: int = None, max_pages: int = None, dry_run: bool = False,
                           resume: bool = True, reset_progress: bool = False):
    """å¼‚æ­¥å¯¼å…¥æµç¨‹ - é€šè¿‡ API è·å–æ•°æ®"""
    print("=" * 70)
    print("ğŸ“¦ AIART.PICS å¯¼å…¥ (API + Twitter)")
    print("=" * 70)
    print(f"æ•°æ®æº: {BASE_URL}/api/prompts")
    print(f"é¢„è§ˆæ¨¡å¼: {dry_run}")
    print(f"æ–­ç‚¹ç»­ä¼ : {resume}")
    if limit:
        print(f"é™åˆ¶æ•°é‡: {limit}")
    if max_pages:
        print(f"æœ€å¤§é¡µæ•°: {max_pages}")
    print("=" * 70)

    # é‡ç½®è¿›åº¦
    if reset_progress:
        clear_progress()

    # æ£€æŸ¥é…ç½®
    if not DATABASE_URL:
        print("âŒ ç¼ºå°‘ DATABASE_URL ç¯å¢ƒå˜é‡")
        sys.exit(1)

    # åŠ è½½è¿›åº¦
    progress = load_progress()
    processed_ids = set(progress.get("processed_slugs", []))  # ç°åœ¨å­˜å‚¨ ID è€Œé slug
    if resume and processed_ids:
        print(f"ğŸ“Š å·²å¤„ç†: {len(processed_ids)} æ¡")
        print(f"   ä¸Šæ¬¡æ›´æ–°: {progress.get('last_updated', 'N/A')}")

    # è¿æ¥æ•°æ®åº“
    db = Database(DATABASE_URL)
    try:
        db.connect()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        sys.exit(1)

    # ç»Ÿè®¡
    stats = {
        "pages": 0,
        "items_found": 0,
        "success": 0,
        "skipped": 0,
        "failed": 0,
        "twitter_failed": 0,
    }

    failed_items = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    processed_count = 0

    page_num = 0
    page_size = 50

    while True:
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é¡µæ•°é™åˆ¶
        if max_pages and page_num >= max_pages:
            print(f"\nğŸ“„ å·²è¾¾åˆ°æœ€å¤§é¡µæ•° {max_pages}")
            break

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ•°é‡é™åˆ¶
        if limit and processed_count >= limit:
            print(f"\nğŸ“Š å·²è¾¾åˆ°æ•°é‡é™åˆ¶ {limit}")
            break

        # é€šè¿‡ API è·å–æ•°æ®
        offset = page_num * page_size
        print(f"\nğŸ“„ è·å–ç¬¬ {page_num + 1} é¡µ (offset={offset})...")
        try:
            items = fetch_prompts_from_api(limit=page_size, offset=offset)
        except Exception as e:
            print(f"   âŒ API è¯·æ±‚å¤±è´¥: {e}")
            break

        if not items:
            print(f"   ğŸ“­ æ²¡æœ‰æ›´å¤šæ•°æ®")
            break

        stats["pages"] += 1
        stats["items_found"] += len(items)
        print(f"   æ‰¾åˆ° {len(items)} æ¡è®°å½•")

        # å¤„ç†æ¯ä¸ªæ¡ç›®
        for item in items:
            item_id = item.get("id", "")

            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if resume and item_id in processed_ids:
                continue

            # æ£€æŸ¥æ•°é‡é™åˆ¶
            if limit and processed_count >= limit:
                break

            # æå–æ•°æ®
            api_data = extract_data_from_api_item(item)
            if not api_data:
                continue

            processed_count += 1
            title_display = api_data.get("title", "")[:40] or item_id[:20]
            print(f"\n[{processed_count}] {title_display}")
            print(f"   ğŸ”— X: {api_data.get('x_url', '')[:60]}")

            result = process_api_item(db, api_data, dry_run=dry_run)

            # è®°å½• Twitter å¤„ç†å¤±è´¥
            if result.get("twitter_failed"):
                stats["twitter_failed"] += 1
                failed_items.append({
                    "id": item_id,
                    "x_url": api_data.get("x_url", ""),
                    "error": result.get("error", "Unknown")
                })

            if result["success"]:
                stats["success"] += 1
                if result["method"] == "dry_run":
                    print(f"   âœ… é¢„è§ˆé€šè¿‡")
                else:
                    print(f"   âœ… æˆåŠŸå…¥åº“")
            else:
                if result["method"] == "skipped":
                    stats["skipped"] += 1
                    print(f"   â­ï¸ è·³è¿‡: {result['error']}")
                elif result["method"] == "twitter_failed":
                    pass  # å·²åœ¨ä¸Šé¢è®°å½•
                else:
                    stats["failed"] += 1
                    print(f"   âŒ å¤±è´¥: {result['error']}")
                    failed_items.append({
                        "id": item_id,
                        "error": result.get("error", "Unknown")
                    })

            # ä¿å­˜è¿›åº¦
            if not dry_run:
                processed_ids.add(item_id)
                if processed_count % 10 == 0:
                    save_progress({"processed_slugs": list(processed_ids)})

        page_num += 1

    # æœ€ç»ˆä¿å­˜è¿›åº¦
    if not dry_run:
        save_progress({"processed_slugs": list(processed_ids)})

    # ä¿å­˜å¤±è´¥è®°å½•
    failed_file = None
    if failed_items and not dry_run:
        failed_file = save_failed_items(failed_items, timestamp)

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¼å…¥å®Œæˆ - ç»Ÿè®¡æ±‡æ€»")
    print("=" * 70)
    print(f"çˆ¬å–é¡µæ•°: {stats['pages']}")
    print(f"å‘ç°è®°å½•: {stats['items_found']}")
    print(f"âœ… æˆåŠŸ: {stats['success']}")
    print(f"â­ï¸ è·³è¿‡: {stats['skipped']}")
    print(f"âŒ å¤±è´¥: {stats['failed']}")
    print(f"âš ï¸ Twitter å¤±è´¥: {stats['twitter_failed']}")

    if failed_file:
        print(f"\nğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜: {failed_file}")

    print("=" * 70)

    db.close()


def run_import(limit: int = None, max_pages: int = None, dry_run: bool = False,
               resume: bool = True, reset_progress: bool = False):
    """åŒæ­¥å…¥å£"""
    # ä½¿ç”¨ asyncio.run è¿è¡Œå¼‚æ­¥å‡½æ•°
    try:
        asyncio.run(run_import_async(
            limit=limit,
            max_pages=max_pages,
            dry_run=dry_run,
            resume=resume,
            reset_progress=reset_progress
        ))
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œè¿›åº¦å·²ä¿å­˜")
        sys.exit(0)


def main():
    parser = argparse.ArgumentParser(
        description="ä» AIART.PICS å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“ (API)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å…¥æ•°æ®
  python import_aiart_pics.py

  # é™åˆ¶å¯¼å…¥æ•°é‡
  python import_aiart_pics.py --limit 10

  # åªè·å–å‰ 5 é¡µ
  python import_aiart_pics.py --pages 5

  # é¢„è§ˆæ¨¡å¼
  python import_aiart_pics.py --dry-run --limit 5

  # é‡ç½®è¿›åº¦
  python import_aiart_pics.py --reset

æµç¨‹:
  1. é€šè¿‡ API (/api/prompts) è·å–æç¤ºè¯åˆ—è¡¨
  2. ä½¿ç”¨ API è¿”å›çš„æ•°æ®ï¼ˆåŒ…å«æç¤ºè¯ã€å›¾ç‰‡ã€ä½œè€…ã€æ ‡ç­¾ï¼‰
  3. å¦‚éœ€è¦åˆ™è¿›è¡Œ AI åˆ†ç±»
  4. å†™å…¥æ•°æ®åº“
        """
    )

    parser.add_argument("--limit", "-l", type=int, help="é™åˆ¶å¯¼å…¥æ•°é‡")
    parser.add_argument("--pages", "-p", type=int, default=2, help="æœ€å¤§çˆ¬å–é¡µæ•° (é»˜è®¤: 2)")
    parser.add_argument("--dry-run", "-d", action="store_true", help="é¢„è§ˆæ¨¡å¼")
    parser.add_argument("--no-resume", action="store_true", help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®è¿›åº¦")

    args = parser.parse_args()

    run_import(
        limit=args.limit,
        max_pages=args.pages,
        dry_run=args.dry_run,
        resume=not args.no_resume,
        reset_progress=args.reset
    )


if __name__ == "__main__":
    main()
