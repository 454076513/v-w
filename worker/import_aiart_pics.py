#!/usr/bin/env python3
"""
AIART.PICS æç¤ºè¯å¯¼å…¥è„šæœ¬

ç›´æ¥ä» aiart.pics ç½‘ç«™çˆ¬å–æ•°æ®å¹¶å¯¼å…¥åˆ°æ•°æ®åº“ã€‚

å·¥ä½œæµç¨‹:
1. ç”¨ Playwright çˆ¬å– aiart.pics åˆ—è¡¨é¡µè·å–æ‰€æœ‰ slug
2. è®¿é—®è¯¦æƒ…é¡µè·å–æç¤ºè¯å’Œ x_url
3. ä» Twitter è·å–é«˜æ¸…å›¾ç‰‡
4. ä½¿ç”¨ AI åˆ†æåˆ†ç±»åå…¥åº“

ç¯å¢ƒå˜é‡:
  DATABASE_URL - PostgreSQL è¿æ¥å­—ç¬¦ä¸² (å¿…éœ€)
  AI_MODEL     - AI æ¨¡å‹ (é»˜è®¤: openai)

ç”¨æ³•:
  python import_aiart_pics.py                    # çˆ¬å–å¹¶å¯¼å…¥
  python import_aiart_pics.py --limit 10         # é™åˆ¶å¯¼å…¥æ•°é‡
  python import_aiart_pics.py --dry-run          # é¢„è§ˆæ¨¡å¼
  python import_aiart_pics.py --pages 5          # åªçˆ¬å–å‰ 5 é¡µ
  python import_aiart_pics.py --reset            # é‡ç½®è¿›åº¦
"""

import os
import sys
import json
import asyncio
import argparse
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv

    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"âœ“ å·²åŠ è½½: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"âœ“ å·²åŠ è½½: {env_file}")
except ImportError:
    pass

# å¯¼å…¥ä¸»æ¨¡å—
from main import Database, AI_MODEL
from fetch_twitter_content import fetch_tweet, classify_prompt_with_ai, extract_username

# ========== é…ç½® ==========
BASE_URL = "https://aiart.pics"
DATABASE_URL = os.environ.get("DATABASE_URL", "")

# æ•°æ®æ–‡ä»¶
CACHE_DIR = Path(__file__).parent / "cache"
PROGRESS_FILE = CACHE_DIR / "aiart_pics_import_progress.json"

# å¤±è´¥è®°å½•
FAILED_OUTPUT_DIR = Path(__file__).parent / "failed_imports"


async def fetch_list_page(page, page_num: int) -> List[Dict]:
    """çˆ¬å–åˆ—è¡¨é¡µè·å– slug åˆ—è¡¨"""
    url = f"{BASE_URL}/?page={page_num}"

    await page.goto(url, wait_until="networkidle", timeout=30000)
    await page.wait_for_timeout(3000)

    # ä»å›¾ç‰‡ URL æå– slug (ç½‘ç«™å·²æ”¹ä¸º onclick è€Œé <a> é“¾æ¥)
    items = await page.evaluate("""
        () => {
            const items = [];
            // æŸ¥æ‰¾æ‰€æœ‰ prompt å›¾ç‰‡
            document.querySelectorAll('img[src*="/prompts/"]').forEach(img => {
                const src = img.src;
                // URL æ ¼å¼: https://img1.aiart.pics/images/prompts/20260104/slug-name-1.jpg
                const match = src.match(/\\/prompts\\/\\d+\\/(.+?)-\\d+\\.(?:jpg|png|webp)/);
                if (match) {
                    const slug = match[1];
                    // å‘ä¸ŠæŸ¥æ‰¾å¡ç‰‡å®¹å™¨è·å–æ ‡é¢˜
                    let el = img;
                    let title = '';
                    for (let i = 0; i < 6 && el; i++) {
                        const h = el.querySelector('h2, h3, p.font-medium, [class*="title"]');
                        if (h) {
                            title = h.innerText?.trim() || '';
                            break;
                        }
                        el = el.parentElement;
                    }
                    items.push({ slug, title });
                }
            });
            // å»é‡
            const seen = new Set();
            return items.filter(item => {
                if (seen.has(item.slug)) return false;
                seen.add(item.slug);
                return true;
            });
        }
    """)

    return items


async def fetch_detail_page(page, slug: str) -> Optional[Dict]:
    """çˆ¬å–è¯¦æƒ…é¡µè·å–æç¤ºè¯å’Œ x_url"""
    url = f"{BASE_URL}/?prompt={slug}"

    await page.goto(url, wait_until="networkidle", timeout=30000)

    # ç­‰å¾…å†…å®¹åŠ è½½
    try:
        await page.wait_for_selector('a[href*="/status/"]', timeout=5000)
    except Exception:
        await page.wait_for_timeout(3000)

    # æå–æ•°æ®
    data = await page.evaluate("""
        () => {
            const result = { prompt: null, x_url: null, title: null };

            // æå– x_url
            const xLink = document.querySelector('a[href*="x.com/"][href*="/status/"], a[href*="twitter.com/"][href*="/status/"]');
            if (xLink) {
                result.x_url = xLink.getAttribute('href').replace('twitter.com', 'x.com');
            }

            // æå–æç¤ºè¯ (ä¼˜å…ˆ textareaï¼Œå…¶æ¬¡ .prose / pre)
            const textarea = document.querySelector('textarea');
            if (textarea && textarea.value) {
                result.prompt = textarea.value.trim();
            } else {
                const prose = document.querySelector('.prose');
                if (prose) {
                    result.prompt = prose.innerText.trim();
                } else {
                    const pre = document.querySelector('pre');
                    if (pre) result.prompt = pre.innerText.trim();
                }
            }

            // æå–æ ‡é¢˜
            const h1 = document.querySelector('h1');
            if (h1) result.title = h1.innerText.trim();

            return result;
        }
    """)

    return data if data.get('x_url') or data.get('prompt') else None


def load_progress() -> Dict:
    """åŠ è½½å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {"processed_slugs": [], "last_updated": None}


def save_progress(progress: Dict):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    progress["last_updated"] = datetime.now(timezone.utc).isoformat()

    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")


def clear_progress():
    """æ¸…é™¤å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        PROGRESS_FILE.unlink()
        print("ğŸ—‘ï¸ å·²æ¸…é™¤å¤„ç†è¿›åº¦")


def save_failed_items(failed_items: List[Dict], timestamp: str) -> Optional[Path]:
    """ä¿å­˜å¤±è´¥è®°å½•"""
    if not failed_items:
        return None

    FAILED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    filepath = FAILED_OUTPUT_DIR / f"aiart_pics_failed_{timestamp}.json"

    output = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "total": len(failed_items),
        "items": failed_items
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    return filepath


async def process_item(db: Database, page, slug: str, title: str, dry_run: bool = False) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ¡ç›®

    æµç¨‹:
    1. ä»è¯¦æƒ…é¡µè·å–æç¤ºè¯å’Œ x_url
    2. ä» Twitter è·å–å›¾ç‰‡
    3. ä½¿ç”¨ AI åˆ†ç±»
    4. å†™å…¥æ•°æ®åº“
    """
    # 1. è·å–è¯¦æƒ…é¡µæ•°æ®
    print(f"   ğŸŒ è·å–è¯¦æƒ…é¡µ...")
    try:
        detail = await fetch_detail_page(page, slug)
    except Exception as e:
        return {"success": False, "method": "page_failed", "error": f"Page error: {e}"}

    if not detail:
        return {"success": False, "method": "skipped", "error": "No data on page"}

    x_url = detail.get("x_url", "")

    if not x_url:
        return {"success": False, "method": "skipped", "error": "No x_url"}

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if db.prompt_exists(x_url):
        return {"success": False, "method": "skipped", "error": "Already exists"}

    # 2. ä» Twitter è·å–å›¾ç‰‡å’Œæ–‡æœ¬ï¼ˆä¸ä½¿ç”¨åŸç½‘é¡µçš„ prompt/titleï¼‰
    print(f"   ğŸ¦ ä» Twitter è·å–æ•°æ®...")
    try:
        result = fetch_tweet(
            x_url,
            download_images=False,
            extract_prompt=False,
            ai_model=AI_MODEL
        )
    except Exception as e:
        return {"success": False, "method": "twitter_failed", "error": str(e), "twitter_failed": True}

    if not result:
        return {"success": False, "method": "twitter_failed", "error": "fetch_tweet returned None", "twitter_failed": True}

    images = result.get("images", [])
    if not images:
        return {"success": False, "method": "twitter_failed", "error": "No images", "twitter_failed": True}

    # ä» Twitter è·å– promptï¼ˆfull_textï¼‰
    prompt = result.get("full_text", "").strip()
    if not prompt:
        return {"success": False, "method": "twitter_failed", "error": "No prompt from Twitter", "twitter_failed": True}

    print(f"   âœ… è·å–åˆ° {len(images)} å¼ å›¾ç‰‡")

    # 3. AI åˆ†ç±» - ä¼˜å…ˆä½¿ç”¨ AI ç»“æœ
    print(f"   ğŸ¤– AI åˆ†ç±»...")
    final_title = None
    category = None
    tags = []

    try:
        classification = classify_prompt_with_ai(prompt, AI_MODEL)
        if classification:
            ai_title = classification.get("title", "").strip()
            if ai_title and ai_title != "Untitled Prompt":
                final_title = ai_title

            ai_category = classification.get("category", "").strip()
            if ai_category:
                category = ai_category

            if classification.get("sub_categories"):
                tags = classification["sub_categories"][:5]

            print(f"   âœ… AI åˆ†ç±»: {category}")
    except Exception as e:
        print(f"   âš ï¸ AI åˆ†ç±»å¤±è´¥: {e}")

    # Fallback: AI å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¸ä½¿ç”¨åŸç½‘é¡µæ•°æ®ï¼‰
    if not final_title:
        final_title = "Untitled"
    if not category:
        category = "Illustration"

    if dry_run:
        print(f"   ğŸ” [Dry Run] å°†å…¥åº“:")
        print(f"      æ ‡é¢˜: {final_title}")
        print(f"      åˆ†ç±»: {category}")
        print(f"      å›¾ç‰‡: {len(images)}")
        print(f"      æç¤ºè¯: {prompt[:80]}...")
        return {"success": True, "method": "dry_run", "error": None}

    # 4. æå–ä½œè€…å¹¶å†™å…¥æ•°æ®åº“
    try:
        author = extract_username(x_url)
    except:
        author = None

    try:
        record = db.save_prompt(
            title=final_title,
            prompt=prompt,
            category=category,
            tags=tags,
            images=images[:5],
            source_link=x_url,
            author=author,
            import_source="aiart_pics"
        )

        if record:
            return {"success": True, "method": "imported", "error": None}
        else:
            return {"success": False, "method": "save_failed", "error": "Database save returned None"}
    except Exception as e:
        return {"success": False, "method": "save_failed", "error": str(e)}


async def run_import_async(limit: int = None, max_pages: int = None, dry_run: bool = False,
                           resume: bool = True, reset_progress: bool = False):
    """å¼‚æ­¥å¯¼å…¥æµç¨‹ - ç›´æ¥ä»ç½‘é¡µçˆ¬å–"""
    from playwright.async_api import async_playwright

    print("=" * 70)
    print("ğŸ“¦ AIART.PICS å¯¼å…¥ (ç½‘é¡µçˆ¬å–)")
    print("=" * 70)
    print(f"æ•°æ®æº: {BASE_URL}")
    print(f"é¢„è§ˆæ¨¡å¼: {dry_run}")
    print(f"æ–­ç‚¹ç»­ä¼ : {resume}")
    if limit:
        print(f"é™åˆ¶æ•°é‡: {limit}")
    if max_pages:
        print(f"æœ€å¤§é¡µæ•°: {max_pages}")
    print("=" * 70)

    # é‡ç½®è¿›åº¦
    if reset_progress:
        clear_progress()

    # æ£€æŸ¥é…ç½®
    if not DATABASE_URL:
        print("âŒ ç¼ºå°‘ DATABASE_URL ç¯å¢ƒå˜é‡")
        sys.exit(1)

    # åŠ è½½è¿›åº¦
    progress = load_progress()
    processed_slugs = set(progress.get("processed_slugs", []))
    if resume and processed_slugs:
        print(f"ğŸ“Š å·²å¤„ç†: {len(processed_slugs)} æ¡")
        print(f"   ä¸Šæ¬¡æ›´æ–°: {progress.get('last_updated', 'N/A')}")

    # è¿æ¥æ•°æ®åº“
    db = Database(DATABASE_URL)
    try:
        db.connect()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        sys.exit(1)

    # ç»Ÿè®¡
    stats = {
        "pages": 0,
        "items_found": 0,
        "success": 0,
        "skipped": 0,
        "failed": 0,
        "twitter_failed": 0,
    }

    failed_items = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    processed_count = 0

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        try:
            browser = await p.chromium.launch(headless=True, channel="chrome")
        except Exception:
            try:
                browser = await p.firefox.launch(headless=True)
            except Exception:
                browser = await p.chromium.launch(
                    headless=True,
                    executable_path="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
                )

        try:
            page = await browser.new_page()
            page_num = 1

            while True:
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é¡µæ•°é™åˆ¶
                if max_pages and page_num > max_pages:
                    print(f"\nğŸ“„ å·²è¾¾åˆ°æœ€å¤§é¡µæ•° {max_pages}")
                    break

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ•°é‡é™åˆ¶
                if limit and processed_count >= limit:
                    print(f"\nğŸ“Š å·²è¾¾åˆ°æ•°é‡é™åˆ¶ {limit}")
                    break

                # çˆ¬å–åˆ—è¡¨é¡µ
                print(f"\nğŸ“„ çˆ¬å–ç¬¬ {page_num} é¡µ...")
                try:
                    items = await fetch_list_page(page, page_num)
                except Exception as e:
                    print(f"   âŒ åˆ—è¡¨é¡µçˆ¬å–å¤±è´¥: {e}")
                    break

                if not items:
                    print(f"   ğŸ“­ æ²¡æœ‰æ›´å¤šæ•°æ®")
                    break

                stats["pages"] += 1
                stats["items_found"] += len(items)
                print(f"   æ‰¾åˆ° {len(items)} æ¡è®°å½•")

                # å¤„ç†æ¯ä¸ªæ¡ç›®
                for item in items:
                    slug = item.get("slug", "")
                    title = item.get("title", "")

                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                    if resume and slug in processed_slugs:
                        continue

                    # æ£€æŸ¥æ•°é‡é™åˆ¶
                    if limit and processed_count >= limit:
                        break

                    processed_count += 1
                    print(f"\n[{processed_count}] {slug[:50]}")

                    result = await process_item(db, page, slug, title, dry_run=dry_run)

                    if result.get("twitter_failed"):
                        stats["twitter_failed"] += 1
                        failed_items.append({
                            "slug": slug,
                            "error": result.get("error", "Unknown")
                        })

                    if result["success"]:
                        stats["success"] += 1
                        print(f"   âœ… æˆåŠŸå…¥åº“")
                    else:
                        if result["method"] == "skipped":
                            stats["skipped"] += 1
                            print(f"   â­ï¸ è·³è¿‡: {result['error']}")
                        else:
                            stats["failed"] += 1
                            print(f"   âŒ å¤±è´¥: {result['error']}")

                    # ä¿å­˜è¿›åº¦
                    if not dry_run:
                        processed_slugs.add(slug)
                        if processed_count % 10 == 0:
                            save_progress({"processed_slugs": list(processed_slugs)})

                page_num += 1

            # æœ€ç»ˆä¿å­˜è¿›åº¦
            if not dry_run:
                save_progress({"processed_slugs": list(processed_slugs)})

        finally:
            await browser.close()

    # ä¿å­˜å¤±è´¥è®°å½•
    failed_file = None
    if failed_items and not dry_run:
        failed_file = save_failed_items(failed_items, timestamp)

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¼å…¥å®Œæˆ - ç»Ÿè®¡æ±‡æ€»")
    print("=" * 70)
    print(f"çˆ¬å–é¡µæ•°: {stats['pages']}")
    print(f"å‘ç°è®°å½•: {stats['items_found']}")
    print(f"âœ… æˆåŠŸ: {stats['success']}")
    print(f"â­ï¸ è·³è¿‡: {stats['skipped']}")
    print(f"âŒ å¤±è´¥: {stats['failed']}")
    print(f"âš ï¸ Twitter å¤±è´¥: {stats['twitter_failed']}")

    if failed_file:
        print(f"\nğŸ“ å¤±è´¥è®°å½•å·²ä¿å­˜: {failed_file}")

    print("=" * 70)

    db.close()


def run_import(limit: int = None, max_pages: int = None, dry_run: bool = False,
               resume: bool = True, reset_progress: bool = False):
    """åŒæ­¥å…¥å£"""
    asyncio.run(run_import_async(
        limit=limit,
        max_pages=max_pages,
        dry_run=dry_run,
        resume=resume,
        reset_progress=reset_progress
    ))


def main():
    parser = argparse.ArgumentParser(
        description="ä» AIART.PICS å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“ (ç½‘é¡µçˆ¬å–)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # çˆ¬å–å¹¶å¯¼å…¥
  python import_aiart_pics.py

  # é™åˆ¶å¯¼å…¥æ•°é‡
  python import_aiart_pics.py --limit 10

  # åªçˆ¬å–å‰ 5 é¡µ
  python import_aiart_pics.py --pages 5

  # é¢„è§ˆæ¨¡å¼
  python import_aiart_pics.py --dry-run --limit 5

  # é‡ç½®è¿›åº¦
  python import_aiart_pics.py --reset

æµç¨‹:
  1. ç”¨ Playwright çˆ¬å– aiart.pics åˆ—è¡¨é¡µè·å–æ‰€æœ‰ slug
  2. è®¿é—®è¯¦æƒ…é¡µè·å–æç¤ºè¯å’Œ x_url
  3. ä» Twitter è·å–é«˜æ¸…å›¾ç‰‡
  4. ä½¿ç”¨ AI åˆ†ç±»åå…¥åº“
        """
    )

    parser.add_argument("--limit", "-l", type=int, help="é™åˆ¶å¯¼å…¥æ•°é‡")
    parser.add_argument("--pages", "-p", type=int, default=2, help="æœ€å¤§çˆ¬å–é¡µæ•° (é»˜è®¤: 2)")
    parser.add_argument("--dry-run", "-d", action="store_true", help="é¢„è§ˆæ¨¡å¼")
    parser.add_argument("--no-resume", action="store_true", help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®è¿›åº¦")

    args = parser.parse_args()

    run_import(
        limit=args.limit,
        max_pages=args.pages,
        dry_run=args.dry_run,
        resume=not args.no_resume,
        reset_progress=args.reset
    )


if __name__ == "__main__":
    main()
