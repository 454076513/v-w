#!/usr/bin/env python3
"""
Twitter/X Content Fetcher
è·å– Twitter/X æ¨æ–‡çš„æ­£æ–‡å†…å®¹å’Œå›¾ç‰‡

ä½¿ç”¨æ–¹æ³•:
    python fetch_twitter_content.py <tweet_url>
    
ç¤ºä¾‹:
    python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637
"""

import re
import sys
import os
import requests
from urllib.parse import urlparse

# å¯é€‰ä¾èµ–
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False


# Pollinations API é…ç½®
POLLINATIONS_API_URL = "https://text.pollinations.ai/"
DEFAULT_MODEL = "openai"  # å…è´¹æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ "deepseek" (éœ€è¦ seed tier)

# Gitee AI API é…ç½® (fallback)
GITEE_AI_API_URL = "https://ai.gitee.com/v1/chat/completions"
GITEE_AI_MODEL = "DeepSeek-V3"
GITEE_AI_API_KEY = os.environ.get("GITEE_AI_API_KEY", "")


def _call_gitee_ai(messages: list) -> str:
    """
    è°ƒç”¨ Gitee AI API (fallback)ï¼Œä½¿ç”¨ stream æ¨¡å¼é¿å…è¶…æ—¶
    
    Args:
        messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        AI å“åº”å†…å®¹
    """
    import json
    
    if not GITEE_AI_API_KEY:
        raise Exception("GITEE_AI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {GITEE_AI_API_KEY}',
        'Accept': 'text/event-stream',
    }
    
    payload = {
        "model": GITEE_AI_MODEL,
        "messages": messages,
        "temperature": 0.7,
        "stream": True,  # å¯ç”¨æµå¼è¾“å‡º
    }
    
    # ä½¿ç”¨ stream=True é¿å…è¯»å–è¶…æ—¶
    response = requests.post(
        GITEE_AI_API_URL, 
        json=payload, 
        headers=headers, 
        timeout=(10, 300),  # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
        stream=True
    )
    
    if response.status_code != 200:
        raise Exception(f"Gitee AI è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
    
    # æ”¶é›†æµå¼å“åº”
    full_content = []
    
    for line in response.iter_lines():
        if not line:
            continue
        
        line = line.decode('utf-8')
        
        # SSE æ ¼å¼: "data: {...}"
        if line.startswith('data: '):
            data_str = line[6:]  # å»æ‰ "data: " å‰ç¼€
            
            # ç»“æŸæ ‡è®°
            if data_str == '[DONE]':
                break
            
            try:
                data = json.loads(data_str)
                if "choices" in data and len(data["choices"]) > 0:
                    delta = data["choices"][0].get("delta", {})
                    content = delta.get("content", "")
                    if content:
                        full_content.append(content)
            except json.JSONDecodeError:
                continue
    
    if not full_content:
        raise Exception("Gitee AI è¿”å›ç©ºå“åº”")
    
    return "".join(full_content)


def _call_pollinations_ai(messages: list, model: str = DEFAULT_MODEL) -> str:
    """
    è°ƒç”¨ Pollinations AI API

    Args:
        messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        model: ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        AI å“åº”å†…å®¹
    """
    # ç¡®ä¿ model ä¸ä¸ºç©º
    if not model or not model.strip():
        model = DEFAULT_MODEL

    headers = {
        'Content-Type': 'application/json',
    }

    payload = {
        "model": model,
        "messages": messages,
    }
    
    response = requests.post(POLLINATIONS_API_URL, json=payload, headers=headers, timeout=60)
    
    if response.status_code == 200:
        # å“åº”å¯èƒ½æ˜¯çº¯æ–‡æœ¬æˆ– JSON
        import json as json_module
        
        try:
            data = response.json()
            if isinstance(data, dict):
                # OpenAI æ ¼å¼: {"choices": [{"message": {"content": "..."}}]}
                if "choices" in data:
                    return data["choices"][0]["message"]["content"]
                # ç®€åŒ–æ ¼å¼: {"content": "..."}
                elif "content" in data:
                    return data["content"]
                elif "reasoning_content" in data:
                    return data["reasoning_content"]
                else:
                    # å¦‚æœè¿”å›çš„æ˜¯ç›´æ¥çš„ JSON å¯¹è±¡ï¼ˆæ¯”å¦‚åˆ†ç±»ç»“æœï¼‰ï¼Œè½¬å› JSON å­—ç¬¦ä¸²
                    return json_module.dumps(data, ensure_ascii=False)
            elif isinstance(data, str):
                return data
            else:
                return json_module.dumps(data, ensure_ascii=False)
        except:
            # çº¯æ–‡æœ¬å“åº”
            return response.text.strip()
    else:
        raise Exception(f"Pollinations API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")


def _call_ai_with_fallback(messages: list, model: str = DEFAULT_MODEL) -> str:
    """
    è°ƒç”¨ AI APIï¼Œå¦‚æœ Pollinations å¤±è´¥åˆ™ fallback åˆ° Gitee AI
    
    Args:
        messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        model: Pollinations ä½¿ç”¨çš„æ¨¡å‹
    
    Returns:
        AI å“åº”å†…å®¹
    """
    # é¦–å…ˆå°è¯• Pollinations AI
    try:
        result = _call_pollinations_ai(messages, model)
        return result
    except Exception as pollinations_error:
        print(f"âš ï¸ Pollinations AI å¤±è´¥: {pollinations_error}")
        
        # Fallback åˆ° Gitee AI
        if GITEE_AI_API_KEY:
            print(f"ğŸ”„ å°è¯• Gitee AI (DeepSeek-V3) ä½œä¸º fallback...")
            try:
                result = _call_gitee_ai(messages)
                print("âœ“ Gitee AI è°ƒç”¨æˆåŠŸ")
                return result
            except Exception as gitee_error:
                print(f"âœ— Gitee AI ä¹Ÿå¤±è´¥: {gitee_error}")
                raise Exception(f"æ‰€æœ‰ AI æœåŠ¡éƒ½å¤±è´¥: Pollinations ({pollinations_error}), Gitee ({gitee_error})")
        else:
            print("âš ï¸ GITEE_AI_API_KEY æœªè®¾ç½®ï¼Œæ— æ³•ä½¿ç”¨ fallback")
            raise pollinations_error


def extract_prompt_with_ai(text: str, model: str = DEFAULT_MODEL) -> str:
    """
    ä½¿ç”¨ AI API ä»æ–‡æœ¬ä¸­æå–æç¤ºè¯
    ä¼˜å…ˆä½¿ç”¨ Pollinations AIï¼Œå¤±è´¥å fallback åˆ° Gitee AI (DeepSeek-V3)
    
    Args:
        text: æ¨æ–‡æ­£æ–‡å†…å®¹
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ openaiï¼Œå¯é€‰ deepseek
    
    Returns:
        æå–å‡ºçš„æç¤ºè¯
    """
    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant that extracts AI image generation prompts from text. Extract only the prompt itself, without any additional explanation or formatting. If no prompt is found, return 'No prompt found'."
        },
        {
            "role": "user",
            "content": f"Extract the AI image generation prompt from this text and return only the prompt itself:\n\n{text}"
        }
    ]
    
    try:
        return _call_ai_with_fallback(messages, model)
    except requests.exceptions.Timeout:
        raise Exception("API è¯·æ±‚è¶…æ—¶")
    except Exception as e:
        raise Exception(f"æå–æç¤ºè¯å¤±è´¥: {e}")


# é¢„å®šä¹‰çš„åˆ†ç±»åˆ—è¡¨
PROMPT_CATEGORIES = [
    "äººåƒ/è‚–åƒ (Portrait)",
    "é£æ™¯/è‡ªç„¶ (Landscape/Nature)",
    "åŠ¨ç‰© (Animals)",
    "å»ºç­‘/åŸå¸‚ (Architecture/Urban)",
    "æŠ½è±¡è‰ºæœ¯ (Abstract Art)",
    "ç§‘å¹»/æœªæ¥ (Sci-Fi/Futuristic)",
    "å¥‡å¹»/é­”æ³• (Fantasy/Magic)",
    "åŠ¨æ¼«/å¡é€š (Anime/Cartoon)",
    "å†™å®æ‘„å½± (Realistic Photography)",
    "æ’ç”»/ç»˜ç”» (Illustration/Painting)",
    "æ—¶å°š/æœè£… (Fashion/Clothing)",
    "é£Ÿç‰©/ç¾é£Ÿ (Food)",
    "äº§å“/å•†ä¸š (Product/Commercial)",
    "ææ€–/é»‘æš— (Horror/Dark)",
    "å¯çˆ±/èŒç³» (Cute/Kawaii)",
    "å¤å¤/æ€€æ—§ (Vintage/Retro)",
    "æç®€ä¸»ä¹‰ (Minimalist)",
    "è¶…ç°å® (Surreal)",
    "å…¶ä»– (Other)",
]


def classify_prompt_with_ai(prompt: str, model: str = DEFAULT_MODEL) -> dict:
    """
    ä½¿ç”¨ AI API å¯¹æç¤ºè¯è¿›è¡Œåˆ†ç±»
    ä¼˜å…ˆä½¿ç”¨ Pollinations AIï¼Œå¤±è´¥å fallback åˆ° Gitee AI (DeepSeek-V3)
    
    Args:
        prompt: æç¤ºè¯å†…å®¹
        model: ä½¿ç”¨çš„æ¨¡å‹
    
    Returns:
        åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸ {"category": "åˆ†ç±»", "confidence": "é«˜/ä¸­/ä½", "reason": "åŸå› "}
    """
    categories_str = "\n".join([f"- {cat}" for cat in PROMPT_CATEGORIES])
    
    messages = [
        {
            "role": "system",
            "content": f"""You are an AI image prompt classifier. Analyze the given prompt and classify it into one of the following categories:

{categories_str}

Respond in JSON format with exactly these fields:
- "title": a concise, descriptive title for this prompt in English (3-8 words, like a short headline)
- "category": the main category (choose from the list above, use the English part only, e.g., "Portrait", "Landscape/Nature")
- "sub_categories": array of 1-3 secondary categories in English (e.g., ["Fashion/Clothing", "Realistic Photography"])
- "style": detected art style (e.g., "photorealistic", "anime", "oil painting", "3D render", etc.)
- "confidence": "high", "medium", or "low"
- "reason": brief explanation in English (1 sentence)

Example response:
{{"title": "Fashion Actress Bird's Eye View", "category": "Portrait", "sub_categories": ["Fashion/Clothing"], "style": "photorealistic", "confidence": "high", "reason": "The prompt describes a Japanese actress in a black coat from above"}}"""
        },
        {
            "role": "user",
            "content": f"Classify this AI image generation prompt:\n\n{prompt}"
        }
    ]
    
    try:
        response_text = _call_ai_with_fallback(messages, model)
        
        # å°è¯•è§£æ JSON
        import json
        
        result = None
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        cleaned_text = response_text.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if cleaned_text.startswith("```json"):
            cleaned_text = cleaned_text[7:]
        elif cleaned_text.startswith("```"):
            cleaned_text = cleaned_text[3:]
        if cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-3]
        cleaned_text = cleaned_text.strip()
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            result = json.loads(cleaned_text)
        except json.JSONDecodeError:
            # å°è¯•ä»å“åº”ä¸­æå– JSON (æ”¯æŒåµŒå¥—)
            json_match = re.search(r'\{.*\}', cleaned_text, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                except:
                    pass
        
        if not result:
            # è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
            print(f"      âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response_text[:200]}")
            return {
                "title": "Untitled Prompt",
                "category": "Other",
                "sub_categories": [],
                "style": "unknown",
                "confidence": "low",
                "reason": "Failed to parse classification result"
            }
        
        # æ ‡å‡†åŒ–ç»“æœï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦å­—æ®µå­˜åœ¨
        normalized = {
            "title": result.get("title", "æœªå‘½åæç¤ºè¯"),
            "category": result.get("category", "å…¶ä»– (Other)"),
            "sub_categories": result.get("sub_categories", []),
            "style": result.get("style", "unknown"),
            "confidence": result.get("confidence", "ä¸­"),
            "reason": result.get("reason", ""),
        }
        
        # ç¡®ä¿ title æ˜¯å­—ç¬¦ä¸²
        if not isinstance(normalized["title"], str) or not normalized["title"].strip():
            normalized["title"] = "Untitled Prompt"
        
        # ç¡®ä¿ sub_categories æ˜¯åˆ—è¡¨
        if not isinstance(normalized["sub_categories"], list):
            normalized["sub_categories"] = []
        
        # æ¸…ç† sub_categories
        cleaned_tags = []
        for tag in normalized["sub_categories"]:
            if isinstance(tag, str) and tag.strip():
                cleaned_tags.append(tag.strip())
        normalized["sub_categories"] = cleaned_tags
        
        # æ·»åŠ  style åˆ° tags ä¸­ï¼ˆå¦‚æœä¸ä¸ºç©ºï¼‰
        if normalized["style"] and normalized["style"] != "unknown":
            if normalized["style"] not in normalized["sub_categories"]:
                normalized["sub_categories"].append(normalized["style"])
        
        print(f"      ğŸ“‹ åˆ†ç±»ç»“æœ: title={normalized['title']}, category={normalized['category']}, tags={normalized['sub_categories']}")
        
        return normalized
        
    except requests.exceptions.Timeout:
        raise Exception("API è¯·æ±‚è¶…æ—¶")
    except Exception as e:
        raise Exception(f"åˆ†ç±»å¤±è´¥: {e}")


def extract_tweet_id(url: str) -> str:
    """ä» URL ä¸­æå–æ¨æ–‡ ID"""
    # æ”¯æŒ twitter.com å’Œ x.com
    pattern = r'(?:twitter\.com|x\.com)/\w+/status/(\d+)'
    match = re.search(pattern, url)
    if match:
        return match.group(1)
    raise ValueError(f"æ— æ³•ä» URL ä¸­æå–æ¨æ–‡ ID: {url}")


def extract_username(url: str) -> str:
    """ä» URL ä¸­æå–ç”¨æˆ·å"""
    pattern = r'(?:twitter\.com|x\.com)/(\w+)/status/\d+'
    match = re.search(pattern, url)
    if match:
        return match.group(1)
    raise ValueError(f"æ— æ³•ä» URL ä¸­æå–ç”¨æˆ·å: {url}")


def fetch_with_syndication_api(tweet_id: str) -> dict:
    """
    ä½¿ç”¨ Twitter Syndication API è·å–æ¨æ–‡å†…å®¹
    è¿™æ˜¯å…¬å¼€ APIï¼Œä¸éœ€è¦è®¤è¯
    """
    url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&token=0"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Syndication API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_fxtwitter(tweet_id: str, username: str) -> dict:
    """
    ä½¿ç”¨ FxTwitter/FixupX API è·å–æ¨æ–‡å†…å®¹
    è¿™æ˜¯ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œæä¾›æ›´å¥½çš„åµŒå…¥ä½“éªŒ
    """
    url = f"https://api.fxtwitter.com/{username}/status/{tweet_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TwitterBot/1.0)',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"FxTwitter API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_vxtwitter(tweet_id: str, username: str) -> dict:
    """
    ä½¿ç”¨ VxTwitter API è·å–æ¨æ–‡å†…å®¹
    """
    url = f"https://api.vxtwitter.com/{username}/status/{tweet_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TwitterBot/1.0)',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"VxTwitter API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_playwright(url: str) -> dict:
    """
    ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–æ¨æ–‡å†…å®¹
    éœ€è¦å®‰è£…: pip install playwright && playwright install chromium
    """
    if not HAS_PLAYWRIGHT:
        raise ImportError("Playwright æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install playwright && playwright install chromium")
    
    result = {"text": None, "images": []}
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        page = context.new_page()
        
        try:
            page.goto(url, wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(3000)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            # è·å–æ¨æ–‡æ–‡æœ¬
            tweet_text_selectors = [
                'article[data-testid="tweet"] div[data-testid="tweetText"]',
                'article div[lang]',
                '[data-testid="tweetText"]',
            ]
            
            for selector in tweet_text_selectors:
                try:
                    text_element = page.query_selector(selector)
                    if text_element:
                        result["text"] = text_element.inner_text()
                        break
                except:
                    continue
            
            # è·å–å›¾ç‰‡
            image_selectors = [
                'article[data-testid="tweet"] img[src*="pbs.twimg.com/media"]',
                'article img[src*="twimg.com/media"]',
                '[data-testid="tweetPhoto"] img',
            ]
            
            for selector in image_selectors:
                try:
                    images = page.query_selector_all(selector)
                    for img in images:
                        src = img.get_attribute('src')
                        if src and 'pbs.twimg.com' in src:
                            # è·å–é«˜æ¸…ç‰ˆæœ¬
                            high_res = re.sub(r'\?.*$', '?format=jpg&name=large', src)
                            if high_res not in result["images"]:
                                result["images"].append(high_res)
                except:
                    continue
            
        finally:
            browser.close()
    
    return result


def download_image(url: str, save_path: str) -> str:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    }
    
    response = requests.get(url, headers=headers, timeout=30, stream=True)
    
    if response.status_code == 200:
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return save_path
    else:
        raise Exception(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {response.status_code}")


def parse_syndication_result(data: dict) -> dict:
    """è§£æ Syndication API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    if "text" in data:
        result["text"] = data["text"]
    
    if "user" in data:
        result["user"] = {
            "name": data["user"].get("name", ""),
            "screen_name": data["user"].get("screen_name", ""),
        }
    
    if "created_at" in data:
        result["created_at"] = data["created_at"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": data.get("reply_count", 0),
        "retweets": data.get("retweet_count", 0),
        "likes": data.get("favorite_count", 0),
        "bookmarks": data.get("bookmark_count", 0),
        "views": data.get("views_count", 0),
    }
    
    # æå–åª’ä½“
    if "mediaDetails" in data:
        for media in data["mediaDetails"]:
            if media.get("type") == "photo":
                result["images"].append(media.get("media_url_https", ""))
    
    # ä¹Ÿæ£€æŸ¥ photos å­—æ®µ
    if "photos" in data:
        for photo in data["photos"]:
            url = photo.get("url", "")
            if url and url not in result["images"]:
                result["images"].append(url)
    
    return result


def parse_fxtwitter_result(data: dict) -> dict:
    """è§£æ FxTwitter API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    tweet = data.get("tweet", {})
    
    if "text" in tweet:
        result["text"] = tweet["text"]
    
    if "author" in tweet:
        result["user"] = {
            "name": tweet["author"].get("name", ""),
            "screen_name": tweet["author"].get("screen_name", ""),
        }
    
    if "created_at" in tweet:
        result["created_at"] = tweet["created_at"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": tweet.get("replies", 0),
        "retweets": tweet.get("retweets", 0),
        "likes": tweet.get("likes", 0),
        "bookmarks": tweet.get("bookmarks", 0),
        "views": tweet.get("views", 0),
    }
    
    # æå–åª’ä½“
    if "media" in tweet and "photos" in tweet["media"]:
        for photo in tweet["media"]["photos"]:
            result["images"].append(photo.get("url", ""))
    
    return result


def parse_vxtwitter_result(data: dict) -> dict:
    """è§£æ VxTwitter API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    if "text" in data:
        result["text"] = data["text"]
    
    result["user"] = {
        "name": data.get("user_name", ""),
        "screen_name": data.get("user_screen_name", ""),
    }
    
    if "date" in data:
        result["created_at"] = data["date"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": data.get("replies", 0),
        "retweets": data.get("retweets", 0),
        "likes": data.get("likes", 0),
        "bookmarks": data.get("bookmarks", 0),
        "views": data.get("views", 0),
    }
    
    # æå–åª’ä½“
    if "media_extended" in data:
        for media in data["media_extended"]:
            if media.get("type") == "image":
                result["images"].append(media.get("url", ""))
    
    return result


def fetch_tweet(url: str, download_images: bool = True, output_dir: str = ".", 
                extract_prompt: bool = False, ai_model: str = DEFAULT_MODEL) -> dict:
    """
    è·å–æ¨æ–‡å†…å®¹çš„ä¸»å‡½æ•°
    ä¼šä¾æ¬¡å°è¯•ä¸åŒçš„æ–¹æ³•ç›´åˆ°æˆåŠŸ
    
    Args:
        url: æ¨æ–‡ URL
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        output_dir: è¾“å‡ºç›®å½•
        extract_prompt: æ˜¯å¦ä½¿ç”¨ AI æå–æç¤ºè¯
        ai_model: AI æ¨¡å‹åç§° (openai, deepseek ç­‰)
    """
    from datetime import datetime
    
    start_time = datetime.now()
    
    # è§£æ URL
    try:
        tweet_id = extract_tweet_id(url)
        username = extract_username(url)
    except Exception as e:
        print(f"âŒ [FAILED] æ— æ•ˆçš„ Twitter URL: {url}")
        print(f"   é”™è¯¯: {e}")
        raise Exception(f"æ— æ•ˆçš„ Twitter URL: {url}")
    
    print()
    print("=" * 70)
    print(f"ğŸ¦ å¼€å§‹å¤„ç†æ¨æ–‡")
    print(f"   URL: {url}")
    print(f"   æ¨æ–‡ ID: {tweet_id}")
    print(f"   ç”¨æˆ·å: @{username}")
    print(f"   æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    result = None
    fetch_method = None
    fetch_errors = []
    
    # æ–¹æ³• 1: å°è¯• FxTwitter API
    print("   [1/4] å°è¯• FxTwitter API...")
    try:
        data = fetch_with_fxtwitter(tweet_id, username)
        result = parse_fxtwitter_result(data)
        if result and result.get("text"):
            fetch_method = "FxTwitter"
            print("   âœ“ FxTwitter API æˆåŠŸ")
        else:
            raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
    except Exception as e:
        fetch_errors.append(f"FxTwitter: {e}")
        print(f"   âœ— FxTwitter API å¤±è´¥: {e}")
    
    # æ–¹æ³• 2: å°è¯• VxTwitter API
    if not result or not result.get("text"):
        print("   [2/4] å°è¯• VxTwitter API...")
        try:
            data = fetch_with_vxtwitter(tweet_id, username)
            result = parse_vxtwitter_result(data)
            if result and result.get("text"):
                fetch_method = "VxTwitter"
                print("   âœ“ VxTwitter API æˆåŠŸ")
            else:
                raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
        except Exception as e:
            fetch_errors.append(f"VxTwitter: {e}")
            print(f"   âœ— VxTwitter API å¤±è´¥: {e}")
    
    # æ–¹æ³• 3: å°è¯• Twitter Syndication API
    if not result or not result.get("text"):
        print("   [3/4] å°è¯• Syndication API...")
        try:
            data = fetch_with_syndication_api(tweet_id)
            result = parse_syndication_result(data)
            if result and result.get("text"):
                fetch_method = "Syndication"
                print("   âœ“ Syndication API æˆåŠŸ")
            else:
                raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
        except Exception as e:
            fetch_errors.append(f"Syndication: {e}")
            print(f"   âœ— Syndication API å¤±è´¥: {e}")
    
    # æ–¹æ³• 4: å°è¯• Playwright (éœ€è¦å®‰è£…)
    if not result or not result.get("text"):
        if HAS_PLAYWRIGHT:
            print("   [4/4] å°è¯• Playwright æµè§ˆå™¨...")
            try:
                result = fetch_with_playwright(url)
                if result and result.get("text"):
                    fetch_method = "Playwright"
                    print("   âœ“ Playwright æˆåŠŸ")
                else:
                    raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
            except Exception as e:
                fetch_errors.append(f"Playwright: {e}")
                print(f"   âœ— Playwright å¤±è´¥: {e}")
        else:
            print("   [4/4] è·³è¿‡ Playwright (æœªå®‰è£…)")
    
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å†…å®¹
    if not result or not result.get("text"):
        elapsed = (datetime.now() - start_time).total_seconds()
        print()
        print(f"âŒ [FAILED] æ¨æ–‡è·å–å¤±è´¥: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è€—æ—¶: {elapsed:.1f}s")
        print(f"   å°è¯•çš„æ–¹æ³•åŠé”™è¯¯:")
        for err in fetch_errors:
            print(f"      - {err}")
        print("=" * 70)
        raise Exception(f"æ‰€æœ‰è·å–æ–¹æ³•éƒ½å¤±è´¥äº†: {url}")
    
    print()
    print(f"   âœ“ å†…å®¹è·å–æˆåŠŸ (via {fetch_method})")
    text_preview = result.get("text", "")[:100].replace("\n", " ")
    print(f"   ğŸ“ å†…å®¹é¢„è§ˆ: {text_preview}...")
    
    # ä¸‹è½½å›¾ç‰‡
    if download_images and result.get("images"):
        print()
        print(f"   ğŸ–¼ï¸  å‘ç° {len(result['images'])} å¼ å›¾ç‰‡")
        
        os.makedirs(output_dir, exist_ok=True)
        downloaded_images = []
        
        for i, img_url in enumerate(result["images"]):
            # è·å–é«˜æ¸…ç‰ˆæœ¬
            if "?" in img_url:
                high_res_url = re.sub(r'name=\w+', 'name=large', img_url)
            else:
                high_res_url = img_url + "?format=jpg&name=large"
            
            filename = f"tweet_{tweet_id}_image_{i+1}.jpg"
            filepath = os.path.join(output_dir, filename)
            
            try:
                download_image(high_res_url, filepath)
                downloaded_images.append(filepath)
                print(f"      âœ“ å›¾ç‰‡ {i+1}: {filename}")
            except Exception as e:
                print(f"      âœ— å›¾ç‰‡ {i+1} ä¸‹è½½å¤±è´¥: {e}")
        
        result["downloaded_images"] = downloaded_images
    
    # ä½¿ç”¨ AI æå–æç¤ºè¯
    if extract_prompt and result.get("text"):
        print()
        print(f"   ğŸ¤– AI å¤„ç† (æ¨¡å‹: {ai_model})")
        
        # æå–æç¤ºè¯
        print(f"      [1/2] æå–æç¤ºè¯...")
        try:
            extracted_prompt = extract_prompt_with_ai(result["text"], model=ai_model)
            result["extracted_prompt"] = extracted_prompt
            
            if extracted_prompt and extracted_prompt != "No prompt found":
                prompt_preview = extracted_prompt[:80].replace("\n", " ")
                print(f"      âœ“ æå–æˆåŠŸ: {prompt_preview}...")
                
                # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
                print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
                try:
                    classification = classify_prompt_with_ai(extracted_prompt, model=ai_model)
                    result["classification"] = classification
                    
                    title = classification.get("title", "æœªçŸ¥")
                    category = classification.get("category", "æœªçŸ¥")
                    confidence = classification.get("confidence", "æœªçŸ¥")
                    print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
                except Exception as e:
                    print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                    result["classification"] = None
            else:
                print(f"      âš ï¸ æœªæ‰¾åˆ°æç¤ºè¯")
                result["classification"] = None
        except Exception as e:
            print(f"      âœ— æå–å¤±è´¥: {e}")
            result["extracted_prompt"] = None
            result["classification"] = None
    
    # å®Œæˆ
    elapsed = (datetime.now() - start_time).total_seconds()
    print()
    print(f"âœ… [SUCCESS] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
    print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
    print(f"   è·å–æ–¹å¼: {fetch_method}")
    print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
    if result.get("extracted_prompt") and result["extracted_prompt"] != "No prompt found":
        print(f"   æç¤ºè¯: å·²æå–")
        if result.get("classification"):
            print(f"   åˆ†ç±»: {result['classification'].get('category', 'æœªçŸ¥')}")
    else:
        print(f"   æç¤ºè¯: æœªæ‰¾åˆ°")
    print(f"   è€—æ—¶: {elapsed:.1f}s")
    print("=" * 70)
    
    return result


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Twitter/X å†…å®¹è·å–å·¥å…· - è·å–æ¨æ–‡æ­£æ–‡ã€å›¾ç‰‡å’Œäº’åŠ¨ç»Ÿè®¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³• (é»˜è®¤å¯ç”¨æç¤ºè¯æå–å’Œåˆ†ç±»)
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637
  
  # ä½¿ç”¨ deepseek æ¨¡å‹
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 --model deepseek
  
  # ç¦ç”¨æç¤ºè¯æå–
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 --no-extract
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 -o ./output
        """
    )
    
    parser.add_argument("url", help="æ¨æ–‡ URL (æ”¯æŒ x.com å’Œ twitter.com)")
    parser.add_argument("-o", "--output", default=".", help="è¾“å‡ºç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)")
    parser.add_argument("--no-extract", action="store_true", 
                        help="ç¦ç”¨ AI æå–æç¤ºè¯å’Œåˆ†ç±» (é»˜è®¤å¯ç”¨)")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL,
                        help=f"AI æ¨¡å‹ (é»˜è®¤: {DEFAULT_MODEL}, å¯é€‰: deepseek, openai ç­‰)")
    parser.add_argument("--no-download", action="store_true", help="ä¸ä¸‹è½½å›¾ç‰‡")
    
    args = parser.parse_args()
    
    url = args.url
    output_dir = args.output
    
    extract_prompt = not args.no_extract  # é»˜è®¤å¯ç”¨æå–æç¤ºè¯
    
    print("=" * 50)
    print("Twitter/X å†…å®¹è·å–å·¥å…·")
    print("=" * 50)
    print(f"URL: {url}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    if extract_prompt:
        print(f"AI æ¨¡å‹: {args.model}")
    print("=" * 50)
    
    try:
        result = fetch_tweet(
            url, 
            download_images=not args.no_download, 
            output_dir=output_dir,
            extract_prompt=extract_prompt,
            ai_model=args.model
        )
        
        print("\n" + "=" * 50)
        print("è·å–ç»“æœ")
        print("=" * 50)
        
        if result.get("user"):
            user = result["user"]
            print(f"ç”¨æˆ·: {user.get('name', '')} (@{user.get('screen_name', '')})")
        
        if result.get("created_at"):
            print(f"æ—¶é—´: {result['created_at']}")
        
        # æ˜¾ç¤ºäº’åŠ¨ç»Ÿè®¡
        if result.get("stats"):
            stats = result["stats"]
            print(f"\näº’åŠ¨ç»Ÿè®¡:")
            print(f"  ğŸ’¬ è¯„è®ºæ•° (Replies): {stats.get('replies', 0)}")
            print(f"  ğŸ” è½¬å‘æ•° (Retweets): {stats.get('retweets', 0)}")
            print(f"  â¤ï¸  ç‚¹èµæ•° (Likes): {stats.get('likes', 0)}")
            print(f"  ğŸ”– ä¹¦ç­¾æ•° (Bookmarks): {stats.get('bookmarks', 0)}")
            print(f"  ğŸ‘ï¸  æµè§ˆé‡ (Views): {stats.get('views', 0)}")
        
        print(f"\næ­£æ–‡å†…å®¹:")
        print("-" * 50)
        print(result.get("text", "æ— æ³•è·å–"))
        print("-" * 50)
        
        # æ˜¾ç¤ºæå–çš„æç¤ºè¯
        if result.get("extracted_prompt"):
            print(f"\nğŸ¨ æå–çš„æç¤ºè¯:")
            print("-" * 50)
            print(result["extracted_prompt"])
            print("-" * 50)
        
        # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
        if result.get("classification"):
            cls = result["classification"]
            print(f"\nğŸ“‚ æç¤ºè¯åˆ†ç±»:")
            print("-" * 50)
            if cls.get("title"):
                print(f"  ğŸ“Œ æ ‡é¢˜: {cls.get('title')}")
            print(f"  ä¸»åˆ†ç±»: {cls.get('category', 'æœªçŸ¥')}")
            if cls.get("sub_categories"):
                print(f"  æ¬¡åˆ†ç±»: {', '.join(cls['sub_categories'])}")
            if cls.get("style"):
                print(f"  é£æ ¼: {cls.get('style', 'æœªçŸ¥')}")
            print(f"  ç½®ä¿¡åº¦: {cls.get('confidence', 'æœªçŸ¥')}")
            if cls.get("reason"):
                print(f"  åŸå› : {cls.get('reason', '')}")
            print("-" * 50)
        
        if result.get("images"):
            print(f"\nå›¾ç‰‡ URL ({len(result['images'])} å¼ ):")
            for i, img in enumerate(result["images"], 1):
                print(f"  {i}. {img}")
        
        if result.get("downloaded_images"):
            print(f"\nå·²ä¸‹è½½å›¾ç‰‡:")
            for img_path in result["downloaded_images"]:
                print(f"  - {img_path}")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = os.path.join(output_dir, f"tweet_{extract_tweet_id(url)}_content.txt")
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"URL: {url}\n")
            if result.get("user"):
                f.write(f"User: {result['user'].get('name', '')} (@{result['user'].get('screen_name', '')})\n")
            if result.get("created_at"):
                f.write(f"Time: {result['created_at']}\n")
            
            # ä¿å­˜äº’åŠ¨ç»Ÿè®¡
            if result.get("stats"):
                stats = result["stats"]
                f.write(f"\nStats:\n")
                f.write(f"  Replies: {stats.get('replies', 0)}\n")
                f.write(f"  Retweets: {stats.get('retweets', 0)}\n")
                f.write(f"  Likes: {stats.get('likes', 0)}\n")
                f.write(f"  Bookmarks: {stats.get('bookmarks', 0)}\n")
                f.write(f"  Views: {stats.get('views', 0)}\n")
            
            f.write(f"\nText:\n{result.get('text', '')}\n")
            
            # ä¿å­˜æå–çš„æç¤ºè¯
            if result.get("extracted_prompt"):
                f.write(f"\nExtracted Prompt:\n{result['extracted_prompt']}\n")
            
            # ä¿å­˜åˆ†ç±»ç»“æœ
            if result.get("classification"):
                cls = result["classification"]
                f.write(f"\nClassification:\n")
                if cls.get("title"):
                    f.write(f"  Title: {cls.get('title')}\n")
                f.write(f"  Category: {cls.get('category', 'Unknown')}\n")
                if cls.get("sub_categories"):
                    f.write(f"  Sub-categories: {', '.join(cls['sub_categories'])}\n")
                if cls.get("style"):
                    f.write(f"  Style: {cls.get('style', 'Unknown')}\n")
                f.write(f"  Confidence: {cls.get('confidence', 'Unknown')}\n")
                if cls.get("reason"):
                    f.write(f"  Reason: {cls.get('reason', '')}\n")
            
            if result.get("images"):
                f.write(f"\nImages:\n")
                for img in result["images"]:
                    f.write(f"  {img}\n")
        print(f"\nâœ“ å†…å®¹å·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

