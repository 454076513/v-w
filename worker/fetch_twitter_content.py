#!/usr/bin/env python3
"""
Twitter/X Content Fetcher
è·å– Twitter/X æ¨æ–‡çš„æ­£æ–‡å†…å®¹å’Œå›¾ç‰‡

ä½¿ç”¨æ–¹æ³•:
    python fetch_twitter_content.py <tweet_url>

ç¤ºä¾‹:
    python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path

import requests

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
    elif env_file.exists():
        load_dotenv(env_file)
except ImportError:
    pass

# å¯é€‰ä¾èµ–
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False


# å¯¼å…¥å…¬ç”¨æ¨¡å— (ç»Ÿä¸€çš„ AI æå–å’Œåˆ†ç±»)
from prompt_utils import (
    extract_prompt,
    extract_prompt_regex,
    extract_prompt_simple,
    classify_prompt,
    detect_prompt_in_reply,
    detect_prompt_in_alt,
    call_ai,
    DEFAULT_MODEL,
)

# å‘åå…¼å®¹åˆ«å (ä¾›å…¶ä»–æ¨¡å—å¯¼å…¥ä½¿ç”¨)
extract_prompt_from_text = extract_prompt_regex  # æ­£åˆ™æå–
classify_prompt_with_ai = classify_prompt        # AIåˆ†ç±»


def extract_prompt_with_ai(text: str, model: str = DEFAULT_MODEL) -> str:
    """
    å‘åå…¼å®¹çš„ AI æå–å‡½æ•°

    ä½¿ç”¨ prompt_utils.extract_prompt çš„ç»Ÿä¸€æµç¨‹

    Returns:
        æå–çš„ prompt å­—ç¬¦ä¸²ï¼Œæˆ–:
        - "Prompt in ALT": prompt åœ¨å›¾ç‰‡ ALT æ–‡æœ¬ä¸­
        - "Prompt in reply": prompt åœ¨è¯„è®ºä¸­
        - "No prompt found": æœªæ‰¾åˆ° prompt
        - "Advertisement": å†…å®¹æ˜¯å¹¿å‘Š/æ¨å¹¿
    """
    result = extract_prompt(text, model=model, use_ai=True)

    if result["prompt"] == "Advertisement":
        return "Advertisement"

    if result["location"] == "alt":
        return "Prompt in ALT"

    if result["location"] == "reply":
        return "Prompt in reply"

    if result["prompt"]:
        return result["prompt"]

    return "No prompt found"

# Twitter Cookies é…ç½® (ç”¨äºè·å–è¯„è®º)
COOKIES_FILE = Path(__file__).parent / "x_cookies.json"
X_COOKIE = os.environ.get("X_COOKIE", "")  # JSON å­—ç¬¦ä¸²: '{"auth_token": "xxx", "ct0": "xxx"}'


def _load_twitter_cookies() -> dict:
    """åŠ è½½ Twitter cookies"""
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    if X_COOKIE:
        try:
            return json.loads(X_COOKIE)
        except:
            pass

    # ä»æ–‡ä»¶åŠ è½½
    if COOKIES_FILE.exists():
        try:
            with open(COOKIES_FILE) as f:
                return json.load(f)
        except:
            pass

    return {}


def fetch_author_replies(tweet_id: str, author_username: str) -> list:
    """
    ä½¿ç”¨ç‹¬ç«‹å­è¿›ç¨‹è·å–ä½œè€…å¯¹è‡ªå·±å¸–å­çš„å›å¤
    é€šè¿‡å­è¿›ç¨‹è°ƒç”¨é¿å…è¿æ¥æ± é—®é¢˜

    Args:
        tweet_id: æ¨æ–‡ ID
        author_username: åŸå§‹ä½œè€…ç”¨æˆ·å

    Returns:
        ä½œè€…å›å¤åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {"text": "...", "is_author": True}
    """
    # æ£€æŸ¥ cookies æ˜¯å¦å­˜åœ¨
    cookies = _load_twitter_cookies()
    if not cookies:
        print("      âš ï¸ æœªé…ç½® Twitter cookiesï¼Œæ— æ³•è·å–è¯„è®º")
        return []

    auth_token = cookies.get("auth_token", "")
    ct0 = cookies.get("ct0", "")

    if not auth_token or not ct0:
        print("      âš ï¸ Twitter cookies ç¼ºå°‘ auth_token æˆ– ct0")
        return []

    # ä½¿ç”¨å­è¿›ç¨‹è°ƒç”¨ç‹¬ç«‹è„šæœ¬ï¼Œé¿å…è¿æ¥æ± é—®é¢˜
    script_path = Path(__file__).parent / "fetch_replies.py"

    try:
        result = subprocess.run(
            [sys.executable, str(script_path), tweet_id, author_username],
            capture_output=True,
            text=True,
            timeout=60
        )

        if result.returncode == 0 and result.stdout.strip():
            replies = json.loads(result.stdout.strip())
            return replies
        else:
            if result.stderr:
                print(f"      âš ï¸ å­è¿›ç¨‹é”™è¯¯: {result.stderr[:200]}")
            return []

    except subprocess.TimeoutExpired:
        print("      âš ï¸ è·å–è¯„è®ºè¶…æ—¶")
        return []
    except json.JSONDecodeError as e:
        print(f"      âš ï¸ è§£æå›å¤å¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"      âš ï¸ è·å–è¯„è®ºå¤±è´¥: {e}")
        return []




def extract_tweet_id(url: str) -> str:
    """ä» URL ä¸­æå–æ¨æ–‡ ID"""
    # æ”¯æŒ twitter.com å’Œ x.com
    pattern = r'(?:twitter\.com|x\.com)/\w+/status/(\d+)'
    match = re.search(pattern, url)
    if match:
        return match.group(1)
    raise ValueError(f"æ— æ³•ä» URL ä¸­æå–æ¨æ–‡ ID: {url}")


def extract_username(url: str) -> str:
    """ä» URL ä¸­æå–ç”¨æˆ·å"""
    pattern = r'(?:twitter\.com|x\.com)/(\w+)/status/\d+'
    match = re.search(pattern, url)
    if match:
        return match.group(1)
    raise ValueError(f"æ— æ³•ä» URL ä¸­æå–ç”¨æˆ·å: {url}")


def fetch_with_syndication_api(tweet_id: str) -> dict:
    """
    ä½¿ç”¨ Twitter Syndication API è·å–æ¨æ–‡å†…å®¹
    è¿™æ˜¯å…¬å¼€ APIï¼Œä¸éœ€è¦è®¤è¯
    """
    url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&token=0"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Syndication API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_fxtwitter(tweet_id: str, username: str) -> dict:
    """
    ä½¿ç”¨ FxTwitter/FixupX API è·å–æ¨æ–‡å†…å®¹
    è¿™æ˜¯ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œæä¾›æ›´å¥½çš„åµŒå…¥ä½“éªŒ
    """
    url = f"https://api.fxtwitter.com/{username}/status/{tweet_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TwitterBot/1.0)',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"FxTwitter API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_vxtwitter(tweet_id: str, username: str) -> dict:
    """
    ä½¿ç”¨ VxTwitter API è·å–æ¨æ–‡å†…å®¹
    """
    url = f"https://api.vxtwitter.com/{username}/status/{tweet_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TwitterBot/1.0)',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"VxTwitter API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_playwright(url: str) -> dict:
    """
    ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–æ¨æ–‡å†…å®¹
    éœ€è¦å®‰è£…: pip install playwright && playwright install chromium
    """
    if not HAS_PLAYWRIGHT:
        raise ImportError("Playwright æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install playwright && playwright install chromium")
    
    result = {"text": None, "images": []}
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        page = context.new_page()
        
        try:
            page.goto(url, wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(3000)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            # è·å–æ¨æ–‡æ–‡æœ¬
            tweet_text_selectors = [
                'article[data-testid="tweet"] div[data-testid="tweetText"]',
                'article div[lang]',
                '[data-testid="tweetText"]',
            ]
            
            for selector in tweet_text_selectors:
                try:
                    text_element = page.query_selector(selector)
                    if text_element:
                        result["text"] = text_element.inner_text()
                        break
                except:
                    continue
            
            # è·å–å›¾ç‰‡
            image_selectors = [
                'article[data-testid="tweet"] img[src*="pbs.twimg.com/media"]',
                'article img[src*="twimg.com/media"]',
                '[data-testid="tweetPhoto"] img',
            ]
            
            for selector in image_selectors:
                try:
                    images = page.query_selector_all(selector)
                    for img in images:
                        src = img.get_attribute('src')
                        if src and 'pbs.twimg.com' in src:
                            # è·å–é«˜æ¸…ç‰ˆæœ¬
                            high_res = re.sub(r'\?.*$', '?format=jpg&name=large', src)
                            if high_res not in result["images"]:
                                result["images"].append(high_res)
                except:
                    continue
            
        finally:
            browser.close()
    
    return result


def download_image(url: str, save_path: str) -> str:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    }
    
    response = requests.get(url, headers=headers, timeout=30, stream=True)
    
    if response.status_code == 200:
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return save_path
    else:
        raise Exception(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {response.status_code}")


def parse_syndication_result(data: dict) -> dict:
    """è§£æ Syndication API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    if "text" in data:
        result["text"] = data["text"]
    
    if "user" in data:
        result["user"] = {
            "name": data["user"].get("name", ""),
            "screen_name": data["user"].get("screen_name", ""),
        }
    
    if "created_at" in data:
        result["created_at"] = data["created_at"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": data.get("reply_count", 0),
        "retweets": data.get("retweet_count", 0),
        "likes": data.get("favorite_count", 0),
        "bookmarks": data.get("bookmark_count", 0),
        "views": data.get("views_count", 0),
    }
    
    # æå–åª’ä½“
    if "mediaDetails" in data:
        for media in data["mediaDetails"]:
            if media.get("type") == "photo":
                result["images"].append(media.get("media_url_https", ""))
    
    # ä¹Ÿæ£€æŸ¥ photos å­—æ®µ
    if "photos" in data:
        for photo in data["photos"]:
            url = photo.get("url", "")
            if url and url not in result["images"]:
                result["images"].append(url)
    
    return result


def parse_fxtwitter_result(data: dict) -> dict:
    """è§£æ FxTwitter API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "image_alt_texts": [],  # å›¾ç‰‡ ALT æ–‡æœ¬åˆ—è¡¨
        "user": {},
        "created_at": "",
        "stats": {},
    }

    tweet = data.get("tweet", {})

    if "text" in tweet:
        result["text"] = tweet["text"]

    if "author" in tweet:
        result["user"] = {
            "name": tweet["author"].get("name", ""),
            "screen_name": tweet["author"].get("screen_name", ""),
        }

    if "created_at" in tweet:
        result["created_at"] = tweet["created_at"]

    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": tweet.get("replies", 0),
        "retweets": tweet.get("retweets", 0),
        "likes": tweet.get("likes", 0),
        "bookmarks": tweet.get("bookmarks", 0),
        "views": tweet.get("views", 0),
    }

    # æå–åª’ä½“ (åŒ…æ‹¬ ALT æ–‡æœ¬)
    if "media" in tweet and "photos" in tweet["media"]:
        for photo in tweet["media"]["photos"]:
            result["images"].append(photo.get("url", ""))
            alt_text = photo.get("altText", "")
            if alt_text:
                result["image_alt_texts"].append(alt_text)

    return result


def parse_vxtwitter_result(data: dict) -> dict:
    """è§£æ VxTwitter API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "image_alt_texts": [],  # å›¾ç‰‡ ALT æ–‡æœ¬åˆ—è¡¨
        "user": {},
        "created_at": "",
        "stats": {},
    }

    if "text" in data:
        result["text"] = data["text"]

    result["user"] = {
        "name": data.get("user_name", ""),
        "screen_name": data.get("user_screen_name", ""),
    }

    if "date" in data:
        result["created_at"] = data["date"]

    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": data.get("replies", 0),
        "retweets": data.get("retweets", 0),
        "likes": data.get("likes", 0),
        "bookmarks": data.get("bookmarks", 0),
        "views": data.get("views", 0),
    }

    # æå–åª’ä½“ (åŒ…æ‹¬ ALT æ–‡æœ¬)
    if "media_extended" in data:
        for media in data["media_extended"]:
            if media.get("type") == "image":
                result["images"].append(media.get("url", ""))
                alt_text = media.get("altText", "")
                if alt_text:
                    result["image_alt_texts"].append(alt_text)

    return result


def fetch_tweet(url: str, download_images: bool = True, output_dir: str = ".",
                extract_prompt: bool = False, ai_model: str = DEFAULT_MODEL,
                detect_ads: bool = True) -> dict:
    """
    è·å–æ¨æ–‡å†…å®¹çš„ä¸»å‡½æ•°
    ä¼šä¾æ¬¡å°è¯•ä¸åŒçš„æ–¹æ³•ç›´åˆ°æˆåŠŸ

    Args:
        url: æ¨æ–‡ URL
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        output_dir: è¾“å‡ºç›®å½•
        extract_prompt: æ˜¯å¦ä½¿ç”¨ AI æå–æç¤ºè¯
        ai_model: AI æ¨¡å‹åç§° (openai, deepseek ç­‰)
        detect_ads: æ˜¯å¦æ£€æµ‹å¹¿å‘Šå†…å®¹ (é»˜è®¤ True)

    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µ:
            - text: æ¨æ–‡æ–‡æœ¬
            - images: å›¾ç‰‡ URL åˆ—è¡¨
            - is_advertisement: æ˜¯å¦ä¸ºå¹¿å‘Š (ä»…å½“ detect_ads=True)
            - prompt_location: "post" | "reply" | "advertisement"
            - extracted_prompt: æå–çš„ prompt (ä»…å½“ extract_prompt=True)
            - ...
    """
    start_time = datetime.now()
    
    # è§£æ URL
    try:
        tweet_id = extract_tweet_id(url)
        username = extract_username(url)
    except Exception as e:
        print(f"âŒ [FAILED] æ— æ•ˆçš„ Twitter URL: {url}")
        print(f"   é”™è¯¯: {e}")
        raise Exception(f"æ— æ•ˆçš„ Twitter URL: {url}")
    
    print()
    print("=" * 70)
    print(f"ğŸ¦ å¼€å§‹å¤„ç†æ¨æ–‡")
    print(f"   URL: {url}")
    print(f"   æ¨æ–‡ ID: {tweet_id}")
    print(f"   ç”¨æˆ·å: @{username}")
    print(f"   æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    result = None
    fetch_method = None
    fetch_errors = []
    
    # æ–¹æ³• 1: å°è¯• FxTwitter API
    print("   [1/4] å°è¯• FxTwitter API...")
    try:
        data = fetch_with_fxtwitter(tweet_id, username)
        result = parse_fxtwitter_result(data)
        if result and result.get("text"):
            fetch_method = "FxTwitter"
            print("   âœ“ FxTwitter API æˆåŠŸ")
        else:
            raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
    except Exception as e:
        fetch_errors.append(f"FxTwitter: {e}")
        print(f"   âœ— FxTwitter API å¤±è´¥: {e}")
    
    # æ–¹æ³• 2: å°è¯• VxTwitter API
    if not result or not result.get("text"):
        print("   [2/4] å°è¯• VxTwitter API...")
        try:
            data = fetch_with_vxtwitter(tweet_id, username)
            result = parse_vxtwitter_result(data)
            if result and result.get("text"):
                fetch_method = "VxTwitter"
                print("   âœ“ VxTwitter API æˆåŠŸ")
            else:
                raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
        except Exception as e:
            fetch_errors.append(f"VxTwitter: {e}")
            print(f"   âœ— VxTwitter API å¤±è´¥: {e}")
    
    # æ–¹æ³• 3: å°è¯• Twitter Syndication API
    if not result or not result.get("text"):
        print("   [3/4] å°è¯• Syndication API...")
        try:
            data = fetch_with_syndication_api(tweet_id)
            result = parse_syndication_result(data)
            if result and result.get("text"):
                fetch_method = "Syndication"
                print("   âœ“ Syndication API æˆåŠŸ")
            else:
                raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
        except Exception as e:
            fetch_errors.append(f"Syndication: {e}")
            print(f"   âœ— Syndication API å¤±è´¥: {e}")
    
    # æ–¹æ³• 4: å°è¯• Playwright (éœ€è¦å®‰è£…)
    if not result or not result.get("text"):
        if HAS_PLAYWRIGHT:
            print("   [4/4] å°è¯• Playwright æµè§ˆå™¨...")
            try:
                result = fetch_with_playwright(url)
                if result and result.get("text"):
                    fetch_method = "Playwright"
                    print("   âœ“ Playwright æˆåŠŸ")
                else:
                    raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
            except Exception as e:
                fetch_errors.append(f"Playwright: {e}")
                print(f"   âœ— Playwright å¤±è´¥: {e}")
        else:
            print("   [4/4] è·³è¿‡ Playwright (æœªå®‰è£…)")
    
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å†…å®¹
    if not result or not result.get("text"):
        elapsed = (datetime.now() - start_time).total_seconds()
        print()
        print(f"âŒ [FAILED] æ¨æ–‡è·å–å¤±è´¥: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è€—æ—¶: {elapsed:.1f}s")
        print(f"   å°è¯•çš„æ–¹æ³•åŠé”™è¯¯:")
        for err in fetch_errors:
            print(f"      - {err}")
        print("=" * 70)
        raise Exception(f"æ‰€æœ‰è·å–æ–¹æ³•éƒ½å¤±è´¥äº†: {url}")
    
    print()
    print(f"   âœ“ å†…å®¹è·å–æˆåŠŸ (via {fetch_method})")
    text_preview = result.get("text", "")[:100].replace("\n", " ")
    print(f"   ğŸ“ å†…å®¹é¢„è§ˆ: {text_preview}...")
    
    # ä¸‹è½½å›¾ç‰‡
    if download_images and result.get("images"):
        print()
        print(f"   ğŸ–¼ï¸  å‘ç° {len(result['images'])} å¼ å›¾ç‰‡")
        
        os.makedirs(output_dir, exist_ok=True)
        downloaded_images = []
        
        for i, img_url in enumerate(result["images"]):
            # è·å–é«˜æ¸…ç‰ˆæœ¬
            if "?" in img_url:
                high_res_url = re.sub(r'name=\w+', 'name=large', img_url)
            else:
                high_res_url = img_url + "?format=jpg&name=large"
            
            filename = f"tweet_{tweet_id}_image_{i+1}.jpg"
            filepath = os.path.join(output_dir, filename)
            
            try:
                download_image(high_res_url, filepath)
                downloaded_images.append(filepath)
                print(f"      âœ“ å›¾ç‰‡ {i+1}: {filename}")
            except Exception as e:
                print(f"      âœ— å›¾ç‰‡ {i+1} ä¸‹è½½å¤±è´¥: {e}")
        
        result["downloaded_images"] = downloaded_images

    # åˆå§‹åŒ–å¹¿å‘Šæ ‡è®°
    result["is_advertisement"] = False

    # ç‹¬ç«‹çš„å¹¿å‘Šæ£€æµ‹ï¼ˆå½“ extract_prompt=False ä½† detect_ads=True æ—¶ï¼‰
    if detect_ads and not extract_prompt and result.get("text"):
        print()
        print(f"   ğŸ” å¹¿å‘Šæ£€æµ‹...")
        try:
            ad_check = extract_prompt_with_ai(result["text"], model=ai_model)
            if ad_check == "Advertisement":
                print(f"      ğŸš« æ£€æµ‹åˆ°å¹¿å‘Š/æ¨å¹¿å†…å®¹")
                result["is_advertisement"] = True
                result["prompt_location"] = "advertisement"
            else:
                print(f"      âœ“ éå¹¿å‘Šå†…å®¹")
        except Exception as e:
            print(f"      âš ï¸ å¹¿å‘Šæ£€æµ‹å¤±è´¥: {e}")

    # ä½¿ç”¨ AI æå–æç¤ºè¯
    if extract_prompt and result.get("text"):
        print()
        print(f"   ğŸ¤– AI å¤„ç† (æ¨¡å‹: {ai_model})")

        # å…ˆæ£€æµ‹ prompt ä½ç½®
        prompt_in_alt = detect_prompt_in_alt(result["text"])
        prompt_in_reply = detect_prompt_in_reply(result["text"])

        # ä¼˜å…ˆæ£€æŸ¥ ALT æ–‡æœ¬
        if prompt_in_alt and result.get("image_alt_texts"):
            print(f"      âœ“ æ£€æµ‹åˆ° prompt åœ¨ ALT æ–‡æœ¬ä¸­")
            result["prompt_location"] = "alt"

            # ç›´æ¥ä» ALT æ–‡æœ¬è·å–æç¤ºè¯ (é€šå¸¸ç¬¬ä¸€ä¸ª ALT å°±æ˜¯)
            alt_prompt = result["image_alt_texts"][0]
            result["extracted_prompt"] = alt_prompt
            prompt_preview = alt_prompt[:80].replace("\n", " ")
            print(f"      âœ“ ä» ALT æ–‡æœ¬æå–æˆåŠŸ: {prompt_preview}...")

            # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
            print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
            try:
                classification = classify_prompt_with_ai(alt_prompt, model=ai_model)
                result["classification"] = classification

                title = classification.get("title", "æœªçŸ¥")
                category = classification.get("category", "æœªçŸ¥")
                confidence = classification.get("confidence", "æœªçŸ¥")
                print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
            except Exception as e:
                print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                result["classification"] = None

            # è·³è¿‡åç»­å¤„ç†
            extracted_prompt = alt_prompt

        elif prompt_in_reply:
            print(f"      âš ï¸ æ£€æµ‹åˆ° prompt å¯èƒ½åœ¨è¯„è®º/å›å¤ä¸­")
            result["prompt_location"] = "reply"
            extracted_prompt = None  # å°†åœ¨ä¸‹é¢å¤„ç†
        else:
            result["prompt_location"] = "post"
            extracted_prompt = None  # å°†åœ¨ä¸‹é¢å¤„ç†

        # å¦‚æœä¸æ˜¯ä» ALT æå–çš„ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        if result.get("prompt_location") != "alt":
            # æå–æç¤ºè¯
            print(f"      [1/2] æå–æç¤ºè¯...")
            try:
                extracted_prompt = extract_prompt_with_ai(result["text"], model=ai_model)
                result["extracted_prompt"] = extracted_prompt

                # å¤„ç†ä¸åŒçš„æå–ç»“æœ
                if extracted_prompt == "Advertisement":
                    print(f"      ğŸš« æ£€æµ‹åˆ°å¹¿å‘Š/æ¨å¹¿å†…å®¹ï¼Œè·³è¿‡")
                    result["is_advertisement"] = True
                    result["prompt_location"] = "advertisement"
                    result["classification"] = None
                elif extracted_prompt == "Prompt in ALT":
                    # AI è¯†åˆ«åˆ°æç¤ºè¯åœ¨ ALT ä¸­ï¼Œä» ALT æ–‡æœ¬è·å–
                    print(f"      âœ“ AI æ£€æµ‹åˆ° prompt åœ¨ ALT æ–‡æœ¬ä¸­")
                    if result.get("image_alt_texts"):
                        alt_prompt = result["image_alt_texts"][0]
                        result["extracted_prompt"] = alt_prompt
                        result["prompt_location"] = "alt"
                        prompt_preview = alt_prompt[:80].replace("\n", " ")
                        print(f"      âœ“ ä» ALT æ–‡æœ¬æå–æˆåŠŸ: {prompt_preview}...")

                        # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
                        print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
                        try:
                            classification = classify_prompt_with_ai(alt_prompt, model=ai_model)
                            result["classification"] = classification

                            title = classification.get("title", "æœªçŸ¥")
                            category = classification.get("category", "æœªçŸ¥")
                            confidence = classification.get("confidence", "æœªçŸ¥")
                            print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
                        except Exception as e:
                            print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                            result["classification"] = None
                    else:
                        print(f"      âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ ALT æ–‡æœ¬")
                        result["classification"] = None
                elif extracted_prompt == "Prompt in reply":
                    print(f"      âš ï¸ Prompt åœ¨è¯„è®º/å›å¤ä¸­ï¼Œå°è¯•è·å–ä½œè€…å›å¤...")
                    result["prompt_location"] = "reply"

                    # å°è¯•è·å–ä½œè€…çš„å›å¤
                    # ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„å®é™…ä½œè€…ç”¨æˆ·åï¼ˆURL ä¸­çš„ç”¨æˆ·åå¯èƒ½ä¸å‡†ç¡®ï¼‰
                    actual_author = result.get("user", {}).get("screen_name", username)
                    if actual_author != username:
                        print(f"      â„¹ï¸ å®é™…ä½œè€…: @{actual_author} (URL ä¸­: @{username})")
                    author_replies = fetch_author_replies(tweet_id, actual_author)
                    if author_replies:
                        print(f"      âœ“ è·å–åˆ° {len(author_replies)} æ¡ä½œè€…å›å¤")

                        # åˆå¹¶æ‰€æœ‰ä½œè€…å›å¤ï¼Œä»ä¸­æå– prompt
                        combined_reply_text = "\n\n".join([r["text"] for r in author_replies])
                        result["author_replies"] = author_replies

                        # å°è¯•ä»å›å¤ä¸­æå– prompt
                        print(f"      [1.5/2] ä»ä½œè€…å›å¤ä¸­æå–æç¤ºè¯...")

                        # é¦–å…ˆå°è¯•æ­£åˆ™è¡¨è¾¾å¼æå– (æ›´å¿«æ›´å¯é )
                        reply_prompt = None
                        for reply in author_replies:
                            reply_prompt = extract_prompt_from_text(reply["text"])
                            if reply_prompt:
                                print(f"      âœ“ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æˆåŠŸ")
                                break

                        # å¦‚æœæ­£åˆ™æ²¡æœ‰æå–åˆ°ï¼Œå°è¯• AI æå–
                        if not reply_prompt:
                            print(f"      â„¹ï¸ æ­£åˆ™æœªåŒ¹é…ï¼Œå°è¯• AI æå–...")
                            try:
                                # ç›´æ¥è°ƒç”¨ AI æå–ï¼Œä¸å†æ£€æµ‹ "Prompt in reply"
                                reply_prompt = call_ai([
                                    {
                                        "role": "system",
                                        "content": "You are a helpful assistant that extracts AI image generation prompts from text. Extract only the prompt itself, without any additional explanation or formatting. If no prompt is found, return 'No prompt found'."
                                    },
                                    {
                                        "role": "user",
                                        "content": f"Extract the AI image generation prompt from this text and return only the prompt itself:\n\n{combined_reply_text}"
                                    }
                                ], ai_model)

                                if reply_prompt == "No prompt found":
                                    reply_prompt = None
                            except Exception as e:
                                print(f"      âš ï¸ AI æå–å¤±è´¥: {e}")
                                reply_prompt = None

                        if reply_prompt:
                            extracted_prompt = reply_prompt
                            result["extracted_prompt"] = extracted_prompt
                            result["prompt_location"] = "reply"  # æ ‡è®°æ˜¯ä»å›å¤ä¸­æå–çš„
                            prompt_preview = extracted_prompt[:80].replace("\n", " ")
                            print(f"      âœ“ ä»å›å¤ä¸­æå–æˆåŠŸ: {prompt_preview}...")

                            # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
                            print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
                            try:
                                classification = classify_prompt_with_ai(extracted_prompt, model=ai_model)
                                result["classification"] = classification

                                title = classification.get("title", "æœªçŸ¥")
                                category = classification.get("category", "æœªçŸ¥")
                                confidence = classification.get("confidence", "æœªçŸ¥")
                                print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
                            except Exception as e:
                                print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                                result["classification"] = None
                        else:
                            print(f"      âš ï¸ ä½œè€…å›å¤ä¸­ä¹Ÿæœªæ‰¾åˆ°æç¤ºè¯")
                            result["classification"] = None
                    else:
                        print(f"      âš ï¸ æœªè·å–åˆ°ä½œè€…å›å¤ (å¯èƒ½éœ€è¦é…ç½® cookies)")
                        result["classification"] = None
                elif extracted_prompt and extracted_prompt != "No prompt found":
                    prompt_preview = extracted_prompt[:80].replace("\n", " ")
                    print(f"      âœ“ æå–æˆåŠŸ: {prompt_preview}...")
                    result["prompt_location"] = "post"

                    # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
                    print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
                    try:
                        classification = classify_prompt_with_ai(extracted_prompt, model=ai_model)
                        result["classification"] = classification

                        title = classification.get("title", "æœªçŸ¥")
                        category = classification.get("category", "æœªçŸ¥")
                        confidence = classification.get("confidence", "æœªçŸ¥")
                        print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
                    except Exception as e:
                        print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                        result["classification"] = None
                else:
                    print(f"      âš ï¸ æœªæ‰¾åˆ°æç¤ºè¯")
                    result["classification"] = None
            except Exception as e:
                print(f"      âœ— æå–å¤±è´¥: {e}")
                result["extracted_prompt"] = None
                result["classification"] = None

    # å®Œæˆ
    elapsed = (datetime.now() - start_time).total_seconds()
    print()
    prompt_location = result.get("prompt_location", "unknown")
    extracted_prompt = result.get("extracted_prompt", "")

    # åˆ¤æ–­æ˜¯å¦æˆåŠŸæå–äº† prompt (å³ä½¿æ˜¯ä»è¯„è®ºä¸­æå–çš„)
    has_valid_prompt = extracted_prompt and extracted_prompt not in ["No prompt found", "Prompt in reply", "Advertisement"]

    if prompt_location == "advertisement":
        print(f"ğŸš« [ADVERTISEMENT] æ¨æ–‡ä¸ºå¹¿å‘Šå†…å®¹ï¼Œå·²è·³è¿‡: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è·å–æ–¹å¼: {fetch_method}")
        print(f"   è€—æ—¶: {elapsed:.1f}s")
        print("=" * 70)
        return result

    if has_valid_prompt:
        if prompt_location == "alt":
            print(f"âœ… [SUCCESS_FROM_ALT] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
            print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
            print(f"   è·å–æ–¹å¼: {fetch_method}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
            print(f"   æç¤ºè¯: å·²ä»å›¾ç‰‡ ALT æ–‡æœ¬æå–")
        elif prompt_location == "reply":
            print(f"âœ… [SUCCESS_FROM_REPLY] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
            print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
            print(f"   è·å–æ–¹å¼: {fetch_method}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
            print(f"   æç¤ºè¯: å·²ä»è¯„è®ºä¸­æå–")
        else:
            print(f"âœ… [SUCCESS] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
            print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
            print(f"   è·å–æ–¹å¼: {fetch_method}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
            print(f"   æç¤ºè¯: å·²æå–")
        if result.get("classification"):
            print(f"   åˆ†ç±»: {result['classification'].get('category', 'æœªçŸ¥')}")
    elif prompt_location == "reply":
        print(f"âš ï¸ [PROMPT_IN_REPLY] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è·å–æ–¹å¼: {fetch_method}")
        print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
        print(f"   æç¤ºè¯ä½ç½®: è¯„è®º/å›å¤ä¸­ (æœªèƒ½æå–)")
    else:
        print(f"âš ï¸ [NO_PROMPT] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è·å–æ–¹å¼: {fetch_method}")
        print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
        print(f"   æç¤ºè¯: æœªæ‰¾åˆ°")
    print(f"   è€—æ—¶: {elapsed:.1f}s")
    print("=" * 70)

    return result


def main():
    parser = argparse.ArgumentParser(
        description="Twitter/X å†…å®¹è·å–å·¥å…· - è·å–æ¨æ–‡æ­£æ–‡ã€å›¾ç‰‡å’Œäº’åŠ¨ç»Ÿè®¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³• (é»˜è®¤å¯ç”¨æç¤ºè¯æå–å’Œåˆ†ç±»)
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637
  
  # ä½¿ç”¨ deepseek æ¨¡å‹
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 --model deepseek
  
  # ç¦ç”¨æç¤ºè¯æå–
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 --no-extract
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 -o ./output
        """
    )
    
    parser.add_argument("url", help="æ¨æ–‡ URL (æ”¯æŒ x.com å’Œ twitter.com)")
    parser.add_argument("-o", "--output", default=".", help="è¾“å‡ºç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)")
    parser.add_argument("--no-extract", action="store_true", 
                        help="ç¦ç”¨ AI æå–æç¤ºè¯å’Œåˆ†ç±» (é»˜è®¤å¯ç”¨)")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL,
                        help=f"AI æ¨¡å‹ (é»˜è®¤: {DEFAULT_MODEL}, å¯é€‰: deepseek, openai ç­‰)")
    parser.add_argument("--no-download", action="store_true", help="ä¸ä¸‹è½½å›¾ç‰‡")
    
    args = parser.parse_args()
    
    url = args.url
    output_dir = args.output
    
    extract_prompt = not args.no_extract  # é»˜è®¤å¯ç”¨æå–æç¤ºè¯
    
    print("=" * 50)
    print("Twitter/X å†…å®¹è·å–å·¥å…·")
    print("=" * 50)
    print(f"URL: {url}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    if extract_prompt:
        print(f"AI æ¨¡å‹: {args.model}")
    print("=" * 50)
    
    try:
        result = fetch_tweet(
            url, 
            download_images=not args.no_download, 
            output_dir=output_dir,
            extract_prompt=extract_prompt,
            ai_model=args.model
        )
        
        print("\n" + "=" * 50)
        print("è·å–ç»“æœ")
        print("=" * 50)
        
        if result.get("user"):
            user = result["user"]
            print(f"ç”¨æˆ·: {user.get('name', '')} (@{user.get('screen_name', '')})")
        
        if result.get("created_at"):
            print(f"æ—¶é—´: {result['created_at']}")
        
        # æ˜¾ç¤ºäº’åŠ¨ç»Ÿè®¡
        if result.get("stats"):
            stats = result["stats"]
            print(f"\näº’åŠ¨ç»Ÿè®¡:")
            print(f"  ğŸ’¬ è¯„è®ºæ•° (Replies): {stats.get('replies', 0)}")
            print(f"  ğŸ” è½¬å‘æ•° (Retweets): {stats.get('retweets', 0)}")
            print(f"  â¤ï¸  ç‚¹èµæ•° (Likes): {stats.get('likes', 0)}")
            print(f"  ğŸ”– ä¹¦ç­¾æ•° (Bookmarks): {stats.get('bookmarks', 0)}")
            print(f"  ğŸ‘ï¸  æµè§ˆé‡ (Views): {stats.get('views', 0)}")
        
        print(f"\næ­£æ–‡å†…å®¹:")
        print("-" * 50)
        print(result.get("text", "æ— æ³•è·å–"))
        print("-" * 50)
        
        # æ˜¾ç¤ºæå–çš„æç¤ºè¯
        if result.get("extracted_prompt"):
            print(f"\nğŸ¨ æå–çš„æç¤ºè¯:")
            print("-" * 50)
            print(result["extracted_prompt"])
            print("-" * 50)
        
        # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
        if result.get("classification"):
            cls = result["classification"]
            print(f"\nğŸ“‚ æç¤ºè¯åˆ†ç±»:")
            print("-" * 50)
            if cls.get("title"):
                print(f"  ğŸ“Œ æ ‡é¢˜: {cls.get('title')}")
            print(f"  ä¸»åˆ†ç±»: {cls.get('category', 'æœªçŸ¥')}")
            if cls.get("sub_categories"):
                print(f"  æ¬¡åˆ†ç±»: {', '.join(cls['sub_categories'])}")
            if cls.get("style"):
                print(f"  é£æ ¼: {cls.get('style', 'æœªçŸ¥')}")
            print(f"  ç½®ä¿¡åº¦: {cls.get('confidence', 'æœªçŸ¥')}")
            if cls.get("reason"):
                print(f"  åŸå› : {cls.get('reason', '')}")
            print("-" * 50)
        
        if result.get("images"):
            print(f"\nå›¾ç‰‡ URL ({len(result['images'])} å¼ ):")
            for i, img in enumerate(result["images"], 1):
                print(f"  {i}. {img}")
        
        if result.get("downloaded_images"):
            print(f"\nå·²ä¸‹è½½å›¾ç‰‡:")
            for img_path in result["downloaded_images"]:
                print(f"  - {img_path}")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = os.path.join(output_dir, f"tweet_{extract_tweet_id(url)}_content.txt")
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"URL: {url}\n")
            if result.get("user"):
                f.write(f"User: {result['user'].get('name', '')} (@{result['user'].get('screen_name', '')})\n")
            if result.get("created_at"):
                f.write(f"Time: {result['created_at']}\n")
            
            # ä¿å­˜äº’åŠ¨ç»Ÿè®¡
            if result.get("stats"):
                stats = result["stats"]
                f.write(f"\nStats:\n")
                f.write(f"  Replies: {stats.get('replies', 0)}\n")
                f.write(f"  Retweets: {stats.get('retweets', 0)}\n")
                f.write(f"  Likes: {stats.get('likes', 0)}\n")
                f.write(f"  Bookmarks: {stats.get('bookmarks', 0)}\n")
                f.write(f"  Views: {stats.get('views', 0)}\n")
            
            f.write(f"\nText:\n{result.get('text', '')}\n")
            
            # ä¿å­˜æå–çš„æç¤ºè¯
            if result.get("extracted_prompt"):
                f.write(f"\nExtracted Prompt:\n{result['extracted_prompt']}\n")
            
            # ä¿å­˜åˆ†ç±»ç»“æœ
            if result.get("classification"):
                cls = result["classification"]
                f.write(f"\nClassification:\n")
                if cls.get("title"):
                    f.write(f"  Title: {cls.get('title')}\n")
                f.write(f"  Category: {cls.get('category', 'Unknown')}\n")
                if cls.get("sub_categories"):
                    f.write(f"  Sub-categories: {', '.join(cls['sub_categories'])}\n")
                if cls.get("style"):
                    f.write(f"  Style: {cls.get('style', 'Unknown')}\n")
                f.write(f"  Confidence: {cls.get('confidence', 'Unknown')}\n")
                if cls.get("reason"):
                    f.write(f"  Reason: {cls.get('reason', '')}\n")
            
            if result.get("images"):
                f.write(f"\nImages:\n")
                for img in result["images"]:
                    f.write(f"  {img}\n")
        print(f"\nâœ“ å†…å®¹å·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

