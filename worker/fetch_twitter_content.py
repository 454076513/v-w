#!/usr/bin/env python3
"""
Twitter/X Content Fetcher
è·å– Twitter/X æ¨æ–‡çš„æ­£æ–‡å†…å®¹å’Œå›¾ç‰‡

ä½¿ç”¨æ–¹æ³•:
    python fetch_twitter_content.py <tweet_url>

ç¤ºä¾‹:
    python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637
"""

import re
import sys
import os
import requests
from urllib.parse import urlparse
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
    elif env_file.exists():
        load_dotenv(env_file)
except ImportError:
    pass

# å¯é€‰ä¾èµ–
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False


# å¯¼å…¥å…¬ç”¨æ¨¡å—
from prompt_utils import (
    extract_prompt,
    extract_prompt_regex,
    extract_prompt_simple,
    classify_prompt,
    detect_prompt_in_reply,
    call_ai,
    DEFAULT_MODEL,
)

# Twitter Cookies é…ç½® (ç”¨äºè·å–è¯„è®º)
import json

COOKIES_FILE = Path(__file__).parent / "x_cookies.json"
X_COOKIE = os.environ.get("X_COOKIE", "")  # JSON å­—ç¬¦ä¸²: '{"auth_token": "xxx", "ct0": "xxx"}'


def _load_twitter_cookies() -> dict:
    """åŠ è½½ Twitter cookies"""
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    if X_COOKIE:
        try:
            return json.loads(X_COOKIE)
        except:
            pass

    # ä»æ–‡ä»¶åŠ è½½
    if COOKIES_FILE.exists():
        try:
            with open(COOKIES_FILE) as f:
                return json.load(f)
        except:
            pass

    return {}


# ========== ä»¥ä¸‹å‡½æ•°å·²ç§»è‡³ prompt_utils.py ==========
# - extract_prompt_from_text -> prompt_utils.extract_prompt_regex
# - detect_prompt_in_reply -> prompt_utils.detect_prompt_in_reply
# - extract_prompt_with_ai -> prompt_utils.extract_prompt
# - classify_prompt_with_ai -> prompt_utils.classify_prompt
# - _call_ai_with_fallback -> prompt_utils.call_ai
# - _call_pollinations_ai, _call_gitee_ai (å†…éƒ¨å‡½æ•°)


def fetch_author_replies(tweet_id: str, author_username: str) -> list:
    """
    ä½¿ç”¨ç‹¬ç«‹å­è¿›ç¨‹è·å–ä½œè€…å¯¹è‡ªå·±å¸–å­çš„å›å¤
    é€šè¿‡å­è¿›ç¨‹è°ƒç”¨é¿å…è¿æ¥æ± é—®é¢˜

    Args:
        tweet_id: æ¨æ–‡ ID
        author_username: åŸå§‹ä½œè€…ç”¨æˆ·å

    Returns:
        ä½œè€…å›å¤åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {"text": "...", "is_author": True}
    """
    import subprocess

    # æ£€æŸ¥ cookies æ˜¯å¦å­˜åœ¨
    cookies = _load_twitter_cookies()
    if not cookies:
        print("      âš ï¸ æœªé…ç½® Twitter cookiesï¼Œæ— æ³•è·å–è¯„è®º")
        return []

    auth_token = cookies.get("auth_token", "")
    ct0 = cookies.get("ct0", "")

    if not auth_token or not ct0:
        print("      âš ï¸ Twitter cookies ç¼ºå°‘ auth_token æˆ– ct0")
        return []

    # ä½¿ç”¨å­è¿›ç¨‹è°ƒç”¨ç‹¬ç«‹è„šæœ¬ï¼Œé¿å…è¿æ¥æ± é—®é¢˜
    script_path = Path(__file__).parent / "fetch_replies.py"

    try:
        result = subprocess.run(
            [sys.executable, str(script_path), tweet_id, author_username],
            capture_output=True,
            text=True,
            timeout=60
        )

        if result.returncode == 0 and result.stdout.strip():
            replies = json.loads(result.stdout.strip())
            return replies
        else:
            if result.stderr:
                print(f"      âš ï¸ å­è¿›ç¨‹é”™è¯¯: {result.stderr[:200]}")
            return []

    except subprocess.TimeoutExpired:
        print("      âš ï¸ è·å–è¯„è®ºè¶…æ—¶")
        return []
    except json.JSONDecodeError as e:
        print(f"      âš ï¸ è§£æå›å¤å¤±è´¥: {e}")
        return []
    except Exception as e:
        print(f"      âš ï¸ è·å–è¯„è®ºå¤±è´¥: {e}")
        return []


# æ£€æµ‹ "prompt åœ¨è¯„è®ºä¸­" çš„æŒ‡ç¤ºç¬¦æ¨¡å¼ (å·²ç§»è‡³ prompt_utils.py)
PROMPT_IN_REPLY_PATTERNS = [
    r'prompt\s*[ğŸ‘‡â¬‡ï¸â†“ğŸ”½]',          # "PromptğŸ‘‡", "prompt â¬‡ï¸"
    r'[ğŸ‘‡â¬‡ï¸â†“ğŸ”½]\s*prompt',          # "ğŸ‘‡prompt"
    r'prompt\s+below',              # "prompt below"
    r'prompt\s+in\s+(the\s+)?(comment|reply|replies|thread)',  # "prompt in comment"
    r'check\s+(the\s+)?(comment|reply|replies)',  # "check the comment"
    r'see\s+(the\s+)?(comment|reply|replies)',    # "see comment"
    r'(comment|reply|replies)\s+for\s+prompt',    # "comment for prompt"
    r'full\s+prompt\s+[ğŸ‘‡â¬‡ï¸â†“ğŸ”½]',   # "full prompt ğŸ‘‡"
    r'æç¤ºè¯\s*[ğŸ‘‡â¬‡ï¸â†“ğŸ”½]',           # ä¸­æ–‡: "æç¤ºè¯ğŸ‘‡"
    r'[ğŸ‘‡â¬‡ï¸â†“ğŸ”½]\s*æç¤ºè¯',           # ä¸­æ–‡: "ğŸ‘‡æç¤ºè¯"
]


def extract_prompt_from_text(text: str) -> str:
    """
    å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ–‡æœ¬ä¸­æå– prompt
    ç”¨äºå¿«é€Ÿæå–æ ¼å¼è§„èŒƒçš„ promptï¼Œé¿å… AI è°ƒç”¨

    Args:
        text: æ¨æ–‡æˆ–å›å¤æ–‡æœ¬

    Returns:
        æå–çš„ prompt æˆ– None
    """
    if not text:
        return None

    # å¸¸è§çš„ prompt å¼•å¯¼æ¨¡å¼
    patterns = [
        # ğŸ‘‰Prompt: ... æˆ– Prompt: ...
        r'(?:ğŸ‘‰\s*)?[Pp]rompt[:\s]+(.+)',
        # "prompt" åé¢è·Ÿç€æ¢è¡Œå’Œå†…å®¹
        r'[Pp]rompt\s*\n+(.+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            prompt = match.group(1).strip()
            # æ¸…ç†å¼€å¤´çš„å¼•å·ã€æ‹¬å·ç­‰
            prompt = re.sub(r'^[\"\'\[\(]+', '', prompt)
            # å¦‚æœ prompt è¶³å¤Ÿé•¿ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„
            if len(prompt) > 50:
                return prompt

    return None


def detect_prompt_in_reply(text: str) -> bool:
    """
    æ£€æµ‹æ¨æ–‡æ–‡æœ¬æ˜¯å¦è¡¨æ˜ prompt åœ¨è¯„è®º/å›å¤ä¸­

    Args:
        text: æ¨æ–‡æ­£æ–‡å†…å®¹

    Returns:
        True å¦‚æœæ£€æµ‹åˆ° prompt å¯èƒ½åœ¨è¯„è®ºä¸­
    """
    if not text:
        return False

    text_lower = text.lower()

    for pattern in PROMPT_IN_REPLY_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return True

    return False


def extract_prompt_with_ai(text: str, model: str = DEFAULT_MODEL) -> str:
    """
    ä½¿ç”¨ AI API ä»æ–‡æœ¬ä¸­æå–æç¤ºè¯
    ä¼˜å…ˆä½¿ç”¨ Pollinations AIï¼Œå¤±è´¥å fallback åˆ° Gitee AI (DeepSeek-V3)

    Args:
        text: æ¨æ–‡æ­£æ–‡å†…å®¹
        model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ openaiï¼Œå¯é€‰ deepseek

    Returns:
        æå–å‡ºçš„æç¤ºè¯ï¼Œå¦‚æœ prompt åœ¨è¯„è®ºä¸­è¿”å› 'Prompt in reply'
    """
    # é¦–å…ˆæ£€æµ‹æ˜¯å¦æ˜¯ "prompt åœ¨è¯„è®ºä¸­" çš„æƒ…å†µ
    if detect_prompt_in_reply(text):
        return "Prompt in reply"

    messages = [
        {
            "role": "system",
            "content": """You are a helpful assistant that extracts AI image generation prompts from text.

IMPORTANT RULES:
1. Extract only the actual prompt itself, without any additional explanation or formatting.
2. If the text contains indicators like "PromptğŸ‘‡", "prompt below", "check comment", "prompt in reply" etc., it means the actual prompt is in a reply/comment, not in the main post. In this case, return 'Prompt in reply'.
3. If the text only contains a title or description of what the image shows (like "Nano Banana prompt" or "Any person to Trash Pop Collage") but NOT the actual detailed prompt, return 'No prompt found'.
4. A real prompt usually contains detailed descriptions, style parameters (like --ar, --v), or specific technical terms.
5. If no actual prompt is found, return 'No prompt found'."""
        },
        {
            "role": "user",
            "content": f"Extract the AI image generation prompt from this text and return only the prompt itself:\n\n{text}"
        }
    ]

    try:
        return _call_ai_with_fallback(messages, model)
    except requests.exceptions.Timeout:
        raise Exception("API è¯·æ±‚è¶…æ—¶")
    except Exception as e:
        raise Exception(f"æå–æç¤ºè¯å¤±è´¥: {e}")


# é¢„å®šä¹‰çš„åˆ†ç±»åˆ—è¡¨
PROMPT_CATEGORIES = [
    "äººåƒ/è‚–åƒ (Portrait)",
    "é£æ™¯/è‡ªç„¶ (Landscape/Nature)",
    "åŠ¨ç‰© (Animals)",
    "å»ºç­‘/åŸå¸‚ (Architecture/Urban)",
    "æŠ½è±¡è‰ºæœ¯ (Abstract Art)",
    "ç§‘å¹»/æœªæ¥ (Sci-Fi/Futuristic)",
    "å¥‡å¹»/é­”æ³• (Fantasy/Magic)",
    "åŠ¨æ¼«/å¡é€š (Anime/Cartoon)",
    "å†™å®æ‘„å½± (Realistic Photography)",
    "æ’ç”»/ç»˜ç”» (Illustration/Painting)",
    "æ—¶å°š/æœè£… (Fashion/Clothing)",
    "é£Ÿç‰©/ç¾é£Ÿ (Food)",
    "äº§å“/å•†ä¸š (Product/Commercial)",
    "ææ€–/é»‘æš— (Horror/Dark)",
    "å¯çˆ±/èŒç³» (Cute/Kawaii)",
    "å¤å¤/æ€€æ—§ (Vintage/Retro)",
    "æç®€ä¸»ä¹‰ (Minimalist)",
    "è¶…ç°å® (Surreal)",
    "å…¶ä»– (Other)",
]


def classify_prompt_with_ai(prompt: str, model: str = DEFAULT_MODEL) -> dict:
    """
    ä½¿ç”¨ AI API å¯¹æç¤ºè¯è¿›è¡Œåˆ†ç±»
    ä¼˜å…ˆä½¿ç”¨ Pollinations AIï¼Œå¤±è´¥å fallback åˆ° Gitee AI (DeepSeek-V3)
    
    Args:
        prompt: æç¤ºè¯å†…å®¹
        model: ä½¿ç”¨çš„æ¨¡å‹
    
    Returns:
        åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸ {"category": "åˆ†ç±»", "confidence": "é«˜/ä¸­/ä½", "reason": "åŸå› "}
    """
    categories_str = "\n".join([f"- {cat}" for cat in PROMPT_CATEGORIES])
    
    messages = [
        {
            "role": "system",
            "content": f"""You are an AI image prompt classifier. Analyze the given prompt and classify it into one of the following categories:

{categories_str}

Respond in JSON format with exactly these fields:
- "title": a concise, descriptive title for this prompt in English (3-8 words, like a short headline)
- "category": the main category (choose from the list above, use the English part only, e.g., "Portrait", "Landscape/Nature")
- "sub_categories": array of 1-3 secondary categories in English (e.g., ["Fashion/Clothing", "Realistic Photography"])
- "style": detected art style (e.g., "photorealistic", "anime", "oil painting", "3D render", etc.)
- "confidence": "high", "medium", or "low"
- "reason": brief explanation in English (1 sentence)

Example response:
{{"title": "Fashion Actress Bird's Eye View", "category": "Portrait", "sub_categories": ["Fashion/Clothing"], "style": "photorealistic", "confidence": "high", "reason": "The prompt describes a Japanese actress in a black coat from above"}}"""
        },
        {
            "role": "user",
            "content": f"Classify this AI image generation prompt:\n\n{prompt}"
        }
    ]
    
    try:
        response_text = _call_ai_with_fallback(messages, model)
        
        # å°è¯•è§£æ JSON
        import json
        
        result = None
        
        # æ¸…ç†å“åº”æ–‡æœ¬
        cleaned_text = response_text.strip()
        
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        if cleaned_text.startswith("```json"):
            cleaned_text = cleaned_text[7:]
        elif cleaned_text.startswith("```"):
            cleaned_text = cleaned_text[3:]
        if cleaned_text.endswith("```"):
            cleaned_text = cleaned_text[:-3]
        cleaned_text = cleaned_text.strip()
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            result = json.loads(cleaned_text)
        except json.JSONDecodeError:
            # å°è¯•ä»å“åº”ä¸­æå– JSON (æ”¯æŒåµŒå¥—)
            json_match = re.search(r'\{.*\}', cleaned_text, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                except:
                    pass
        
        if not result:
            # è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
            print(f"      âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response_text[:200]}")
            return {
                "title": "Untitled Prompt",
                "category": "Other",
                "sub_categories": [],
                "style": "unknown",
                "confidence": "low",
                "reason": "Failed to parse classification result"
            }
        
        # æ ‡å‡†åŒ–ç»“æœï¼Œç¡®ä¿æ‰€æœ‰å¿…è¦å­—æ®µå­˜åœ¨
        normalized = {
            "title": result.get("title", "æœªå‘½åæç¤ºè¯"),
            "category": result.get("category", "å…¶ä»– (Other)"),
            "sub_categories": result.get("sub_categories", []),
            "style": result.get("style", "unknown"),
            "confidence": result.get("confidence", "ä¸­"),
            "reason": result.get("reason", ""),
        }
        
        # ç¡®ä¿ title æ˜¯å­—ç¬¦ä¸²
        if not isinstance(normalized["title"], str) or not normalized["title"].strip():
            normalized["title"] = "Untitled Prompt"
        
        # ç¡®ä¿ sub_categories æ˜¯åˆ—è¡¨
        if not isinstance(normalized["sub_categories"], list):
            normalized["sub_categories"] = []
        
        # æ¸…ç† sub_categories
        cleaned_tags = []
        for tag in normalized["sub_categories"]:
            if isinstance(tag, str) and tag.strip():
                cleaned_tags.append(tag.strip())
        normalized["sub_categories"] = cleaned_tags
        
        # æ·»åŠ  style åˆ° tags ä¸­ï¼ˆå¦‚æœä¸ä¸ºç©ºï¼‰
        if normalized["style"] and normalized["style"] != "unknown":
            if normalized["style"] not in normalized["sub_categories"]:
                normalized["sub_categories"].append(normalized["style"])
        
        print(f"      ğŸ“‹ åˆ†ç±»ç»“æœ: title={normalized['title']}, category={normalized['category']}, tags={normalized['sub_categories']}")
        
        return normalized
        
    except requests.exceptions.Timeout:
        raise Exception("API è¯·æ±‚è¶…æ—¶")
    except Exception as e:
        raise Exception(f"åˆ†ç±»å¤±è´¥: {e}")


def extract_tweet_id(url: str) -> str:
    """ä» URL ä¸­æå–æ¨æ–‡ ID"""
    # æ”¯æŒ twitter.com å’Œ x.com
    pattern = r'(?:twitter\.com|x\.com)/\w+/status/(\d+)'
    match = re.search(pattern, url)
    if match:
        return match.group(1)
    raise ValueError(f"æ— æ³•ä» URL ä¸­æå–æ¨æ–‡ ID: {url}")


def extract_username(url: str) -> str:
    """ä» URL ä¸­æå–ç”¨æˆ·å"""
    pattern = r'(?:twitter\.com|x\.com)/(\w+)/status/\d+'
    match = re.search(pattern, url)
    if match:
        return match.group(1)
    raise ValueError(f"æ— æ³•ä» URL ä¸­æå–ç”¨æˆ·å: {url}")


def fetch_with_syndication_api(tweet_id: str) -> dict:
    """
    ä½¿ç”¨ Twitter Syndication API è·å–æ¨æ–‡å†…å®¹
    è¿™æ˜¯å…¬å¼€ APIï¼Œä¸éœ€è¦è®¤è¯
    """
    url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&token=0"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"Syndication API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_fxtwitter(tweet_id: str, username: str) -> dict:
    """
    ä½¿ç”¨ FxTwitter/FixupX API è·å–æ¨æ–‡å†…å®¹
    è¿™æ˜¯ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œæä¾›æ›´å¥½çš„åµŒå…¥ä½“éªŒ
    """
    url = f"https://api.fxtwitter.com/{username}/status/{tweet_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TwitterBot/1.0)',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"FxTwitter API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_vxtwitter(tweet_id: str, username: str) -> dict:
    """
    ä½¿ç”¨ VxTwitter API è·å–æ¨æ–‡å†…å®¹
    """
    url = f"https://api.vxtwitter.com/{username}/status/{tweet_id}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; TwitterBot/1.0)',
        'Accept': 'application/json',
    }
    
    response = requests.get(url, headers=headers, timeout=30)
    
    if response.status_code == 200:
        return response.json()
    else:
        raise Exception(f"VxTwitter API è¯·æ±‚å¤±è´¥: {response.status_code}")


def fetch_with_playwright(url: str) -> dict:
    """
    ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–æ¨æ–‡å†…å®¹
    éœ€è¦å®‰è£…: pip install playwright && playwright install chromium
    """
    if not HAS_PLAYWRIGHT:
        raise ImportError("Playwright æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install playwright && playwright install chromium")
    
    result = {"text": None, "images": []}
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        page = context.new_page()
        
        try:
            page.goto(url, wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(3000)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            # è·å–æ¨æ–‡æ–‡æœ¬
            tweet_text_selectors = [
                'article[data-testid="tweet"] div[data-testid="tweetText"]',
                'article div[lang]',
                '[data-testid="tweetText"]',
            ]
            
            for selector in tweet_text_selectors:
                try:
                    text_element = page.query_selector(selector)
                    if text_element:
                        result["text"] = text_element.inner_text()
                        break
                except:
                    continue
            
            # è·å–å›¾ç‰‡
            image_selectors = [
                'article[data-testid="tweet"] img[src*="pbs.twimg.com/media"]',
                'article img[src*="twimg.com/media"]',
                '[data-testid="tweetPhoto"] img',
            ]
            
            for selector in image_selectors:
                try:
                    images = page.query_selector_all(selector)
                    for img in images:
                        src = img.get_attribute('src')
                        if src and 'pbs.twimg.com' in src:
                            # è·å–é«˜æ¸…ç‰ˆæœ¬
                            high_res = re.sub(r'\?.*$', '?format=jpg&name=large', src)
                            if high_res not in result["images"]:
                                result["images"].append(high_res)
                except:
                    continue
            
        finally:
            browser.close()
    
    return result


def download_image(url: str, save_path: str) -> str:
    """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    }
    
    response = requests.get(url, headers=headers, timeout=30, stream=True)
    
    if response.status_code == 200:
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return save_path
    else:
        raise Exception(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {response.status_code}")


def parse_syndication_result(data: dict) -> dict:
    """è§£æ Syndication API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    if "text" in data:
        result["text"] = data["text"]
    
    if "user" in data:
        result["user"] = {
            "name": data["user"].get("name", ""),
            "screen_name": data["user"].get("screen_name", ""),
        }
    
    if "created_at" in data:
        result["created_at"] = data["created_at"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": data.get("reply_count", 0),
        "retweets": data.get("retweet_count", 0),
        "likes": data.get("favorite_count", 0),
        "bookmarks": data.get("bookmark_count", 0),
        "views": data.get("views_count", 0),
    }
    
    # æå–åª’ä½“
    if "mediaDetails" in data:
        for media in data["mediaDetails"]:
            if media.get("type") == "photo":
                result["images"].append(media.get("media_url_https", ""))
    
    # ä¹Ÿæ£€æŸ¥ photos å­—æ®µ
    if "photos" in data:
        for photo in data["photos"]:
            url = photo.get("url", "")
            if url and url not in result["images"]:
                result["images"].append(url)
    
    return result


def parse_fxtwitter_result(data: dict) -> dict:
    """è§£æ FxTwitter API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    tweet = data.get("tweet", {})
    
    if "text" in tweet:
        result["text"] = tweet["text"]
    
    if "author" in tweet:
        result["user"] = {
            "name": tweet["author"].get("name", ""),
            "screen_name": tweet["author"].get("screen_name", ""),
        }
    
    if "created_at" in tweet:
        result["created_at"] = tweet["created_at"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": tweet.get("replies", 0),
        "retweets": tweet.get("retweets", 0),
        "likes": tweet.get("likes", 0),
        "bookmarks": tweet.get("bookmarks", 0),
        "views": tweet.get("views", 0),
    }
    
    # æå–åª’ä½“
    if "media" in tweet and "photos" in tweet["media"]:
        for photo in tweet["media"]["photos"]:
            result["images"].append(photo.get("url", ""))
    
    return result


def parse_vxtwitter_result(data: dict) -> dict:
    """è§£æ VxTwitter API çš„è¿”å›ç»“æœ"""
    result = {
        "text": "",
        "images": [],
        "user": {},
        "created_at": "",
        "stats": {},
    }
    
    if "text" in data:
        result["text"] = data["text"]
    
    result["user"] = {
        "name": data.get("user_name", ""),
        "screen_name": data.get("user_screen_name", ""),
    }
    
    if "date" in data:
        result["created_at"] = data["date"]
    
    # æå–äº’åŠ¨ç»Ÿè®¡
    result["stats"] = {
        "replies": data.get("replies", 0),
        "retweets": data.get("retweets", 0),
        "likes": data.get("likes", 0),
        "bookmarks": data.get("bookmarks", 0),
        "views": data.get("views", 0),
    }
    
    # æå–åª’ä½“
    if "media_extended" in data:
        for media in data["media_extended"]:
            if media.get("type") == "image":
                result["images"].append(media.get("url", ""))
    
    return result


def fetch_tweet(url: str, download_images: bool = True, output_dir: str = ".", 
                extract_prompt: bool = False, ai_model: str = DEFAULT_MODEL) -> dict:
    """
    è·å–æ¨æ–‡å†…å®¹çš„ä¸»å‡½æ•°
    ä¼šä¾æ¬¡å°è¯•ä¸åŒçš„æ–¹æ³•ç›´åˆ°æˆåŠŸ
    
    Args:
        url: æ¨æ–‡ URL
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        output_dir: è¾“å‡ºç›®å½•
        extract_prompt: æ˜¯å¦ä½¿ç”¨ AI æå–æç¤ºè¯
        ai_model: AI æ¨¡å‹åç§° (openai, deepseek ç­‰)
    """
    from datetime import datetime
    
    start_time = datetime.now()
    
    # è§£æ URL
    try:
        tweet_id = extract_tweet_id(url)
        username = extract_username(url)
    except Exception as e:
        print(f"âŒ [FAILED] æ— æ•ˆçš„ Twitter URL: {url}")
        print(f"   é”™è¯¯: {e}")
        raise Exception(f"æ— æ•ˆçš„ Twitter URL: {url}")
    
    print()
    print("=" * 70)
    print(f"ğŸ¦ å¼€å§‹å¤„ç†æ¨æ–‡")
    print(f"   URL: {url}")
    print(f"   æ¨æ–‡ ID: {tweet_id}")
    print(f"   ç”¨æˆ·å: @{username}")
    print(f"   æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    result = None
    fetch_method = None
    fetch_errors = []
    
    # æ–¹æ³• 1: å°è¯• FxTwitter API
    print("   [1/4] å°è¯• FxTwitter API...")
    try:
        data = fetch_with_fxtwitter(tweet_id, username)
        result = parse_fxtwitter_result(data)
        if result and result.get("text"):
            fetch_method = "FxTwitter"
            print("   âœ“ FxTwitter API æˆåŠŸ")
        else:
            raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
    except Exception as e:
        fetch_errors.append(f"FxTwitter: {e}")
        print(f"   âœ— FxTwitter API å¤±è´¥: {e}")
    
    # æ–¹æ³• 2: å°è¯• VxTwitter API
    if not result or not result.get("text"):
        print("   [2/4] å°è¯• VxTwitter API...")
        try:
            data = fetch_with_vxtwitter(tweet_id, username)
            result = parse_vxtwitter_result(data)
            if result and result.get("text"):
                fetch_method = "VxTwitter"
                print("   âœ“ VxTwitter API æˆåŠŸ")
            else:
                raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
        except Exception as e:
            fetch_errors.append(f"VxTwitter: {e}")
            print(f"   âœ— VxTwitter API å¤±è´¥: {e}")
    
    # æ–¹æ³• 3: å°è¯• Twitter Syndication API
    if not result or not result.get("text"):
        print("   [3/4] å°è¯• Syndication API...")
        try:
            data = fetch_with_syndication_api(tweet_id)
            result = parse_syndication_result(data)
            if result and result.get("text"):
                fetch_method = "Syndication"
                print("   âœ“ Syndication API æˆåŠŸ")
            else:
                raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
        except Exception as e:
            fetch_errors.append(f"Syndication: {e}")
            print(f"   âœ— Syndication API å¤±è´¥: {e}")
    
    # æ–¹æ³• 4: å°è¯• Playwright (éœ€è¦å®‰è£…)
    if not result or not result.get("text"):
        if HAS_PLAYWRIGHT:
            print("   [4/4] å°è¯• Playwright æµè§ˆå™¨...")
            try:
                result = fetch_with_playwright(url)
                if result and result.get("text"):
                    fetch_method = "Playwright"
                    print("   âœ“ Playwright æˆåŠŸ")
                else:
                    raise Exception("è¿”å›æ•°æ®ä¸ºç©º")
            except Exception as e:
                fetch_errors.append(f"Playwright: {e}")
                print(f"   âœ— Playwright å¤±è´¥: {e}")
        else:
            print("   [4/4] è·³è¿‡ Playwright (æœªå®‰è£…)")
    
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å†…å®¹
    if not result or not result.get("text"):
        elapsed = (datetime.now() - start_time).total_seconds()
        print()
        print(f"âŒ [FAILED] æ¨æ–‡è·å–å¤±è´¥: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è€—æ—¶: {elapsed:.1f}s")
        print(f"   å°è¯•çš„æ–¹æ³•åŠé”™è¯¯:")
        for err in fetch_errors:
            print(f"      - {err}")
        print("=" * 70)
        raise Exception(f"æ‰€æœ‰è·å–æ–¹æ³•éƒ½å¤±è´¥äº†: {url}")
    
    print()
    print(f"   âœ“ å†…å®¹è·å–æˆåŠŸ (via {fetch_method})")
    text_preview = result.get("text", "")[:100].replace("\n", " ")
    print(f"   ğŸ“ å†…å®¹é¢„è§ˆ: {text_preview}...")
    
    # ä¸‹è½½å›¾ç‰‡
    if download_images and result.get("images"):
        print()
        print(f"   ğŸ–¼ï¸  å‘ç° {len(result['images'])} å¼ å›¾ç‰‡")
        
        os.makedirs(output_dir, exist_ok=True)
        downloaded_images = []
        
        for i, img_url in enumerate(result["images"]):
            # è·å–é«˜æ¸…ç‰ˆæœ¬
            if "?" in img_url:
                high_res_url = re.sub(r'name=\w+', 'name=large', img_url)
            else:
                high_res_url = img_url + "?format=jpg&name=large"
            
            filename = f"tweet_{tweet_id}_image_{i+1}.jpg"
            filepath = os.path.join(output_dir, filename)
            
            try:
                download_image(high_res_url, filepath)
                downloaded_images.append(filepath)
                print(f"      âœ“ å›¾ç‰‡ {i+1}: {filename}")
            except Exception as e:
                print(f"      âœ— å›¾ç‰‡ {i+1} ä¸‹è½½å¤±è´¥: {e}")
        
        result["downloaded_images"] = downloaded_images
    
    # ä½¿ç”¨ AI æå–æç¤ºè¯
    if extract_prompt and result.get("text"):
        print()
        print(f"   ğŸ¤– AI å¤„ç† (æ¨¡å‹: {ai_model})")

        # å…ˆæ£€æµ‹æ˜¯å¦æ˜¯ "prompt åœ¨è¯„è®ºä¸­" çš„æƒ…å†µ
        prompt_in_reply = detect_prompt_in_reply(result["text"])
        if prompt_in_reply:
            print(f"      âš ï¸ æ£€æµ‹åˆ° prompt å¯èƒ½åœ¨è¯„è®º/å›å¤ä¸­")
            result["prompt_location"] = "reply"
        else:
            result["prompt_location"] = "post"

        # æå–æç¤ºè¯
        print(f"      [1/2] æå–æç¤ºè¯...")
        try:
            extracted_prompt = extract_prompt_with_ai(result["text"], model=ai_model)
            result["extracted_prompt"] = extracted_prompt

            # å¤„ç†ä¸åŒçš„æå–ç»“æœ
            if extracted_prompt == "Prompt in reply":
                print(f"      âš ï¸ Prompt åœ¨è¯„è®º/å›å¤ä¸­ï¼Œå°è¯•è·å–ä½œè€…å›å¤...")
                result["prompt_location"] = "reply"

                # å°è¯•è·å–ä½œè€…çš„å›å¤
                # ä¼˜å…ˆä½¿ç”¨ API è¿”å›çš„å®é™…ä½œè€…ç”¨æˆ·åï¼ˆURL ä¸­çš„ç”¨æˆ·åå¯èƒ½ä¸å‡†ç¡®ï¼‰
                actual_author = result.get("user", {}).get("screen_name", username)
                if actual_author != username:
                    print(f"      â„¹ï¸ å®é™…ä½œè€…: @{actual_author} (URL ä¸­: @{username})")
                author_replies = fetch_author_replies(tweet_id, actual_author)
                if author_replies:
                    print(f"      âœ“ è·å–åˆ° {len(author_replies)} æ¡ä½œè€…å›å¤")

                    # åˆå¹¶æ‰€æœ‰ä½œè€…å›å¤ï¼Œä»ä¸­æå– prompt
                    combined_reply_text = "\n\n".join([r["text"] for r in author_replies])
                    result["author_replies"] = author_replies

                    # å°è¯•ä»å›å¤ä¸­æå– prompt
                    print(f"      [1.5/2] ä»ä½œè€…å›å¤ä¸­æå–æç¤ºè¯...")

                    # é¦–å…ˆå°è¯•æ­£åˆ™è¡¨è¾¾å¼æå– (æ›´å¿«æ›´å¯é )
                    reply_prompt = None
                    for reply in author_replies:
                        reply_prompt = extract_prompt_from_text(reply["text"])
                        if reply_prompt:
                            print(f"      âœ“ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æˆåŠŸ")
                            break

                    # å¦‚æœæ­£åˆ™æ²¡æœ‰æå–åˆ°ï¼Œå°è¯• AI æå–
                    if not reply_prompt:
                        print(f"      â„¹ï¸ æ­£åˆ™æœªåŒ¹é…ï¼Œå°è¯• AI æå–...")
                        try:
                            # ç›´æ¥è°ƒç”¨ AI æå–ï¼Œä¸å†æ£€æµ‹ "Prompt in reply"
                            reply_prompt = _call_ai_with_fallback([
                                {
                                    "role": "system",
                                    "content": "You are a helpful assistant that extracts AI image generation prompts from text. Extract only the prompt itself, without any additional explanation or formatting. If no prompt is found, return 'No prompt found'."
                                },
                                {
                                    "role": "user",
                                    "content": f"Extract the AI image generation prompt from this text and return only the prompt itself:\n\n{combined_reply_text}"
                                }
                            ], ai_model)

                            if reply_prompt == "No prompt found":
                                reply_prompt = None
                        except Exception as e:
                            print(f"      âš ï¸ AI æå–å¤±è´¥: {e}")
                            reply_prompt = None

                    if reply_prompt:
                        extracted_prompt = reply_prompt
                        result["extracted_prompt"] = extracted_prompt
                        result["prompt_location"] = "reply"  # æ ‡è®°æ˜¯ä»å›å¤ä¸­æå–çš„
                        prompt_preview = extracted_prompt[:80].replace("\n", " ")
                        print(f"      âœ“ ä»å›å¤ä¸­æå–æˆåŠŸ: {prompt_preview}...")

                        # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
                        print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
                        try:
                            classification = classify_prompt_with_ai(extracted_prompt, model=ai_model)
                            result["classification"] = classification

                            title = classification.get("title", "æœªçŸ¥")
                            category = classification.get("category", "æœªçŸ¥")
                            confidence = classification.get("confidence", "æœªçŸ¥")
                            print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
                        except Exception as e:
                            print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                            result["classification"] = None
                    else:
                        print(f"      âš ï¸ ä½œè€…å›å¤ä¸­ä¹Ÿæœªæ‰¾åˆ°æç¤ºè¯")
                        result["classification"] = None
                else:
                    print(f"      âš ï¸ æœªè·å–åˆ°ä½œè€…å›å¤ (å¯èƒ½éœ€è¦é…ç½® cookies)")
                    result["classification"] = None
            elif extracted_prompt and extracted_prompt != "No prompt found":
                prompt_preview = extracted_prompt[:80].replace("\n", " ")
                print(f"      âœ“ æå–æˆåŠŸ: {prompt_preview}...")
                result["prompt_location"] = "post"

                # å¯¹æå–çš„æç¤ºè¯è¿›è¡Œåˆ†ç±»
                print(f"      [2/2] åˆ†ç±»æç¤ºè¯...")
                try:
                    classification = classify_prompt_with_ai(extracted_prompt, model=ai_model)
                    result["classification"] = classification

                    title = classification.get("title", "æœªçŸ¥")
                    category = classification.get("category", "æœªçŸ¥")
                    confidence = classification.get("confidence", "æœªçŸ¥")
                    print(f"      âœ“ åˆ†ç±»æˆåŠŸ: {title} | {category} | ç½®ä¿¡åº¦: {confidence}")
                except Exception as e:
                    print(f"      âœ— åˆ†ç±»å¤±è´¥: {e}")
                    result["classification"] = None
            else:
                print(f"      âš ï¸ æœªæ‰¾åˆ°æç¤ºè¯")
                result["classification"] = None
        except Exception as e:
            print(f"      âœ— æå–å¤±è´¥: {e}")
            result["extracted_prompt"] = None
            result["classification"] = None

    # å®Œæˆ
    elapsed = (datetime.now() - start_time).total_seconds()
    print()
    prompt_location = result.get("prompt_location", "unknown")
    extracted_prompt = result.get("extracted_prompt", "")

    # åˆ¤æ–­æ˜¯å¦æˆåŠŸæå–äº† prompt (å³ä½¿æ˜¯ä»è¯„è®ºä¸­æå–çš„)
    has_valid_prompt = extracted_prompt and extracted_prompt not in ["No prompt found", "Prompt in reply"]

    if has_valid_prompt:
        if prompt_location == "reply":
            print(f"âœ… [SUCCESS_FROM_REPLY] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
            print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
            print(f"   è·å–æ–¹å¼: {fetch_method}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
            print(f"   æç¤ºè¯: å·²ä»è¯„è®ºä¸­æå–")
        else:
            print(f"âœ… [SUCCESS] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
            print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
            print(f"   è·å–æ–¹å¼: {fetch_method}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
            print(f"   æç¤ºè¯: å·²æå–")
        if result.get("classification"):
            print(f"   åˆ†ç±»: {result['classification'].get('category', 'æœªçŸ¥')}")
    elif prompt_location == "reply":
        print(f"âš ï¸ [PROMPT_IN_REPLY] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è·å–æ–¹å¼: {fetch_method}")
        print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
        print(f"   æç¤ºè¯ä½ç½®: è¯„è®º/å›å¤ä¸­ (æœªèƒ½æå–)")
    else:
        print(f"âš ï¸ [NO_PROMPT] æ¨æ–‡å¤„ç†å®Œæˆ: {url}")
        print(f"   ç”¨æˆ·: @{username} | æ¨æ–‡ID: {tweet_id}")
        print(f"   è·å–æ–¹å¼: {fetch_method}")
        print(f"   å›¾ç‰‡æ•°é‡: {len(result.get('images', []))}")
        print(f"   æç¤ºè¯: æœªæ‰¾åˆ°")
    print(f"   è€—æ—¶: {elapsed:.1f}s")
    print("=" * 70)

    return result


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Twitter/X å†…å®¹è·å–å·¥å…· - è·å–æ¨æ–‡æ­£æ–‡ã€å›¾ç‰‡å’Œäº’åŠ¨ç»Ÿè®¡",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³• (é»˜è®¤å¯ç”¨æç¤ºè¯æå–å’Œåˆ†ç±»)
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637
  
  # ä½¿ç”¨ deepseek æ¨¡å‹
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 --model deepseek
  
  # ç¦ç”¨æç¤ºè¯æå–
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 --no-extract
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python fetch_twitter_content.py https://x.com/oggii_0/status/2001232399368380637 -o ./output
        """
    )
    
    parser.add_argument("url", help="æ¨æ–‡ URL (æ”¯æŒ x.com å’Œ twitter.com)")
    parser.add_argument("-o", "--output", default=".", help="è¾“å‡ºç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)")
    parser.add_argument("--no-extract", action="store_true", 
                        help="ç¦ç”¨ AI æå–æç¤ºè¯å’Œåˆ†ç±» (é»˜è®¤å¯ç”¨)")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL,
                        help=f"AI æ¨¡å‹ (é»˜è®¤: {DEFAULT_MODEL}, å¯é€‰: deepseek, openai ç­‰)")
    parser.add_argument("--no-download", action="store_true", help="ä¸ä¸‹è½½å›¾ç‰‡")
    
    args = parser.parse_args()
    
    url = args.url
    output_dir = args.output
    
    extract_prompt = not args.no_extract  # é»˜è®¤å¯ç”¨æå–æç¤ºè¯
    
    print("=" * 50)
    print("Twitter/X å†…å®¹è·å–å·¥å…·")
    print("=" * 50)
    print(f"URL: {url}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    if extract_prompt:
        print(f"AI æ¨¡å‹: {args.model}")
    print("=" * 50)
    
    try:
        result = fetch_tweet(
            url, 
            download_images=not args.no_download, 
            output_dir=output_dir,
            extract_prompt=extract_prompt,
            ai_model=args.model
        )
        
        print("\n" + "=" * 50)
        print("è·å–ç»“æœ")
        print("=" * 50)
        
        if result.get("user"):
            user = result["user"]
            print(f"ç”¨æˆ·: {user.get('name', '')} (@{user.get('screen_name', '')})")
        
        if result.get("created_at"):
            print(f"æ—¶é—´: {result['created_at']}")
        
        # æ˜¾ç¤ºäº’åŠ¨ç»Ÿè®¡
        if result.get("stats"):
            stats = result["stats"]
            print(f"\näº’åŠ¨ç»Ÿè®¡:")
            print(f"  ğŸ’¬ è¯„è®ºæ•° (Replies): {stats.get('replies', 0)}")
            print(f"  ğŸ” è½¬å‘æ•° (Retweets): {stats.get('retweets', 0)}")
            print(f"  â¤ï¸  ç‚¹èµæ•° (Likes): {stats.get('likes', 0)}")
            print(f"  ğŸ”– ä¹¦ç­¾æ•° (Bookmarks): {stats.get('bookmarks', 0)}")
            print(f"  ğŸ‘ï¸  æµè§ˆé‡ (Views): {stats.get('views', 0)}")
        
        print(f"\næ­£æ–‡å†…å®¹:")
        print("-" * 50)
        print(result.get("text", "æ— æ³•è·å–"))
        print("-" * 50)
        
        # æ˜¾ç¤ºæå–çš„æç¤ºè¯
        if result.get("extracted_prompt"):
            print(f"\nğŸ¨ æå–çš„æç¤ºè¯:")
            print("-" * 50)
            print(result["extracted_prompt"])
            print("-" * 50)
        
        # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
        if result.get("classification"):
            cls = result["classification"]
            print(f"\nğŸ“‚ æç¤ºè¯åˆ†ç±»:")
            print("-" * 50)
            if cls.get("title"):
                print(f"  ğŸ“Œ æ ‡é¢˜: {cls.get('title')}")
            print(f"  ä¸»åˆ†ç±»: {cls.get('category', 'æœªçŸ¥')}")
            if cls.get("sub_categories"):
                print(f"  æ¬¡åˆ†ç±»: {', '.join(cls['sub_categories'])}")
            if cls.get("style"):
                print(f"  é£æ ¼: {cls.get('style', 'æœªçŸ¥')}")
            print(f"  ç½®ä¿¡åº¦: {cls.get('confidence', 'æœªçŸ¥')}")
            if cls.get("reason"):
                print(f"  åŸå› : {cls.get('reason', '')}")
            print("-" * 50)
        
        if result.get("images"):
            print(f"\nå›¾ç‰‡ URL ({len(result['images'])} å¼ ):")
            for i, img in enumerate(result["images"], 1):
                print(f"  {i}. {img}")
        
        if result.get("downloaded_images"):
            print(f"\nå·²ä¸‹è½½å›¾ç‰‡:")
            for img_path in result["downloaded_images"]:
                print(f"  - {img_path}")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = os.path.join(output_dir, f"tweet_{extract_tweet_id(url)}_content.txt")
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"URL: {url}\n")
            if result.get("user"):
                f.write(f"User: {result['user'].get('name', '')} (@{result['user'].get('screen_name', '')})\n")
            if result.get("created_at"):
                f.write(f"Time: {result['created_at']}\n")
            
            # ä¿å­˜äº’åŠ¨ç»Ÿè®¡
            if result.get("stats"):
                stats = result["stats"]
                f.write(f"\nStats:\n")
                f.write(f"  Replies: {stats.get('replies', 0)}\n")
                f.write(f"  Retweets: {stats.get('retweets', 0)}\n")
                f.write(f"  Likes: {stats.get('likes', 0)}\n")
                f.write(f"  Bookmarks: {stats.get('bookmarks', 0)}\n")
                f.write(f"  Views: {stats.get('views', 0)}\n")
            
            f.write(f"\nText:\n{result.get('text', '')}\n")
            
            # ä¿å­˜æå–çš„æç¤ºè¯
            if result.get("extracted_prompt"):
                f.write(f"\nExtracted Prompt:\n{result['extracted_prompt']}\n")
            
            # ä¿å­˜åˆ†ç±»ç»“æœ
            if result.get("classification"):
                cls = result["classification"]
                f.write(f"\nClassification:\n")
                if cls.get("title"):
                    f.write(f"  Title: {cls.get('title')}\n")
                f.write(f"  Category: {cls.get('category', 'Unknown')}\n")
                if cls.get("sub_categories"):
                    f.write(f"  Sub-categories: {', '.join(cls['sub_categories'])}\n")
                if cls.get("style"):
                    f.write(f"  Style: {cls.get('style', 'Unknown')}\n")
                f.write(f"  Confidence: {cls.get('confidence', 'Unknown')}\n")
                if cls.get("reason"):
                    f.write(f"  Reason: {cls.get('reason', '')}\n")
            
            if result.get("images"):
                f.write(f"\nImages:\n")
                for img in result["images"]:
                    f.write(f"  {img}\n")
        print(f"\nâœ“ å†…å®¹å·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

