#!/usr/bin/env python3
"""
X/Twitter çˆ†æ¬¾æç¤ºè¯æœç´¢å™¨
ä½¿ç”¨ twikit ç›´æ¥æœç´¢å…³é”®è¯ï¼Œè·å–é«˜äº’åŠ¨çš„ AI æç¤ºè¯

æŠ€æœ¯æ–¹æ¡ˆ:
- twikit: æœç´¢ Twitter å…³é”®è¯ (æ”¯æŒ min_faves è¿‡æ»¤)
- FxTwitter: è·å–æ¨æ–‡è¯¦æƒ…å’Œå›¾ç‰‡
- AI: æå–å’Œåˆ†ç±»æç¤ºè¯

ä½¿ç”¨æ–¹æ³•:
    # æœç´¢çˆ†æ¬¾æç¤ºè¯ (é»˜è®¤ min_faves:500)
    python search_viral_prompts.py

    # è‡ªå®šä¹‰æœ€ä½ç‚¹èµæ•°
    python search_viral_prompts.py --min-likes 1000

    # æœç´¢ç‰¹å®šå…³é”®è¯
    python search_viral_prompts.py --keyword "nano banana"

    # é¢„è§ˆæ¨¡å¼
    python search_viral_prompts.py --dry-run

    # æŒç»­æœç´¢ (æ¯ 30 åˆ†é’Ÿ)
    python search_viral_prompts.py --interval 30

ç¯å¢ƒå˜é‡:
    DATABASE_URL        - PostgreSQL è¿æ¥å­—ç¬¦ä¸² (å¿…éœ€)
    GITEE_AI_API_KEY    - AI æ¨¡å‹ API Key
    X_COOKIE            - X è´¦å· cookies JSON (æ¨è): '{"auth_token": "xxx", "ct0": "xxx"}'
    X_USERNAME          - X è´¦å·ç”¨æˆ·å (å¤‡ç”¨ç™»å½•æ–¹å¼)
    X_EMAIL             - X è´¦å·é‚®ç®±
    X_PASSWORD          - X è´¦å·å¯†ç 
"""

import os
import sys
import asyncio
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List, Dict

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"[env] Loaded: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"[env] Loaded: {env_file}")
except ImportError:
    pass

# twikit
try:
    from twikit import Client
    HAS_TWIKIT = True
except ImportError:
    HAS_TWIKIT = False
    print("[Warning] twikit not installed. Run: pip install twikit")

# æ•°æ®åº“
try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("Please install psycopg2: pip install psycopg2-binary")
    sys.exit(1)

# å¯¼å…¥ AI å¤„ç†å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ prompt_utils)
from prompt_utils import DEFAULT_MODEL

# å¯¼å…¥ Twitter API å‡½æ•°å’Œ AI å¤„ç†é€‚é…å‡½æ•°
from fetch_twitter_content import (
    extract_prompt_with_ai,
    classify_prompt_with_ai,
    fetch_with_fxtwitter,
    parse_fxtwitter_result,
)

# ========== é…ç½® ==========

DATABASE_URL = os.environ.get("DATABASE_URL", "")
AI_MODEL = os.environ.get("AI_MODEL", DEFAULT_MODEL)

# X è´¦å·å‡­è¯
X_USERNAME = os.environ.get("X_USERNAME", "")
X_EMAIL = os.environ.get("X_EMAIL", "")
X_PASSWORD = os.environ.get("X_PASSWORD", "")

# Cookies é…ç½® (ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡)
X_COOKIE = os.environ.get("X_COOKIE", "")  # JSON å­—ç¬¦ä¸²: '{"auth_token": "xxx", "ct0": "xxx"}'
COOKIES_FILE = Path(__file__).parent / "x_cookies.json"

# ä»£ç†é…ç½® (å¦‚æœéœ€è¦)
PROXY_URL = os.environ.get("X_PROXY", "")

# çŠ¶æ€æ–‡ä»¶
STATE_FILE = Path(__file__).parent / "x_search_state.json"

# ========== æœç´¢å…³é”®è¯ ==========

# Nano Banana ç›¸å…³æœç´¢è¯
SEARCH_KEYWORDS = [
    # Nano Banana ç›´æ¥æœç´¢
    '"nano banana"',
    'nanobanana',
    '"Nano Banana Pro"',

    # Gemini å›¾åƒç”Ÿæˆ
    '"gemini image"',
    '"gemini 2.5" image',
    'geminiç”Ÿå›¾',

    # å°é¦™è•‰ (ä¸­æ–‡)
    'å°é¦™è•‰ æç¤ºè¯',
    'å°é¦™è•‰ prompt',
]

# é»˜è®¤æœç´¢é…ç½®
DEFAULT_MIN_LIKES = 100  # Nano Banana å†…å®¹è¾ƒæ–°ï¼Œé™ä½é˜ˆå€¼
DEFAULT_MIN_RETWEETS = 20
DEFAULT_RESULTS_PER_KEYWORD = 30
DEFAULT_DAYS_BACK = 1  # åªæœç´¢æœ€è¿‘ N å¤©çš„æ¨æ–‡
DEFAULT_HOURS_BACK = 1  # åªå¤„ç†æœ€è¿‘ N å°æ—¶çš„æ¨æ–‡ (0=ä¸é™åˆ¶)


# ========== æ•°æ®åº“æ“ä½œ ==========

class Database:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.conn = None

    def connect(self):
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(self.connection_string)
        return self.conn

    def close(self):
        if self.conn and not self.conn.closed:
            self.conn.close()

    def execute_write(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            conn.commit()
            if cur.description:
                result = cur.fetchone()
                return dict(result) if result else None
            return None

    def execute_one(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            result = cur.fetchone()
            return dict(result) if result else None

    def prompt_exists(self, source_link: str) -> bool:
        result = self.execute_one(
            "SELECT id FROM prompts WHERE source_link = %s",
            (source_link,)
        )
        return result is not None

    def save_prompt(self, title: str, prompt: str, category: str,
                    tags: List[str], images: List[str], source_link: str,
                    author: str = None, import_source: str = None) -> Optional[Dict]:
        return self.execute_write(
            """
            INSERT INTO prompts (title, prompt, category, tags, images, source_link, author, import_source)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING *
            """,
            (title, prompt, category, tags or [], images or [], source_link, author, import_source)
        )


# ========== çŠ¶æ€ç®¡ç† ==========

def load_state() -> Dict:
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
        except:
            pass
    return {"processed_tweets": [], "last_search": None}


def save_state(state: Dict):
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)


def is_tweet_processed(state: Dict, tweet_id: str) -> bool:
    return tweet_id in state.get("processed_tweets", [])


def mark_tweet_processed(state: Dict, tweet_id: str):
    if "processed_tweets" not in state:
        state["processed_tweets"] = []
    state["processed_tweets"].append(tweet_id)
    # ä¿ç•™æœ€è¿‘ 10000 æ¡
    if len(state["processed_tweets"]) > 10000:
        state["processed_tweets"] = state["processed_tweets"][-5000:]
    save_state(state)


# ========== åˆ†ç±»æ˜ å°„ ==========

CATEGORY_MAP = {
    "Portrait": "Portrait",
    "Landscape/Nature": "Landscape",
    "Landscape": "Landscape",
    "Nature": "Nature",
    "Animals": "Nature",
    "Architecture/Urban": "Architecture",
    "Architecture": "Architecture",
    "Abstract Art": "Abstract",
    "Abstract": "Abstract",
    "Sci-Fi/Futuristic": "Sci-Fi",
    "Sci-Fi": "Sci-Fi",
    "Fantasy/Magic": "Fantasy",
    "Fantasy": "Fantasy",
    "Anime/Cartoon": "Anime",
    "Anime": "Anime",
    "Realistic Photography": "Photography",
    "Photography": "Photography",
    "Illustration/Painting": "Illustration",
    "Illustration": "Illustration",
    "Fashion/Clothing": "Fashion",
    "Fashion": "Fashion",
    "Food": "Food",
    "Product/Commercial": "Product",
    "Product": "Product",
    "Cinematic": "Cinematic",
    "Horror/Dark": "Cinematic",
    "Cute/Kawaii": "Clay / Felt",
    "Vintage/Retro": "Retro / Vintage",
    "Minimalist": "Minimalist",
    "Surreal": "Abstract",
    "Other": "Other",
}


def map_category(classification: Dict) -> str:
    raw_category = classification.get("category", "Other")
    if raw_category in CATEGORY_MAP:
        return CATEGORY_MAP[raw_category]
    raw_lower = raw_category.lower()
    for key, value in CATEGORY_MAP.items():
        if key.lower() in raw_lower or raw_lower in key.lower():
            return value
    return "Photography"


# ========== twikit æœç´¢ ==========

class TwikitSearcher:
    """ä½¿ç”¨ twikit æœç´¢æ¨æ–‡"""

    def __init__(self):
        self.client = None
        self.logged_in = False

    async def login(self):
        """ç™»å½• X è´¦å·"""
        if not HAS_TWIKIT:
            raise RuntimeError("twikit not installed")

        # åˆå§‹åŒ–å®¢æˆ·ç«¯ (æ”¯æŒä»£ç†)
        if PROXY_URL:
            print(f"[twikit] Using proxy: {PROXY_URL[:20]}...")
            self.client = Client('en-US', proxy=PROXY_URL)
        else:
            self.client = Client('en-US')

        # å°è¯•ä½¿ç”¨ cookies ç™»å½• (ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡)
        if X_COOKIE:
            try:
                import tempfile
                cookie_data = json.loads(X_COOKIE)
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶ä¾› twikit åŠ è½½
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json.dump(cookie_data, f)
                    temp_cookie_file = f.name
                self.client.load_cookies(temp_cookie_file)
                os.unlink(temp_cookie_file)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                print("[twikit] Loaded cookies from X_COOKIE env")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Failed to load cookies from env: {e}")

        if COOKIES_FILE.exists():
            try:
                self.client.load_cookies(str(COOKIES_FILE))
                # éªŒè¯ cookies æ˜¯å¦æœ‰æ•ˆ
                print("[twikit] Loaded cookies from file")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Failed to load cookies: {e}")

        # ä½¿ç”¨è´¦å·å¯†ç ç™»å½•
        if X_USERNAME and X_PASSWORD:
            try:
                print("[twikit] Logging in with credentials...")
                await self.client.login(
                    auth_info_1=X_USERNAME,
                    auth_info_2=X_EMAIL,
                    password=X_PASSWORD
                )
                # ä¿å­˜ cookies
                self.client.save_cookies(str(COOKIES_FILE))
                print("[twikit] Login successful, cookies saved")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Login failed: {e}")
                raise

        # æç¤ºå¦‚ä½•è·å– cookies
        print("\n" + "=" * 60)
        print("éœ€è¦ X è´¦å· cookies æ‰èƒ½æœç´¢")
        print("=" * 60)
        print("\næ–¹æ³•: ä»æµè§ˆå™¨å¯¼å‡º cookies")
        print("1. åœ¨ Chrome ç™»å½• x.com")
        print("2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("3. åˆ‡æ¢åˆ° Application > Cookies > https://x.com")
        print("4. æ‰¾åˆ°ä»¥ä¸‹ cookies å¹¶å¤åˆ¶å€¼:")
        print("   - auth_token")
        print("   - ct0")
        print("5. åˆ›å»º worker/x_cookies.json æ–‡ä»¶:\n")
        print('''{
    "auth_token": "ä½ çš„auth_tokenå€¼",
    "ct0": "ä½ çš„ct0å€¼"
}''')
        print("\n" + "=" * 60)
        raise RuntimeError("No cookies file found. See instructions above.")

    async def search(self, keyword: str, min_likes: int = 500,
                     min_retweets: int = 0, count: int = 20,
                     days_back: int = 7) -> List[Dict]:
        """
        æœç´¢æ¨æ–‡

        Args:
            keyword: æœç´¢å…³é”®è¯
            min_likes: æœ€ä½ç‚¹èµæ•°
            min_retweets: æœ€ä½è½¬å‘æ•°
            count: ç»“æœæ•°é‡
            days_back: åªæœç´¢æœ€è¿‘ N å¤©çš„æ¨æ–‡

        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        if not self.logged_in:
            await self.login()

        # æ„å»ºæœç´¢æŸ¥è¯¢
        # ä½¿ç”¨ Twitter é«˜çº§æœç´¢è¯­æ³•
        query_parts = [keyword]

        # æ·»åŠ æ—¶é—´è¿‡æ»¤ - åªæœç´¢æœ€è¿‘ N å¤©
        if days_back > 0:
            from datetime import timedelta
            since_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
            query_parts.append(f"since:{since_date}")

        # æ·»åŠ å›¾ç‰‡è¿‡æ»¤
        query_parts.append("filter:images")

        # æ·»åŠ äº’åŠ¨è¿‡æ»¤
        if min_likes > 0:
            query_parts.append(f"min_faves:{min_likes}")
        if min_retweets > 0:
            query_parts.append(f"min_retweets:{min_retweets}")

        # æ’é™¤è½¬æ¨
        query_parts.append("-filter:retweets")

        query = " ".join(query_parts)
        print(f"   Query: {query}")

        tweets = []
        try:
            # æœç´¢æœ€æ–°ç»“æœ (Latest æ¨¡å¼æ›´ç¨³å®š)
            results = await self.client.search_tweet(query, 'Latest', count=count)

            for tweet in results:
                try:
                    tweet_data = {
                        "id": tweet.id,
                        "text": tweet.text or "",
                        "username": tweet.user.screen_name if tweet.user else "unknown",
                        "user_name": tweet.user.name if tweet.user else "Unknown",
                        "url": f"https://x.com/{tweet.user.screen_name}/status/{tweet.id}" if tweet.user else "",
                        "likes": tweet.favorite_count or 0,
                        "retweets": tweet.retweet_count or 0,
                        "views": tweet.view_count or 0,
                        "created_at": str(tweet.created_at) if tweet.created_at else None,
                        "images": [],
                    }

                    # æå–å›¾ç‰‡ (twikit 2.3+ ä½¿ç”¨ Photo/Video ç±»)
                    if tweet.media:
                        for media in tweet.media:
                            # ä¼˜å…ˆä½¿ç”¨ media_url (å®é™…å›¾ç‰‡åœ°å€)
                            if hasattr(media, 'media_url') and media.media_url:
                                img_url = media.media_url
                                # ç¡®ä¿ä½¿ç”¨ https
                                if img_url.startswith('http://'):
                                    img_url = img_url.replace('http://', 'https://')
                                tweet_data["images"].append(img_url)
                            elif hasattr(media, 'media_url_https') and media.media_url_https:
                                tweet_data["images"].append(media.media_url_https)

                    tweets.append(tweet_data)
                except Exception as e:
                    print(f"   [Warning] Failed to parse tweet: {e}")
                    continue

        except Exception as e:
            print(f"   [Error] Search failed: {e}")

        return tweets

    async def search_multiple(self, keywords: List[str], min_likes: int = 500,
                              count_per_keyword: int = 20, days_back: int = 7) -> List[Dict]:
        """æœç´¢å¤šä¸ªå…³é”®è¯"""
        all_tweets = []
        seen_ids = set()

        for keyword in keywords:
            print(f"\n[Search] Keyword: {keyword}")
            tweets = await self.search(keyword, min_likes=min_likes, count=count_per_keyword, days_back=days_back)

            for tweet in tweets:
                if tweet["id"] not in seen_ids:
                    seen_ids.add(tweet["id"])
                    all_tweets.append(tweet)

            print(f"   Found {len(tweets)} tweets, {len(all_tweets)} total unique")

            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(2)

        return all_tweets


# ========== å¤„ç†é€»è¾‘ ==========

async def process_tweet(db: Database, tweet: Dict, state: Dict,
                        dry_run: bool = False, hours_back: int = 0) -> bool:
    """å¤„ç†å•æ¡æ¨æ–‡"""
    tweet_id = tweet["id"]
    tweet_url = tweet["url"]
    text = tweet["text"]
    images = tweet.get("images", [])
    username = tweet["username"]
    likes = tweet.get("likes", 0)
    retweets = tweet.get("retweets", 0)
    views = tweet.get("views", 0)
    created_at = tweet.get("created_at", "")

    # æ£€æŸ¥æ¨æ–‡æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šå°æ—¶å†…
    if hours_back > 0 and created_at:
        try:
            from datetime import timedelta
            from dateutil import parser as date_parser
            tweet_time = date_parser.parse(created_at)
            # ç¡®ä¿æ—¶åŒºä¸€è‡´
            if tweet_time.tzinfo is None:
                tweet_time = tweet_time.replace(tzinfo=timezone.utc)
            # æ”¾å®½æ—¶é—´é™åˆ¶åˆ° 1.5 å€
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back * 1.5)
            if tweet_time < cutoff_time:
                print(f"   [Skip] Tweet too old: {created_at} (cutoff: {hours_back * 1.5:.0f}h)")
                return False
        except Exception as e:
            print(f"   [Warning] Failed to parse tweet time: {e}")

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if is_tweet_processed(state, tweet_id):
        return False

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
    if db.prompt_exists(tweet_url):
        mark_tweet_processed(state, tweet_id)
        return False

    # æ€»æ˜¯å°è¯•ä» FxTwitter è·å–å®Œæ•´å†…å®¹
    # twikit æœç´¢ API å¯¹äºé•¿æ¨æ–‡ (Twitter Notes) å¯èƒ½è¿”å›æˆªæ–­çš„æ–‡æœ¬
    try:
        fx_data = fetch_with_fxtwitter(tweet_id, username)
        fx_result = parse_fxtwitter_result(fx_data)
        if fx_result:
            # ä½¿ç”¨ FxTwitter è¿”å›çš„å®Œæ•´æ–‡æœ¬ (å¦‚æœæ›´é•¿)
            fx_text = fx_result.get("text", "")
            if fx_text and len(fx_text) > len(text):
                print(f"   [FxTwitter] Got longer text: {len(text)} -> {len(fx_text)} chars")
                text = fx_text
                tweet["text"] = fx_text
            # è¡¥å……å›¾ç‰‡
            if fx_result.get("images") and not images:
                images = fx_result["images"]
                tweet["images"] = images
    except Exception as e:
        print(f"   [FxTwitter] Failed to get full text: {e}")

    if not images:
        print(f"   [Skip] No images: @{username}/{tweet_id}")
        mark_tweet_processed(state, tweet_id)
        return False

    # æ˜¾ç¤ºæ¨æ–‡ä¿¡æ¯
    print(f"\n   [Tweet] @{username} - {tweet_id}")
    print(f"   Text: {text[:100]}...")
    print(f"   Stats: â¤ï¸ {int(likes or 0):,} | ğŸ” {int(retweets or 0):,} | ğŸ‘ï¸ {int(views or 0):,}")
    print(f"   Images: {len(images)}")

    # AI æå–æç¤ºè¯
    try:
        print(f"   [AI] Extracting prompt...")
        extracted_prompt = extract_prompt_with_ai(text, model=AI_MODEL)

        # æ£€æŸ¥æ˜¯å¦ä¸ºå¹¿å‘Š
        if extracted_prompt == "Advertisement":
            print(f"   [Skip] Advertisement content detected")
            mark_tweet_processed(state, tweet_id)
            return False

        # æ£€æŸ¥æ˜¯å¦æå–å¤±è´¥
        invalid_prompts = [
            "No prompt found",
            "No prompt",
            "None",
            "N/A",
            "",
        ]
        if not extracted_prompt or extracted_prompt.strip().lower() in [p.lower() for p in invalid_prompts]:
            print(f"   [Skip] AI found no prompt")
            mark_tweet_processed(state, tweet_id)
            return False

        # æ£€æŸ¥æç¤ºè¯æ˜¯å¦å¤ªçŸ­ (å¯èƒ½æ˜¯æ— æ•ˆæå–)
        if len(extracted_prompt.strip()) < 20:
            print(f"   [Skip] Prompt too short ({len(extracted_prompt)} chars)")
            mark_tweet_processed(state, tweet_id)
            return False

        # AI åˆ†ç±»
        print(f"   [AI] Classifying...")
        classification = classify_prompt_with_ai(extracted_prompt, model=AI_MODEL)

        # å‡†å¤‡æ•°æ®
        title = classification.get("title", "").strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ— æ•ˆæ ‡é¢˜ (è¡¨ç¤ºè§£æå¤±è´¥)
        invalid_titles = [
            "Untitled Prompt",
            "No Prompt Provided",
            "Unknown Prompt",
            "No Title",
            "Untitled",
            "N/A",
            "",
        ]
        if title.lower() in [t.lower() for t in invalid_titles]:
            # ä½¿ç”¨é»˜è®¤æ ‡é¢˜
            title = f"@{username} #{tweet_id[-6:]}"

        # å†æ¬¡æ£€æŸ¥åˆ†ç±»æ˜¯å¦æœ‰æ•ˆ - å¦‚æœ category ä¹Ÿæ˜¯ Unknown/Other ä¸”æ²¡æœ‰ tagsï¼Œå¯èƒ½æ˜¯æ— æ•ˆæå–
        raw_category = classification.get("category", "")
        tags = classification.get("sub_categories", [])
        if (raw_category.lower() in ["unknown", "other", ""] and
            (not tags or len(tags) == 0) and
            title.startswith("@")):  # ä½¿ç”¨äº†é»˜è®¤æ ‡é¢˜
            print(f"   [Skip] Classification failed (unknown category, no tags)")
            mark_tweet_processed(state, tweet_id)
            return False

        category = map_category(classification)
        tags = classification.get("sub_categories", [])
        if not isinstance(tags, list):
            tags = []
        tags = [str(t).strip() for t in tags if t][:5]

        # Dry Run
        if dry_run:
            print(f"   [DRY RUN] Would save: {title}")
            print(f"      Category: {category}")
            print(f"      Tags: {tags}")
            print(f"      Images: {len(images)}")
            print(f"      Prompt: {extracted_prompt[:100]}...")
            mark_tweet_processed(state, tweet_id)
            return True

        # ä¿å­˜åˆ°æ•°æ®åº“
        print(f"   [DB] Saving: {title}")
        prompt_record = db.save_prompt(
            title=title,
            prompt=extracted_prompt,
            category=category,
            tags=tags,
            images=images[:5],
            source_link=tweet_url,
            author=username,
            import_source="x-search-viral"
        )

        if prompt_record:
            print(f"   [OK] Saved: {title}")
            mark_tweet_processed(state, tweet_id)
            return True
        else:
            print(f"   [Error] Failed to save")
            return False

    except Exception as e:
        print(f"   [Error] {e}")
        mark_tweet_processed(state, tweet_id)
        return False


async def search_viral_prompts(
    keywords: List[str] = None,
    min_likes: int = DEFAULT_MIN_LIKES,
    count_per_keyword: int = DEFAULT_RESULTS_PER_KEYWORD,
    days_back: int = DEFAULT_DAYS_BACK,
    hours_back: int = DEFAULT_HOURS_BACK,
    dry_run: bool = False
) -> Dict:
    """
    æœç´¢çˆ†æ¬¾æç¤ºè¯

    Args:
        keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        min_likes: æœ€ä½ç‚¹èµæ•°
        count_per_keyword: æ¯ä¸ªå…³é”®è¯æœç´¢æ•°é‡
        days_back: åªæœç´¢æœ€è¿‘ N å¤©çš„æ¨æ–‡ (Twitter API çº§åˆ«)
        hours_back: åªå¤„ç†æœ€è¿‘ N å°æ—¶çš„æ¨æ–‡ (0=ä¸é™åˆ¶)
        dry_run: é¢„è§ˆæ¨¡å¼
    """
    keywords = keywords or SEARCH_KEYWORDS

    print("=" * 60)
    print("X/Twitter Viral Prompt Search")
    print("=" * 60)
    print(f"Keywords: {len(keywords)}")
    print(f"Min Likes: {min_likes:,}")
    print(f"Days Back: {days_back}")
    print(f"Hours Back: {hours_back}" + (" (filter enabled)" if hours_back > 0 else " (no filter)"))
    print(f"Count per keyword: {count_per_keyword}")
    print(f"Dry Run: {dry_run}")
    print(f"AI Model: {AI_MODEL}")
    print("=" * 60)

    if not DATABASE_URL:
        print("[Error] DATABASE_URL not set")
        return {"error": "DATABASE_URL not set"}

    if not HAS_TWIKIT:
        print("[Error] twikit not installed")
        return {"error": "twikit not installed"}

    # åˆå§‹åŒ–
    db = Database(DATABASE_URL)
    searcher = TwikitSearcher()
    state = load_state()

    stats = {
        "keywords_searched": 0,
        "tweets_found": 0,
        "prompts_saved": 0,
        "skipped": 0,
        "errors": 0,
    }

    try:
        db.connect()
        print("[DB] Connected")

        await searcher.login()
        print("[twikit] Ready")

        # æœç´¢æ‰€æœ‰å…³é”®è¯
        all_tweets = await searcher.search_multiple(
            keywords,
            min_likes=min_likes,
            count_per_keyword=count_per_keyword,
            days_back=days_back
        )

        stats["keywords_searched"] = len(keywords)
        stats["tweets_found"] = len(all_tweets)

        print(f"\n[Processing] {len(all_tweets)} unique tweets...")

        # æŒ‰ç‚¹èµæ•°æ’åº
        all_tweets.sort(key=lambda x: x.get("likes", 0), reverse=True)

        for i, tweet in enumerate(all_tweets, 1):
            print(f"\n[{i}/{len(all_tweets)}]", end="")
            result = await process_tweet(db, tweet, state, dry_run=dry_run, hours_back=hours_back)
            if result:
                stats["prompts_saved"] += 1
            else:
                stats["skipped"] += 1

        # æ›´æ–°çŠ¶æ€
        state["last_search"] = datetime.now(timezone.utc).isoformat()
        save_state(state)

    except Exception as e:
        print(f"[Error] {e}")
        stats["errors"] += 1
    finally:
        db.close()

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    print(f"Keywords searched: {stats['keywords_searched']}")
    print(f"Tweets found: {stats['tweets_found']}")
    print(f"Prompts saved: {stats['prompts_saved']}")
    print(f"Skipped: {stats['skipped']}")
    print(f"Errors: {stats['errors']}")
    print("=" * 60)

    return stats


async def run_continuous(
    keywords: List[str] = None,
    min_likes: int = DEFAULT_MIN_LIKES,
    days_back: int = DEFAULT_DAYS_BACK,
    hours_back: int = DEFAULT_HOURS_BACK,
    interval_minutes: int = 30,
    dry_run: bool = False
):
    """æŒç»­æœç´¢æ¨¡å¼"""
    print(f"Starting continuous search (interval: {interval_minutes} min)")
    print("Press Ctrl+C to stop\n")

    while True:
        try:
            await search_viral_prompts(
                keywords=keywords,
                min_likes=min_likes,
                days_back=days_back,
                hours_back=hours_back,
                dry_run=dry_run
            )

            print(f"\nNext search in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)

        except KeyboardInterrupt:
            print("\nStopped by user")
            break
        except Exception as e:
            print(f"\n[Error] {e}")
            print(f"Retrying in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)


# ========== CLI ==========

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Search viral AI prompts on X/Twitter using twikit",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Search with default settings (min_likes=500)
  python search_viral_prompts.py

  # Higher threshold for more viral content
  python search_viral_prompts.py --min-likes 1000

  # Search specific keyword
  python search_viral_prompts.py --keyword "nano banana prompt"

  # Dry run (don't save to database)
  python search_viral_prompts.py --dry-run

  # Continuous search (every 30 min)
  python search_viral_prompts.py --interval 30

  # Custom keywords (comma-separated)
  python search_viral_prompts.py --keywords "midjourney prompt,AI art prompt"

Environment Variables:
  DATABASE_URL     - PostgreSQL connection string
  X_USERNAME       - X account username
  X_EMAIL          - X account email
  X_PASSWORD       - X account password
        """
    )

    parser.add_argument("--keyword", "-k", type=str,
                        help="Single search keyword")
    parser.add_argument("--keywords", type=str,
                        help="Comma-separated keywords")
    parser.add_argument("--min-likes", "-l", type=int, default=DEFAULT_MIN_LIKES,
                        help=f"Minimum likes (default: {DEFAULT_MIN_LIKES})")
    parser.add_argument("--days", type=int, default=DEFAULT_DAYS_BACK,
                        help=f"Only search tweets from last N days (default: {DEFAULT_DAYS_BACK})")
    parser.add_argument("--hours", type=int, default=DEFAULT_HOURS_BACK,
                        help=f"Only process tweets from last N hours, 0=no filter (default: {DEFAULT_HOURS_BACK})")
    parser.add_argument("--count", "-c", type=int, default=DEFAULT_RESULTS_PER_KEYWORD,
                        help=f"Results per keyword (default: {DEFAULT_RESULTS_PER_KEYWORD})")
    parser.add_argument("--interval", "-i", type=int, default=0,
                        help="Continuous mode interval in minutes (0=run once)")
    parser.add_argument("--dry-run", "-d", action="store_true",
                        help="Dry run mode - don't save to database")

    args = parser.parse_args()

    # è§£æå…³é”®è¯
    keywords = None
    if args.keyword:
        keywords = [args.keyword]
    elif args.keywords:
        keywords = [k.strip() for k in args.keywords.split(",") if k.strip()]

    # è¿è¡Œ
    if args.interval > 0:
        asyncio.run(run_continuous(
            keywords=keywords,
            min_likes=args.min_likes,
            days_back=args.days,
            hours_back=args.hours,
            interval_minutes=args.interval,
            dry_run=args.dry_run
        ))
    else:
        asyncio.run(search_viral_prompts(
            keywords=keywords,
            min_likes=args.min_likes,
            days_back=args.days,
            hours_back=args.hours,
            count_per_keyword=args.count,
            dry_run=args.dry_run
        ))


if __name__ == "__main__":
    main()
