#!/usr/bin/env python3
"""
X/Twitter çˆ†æ¬¾æç¤ºè¯æœç´¢å™¨
ä½¿ç”¨ twikit ç›´æŽ¥æœç´¢å…³é”®è¯ï¼ŒèŽ·å–é«˜äº’åŠ¨çš„ AI æç¤ºè¯

æŠ€æœ¯æ–¹æ¡ˆ:
- twikit: æœç´¢ Twitter å…³é”®è¯ (æ”¯æŒ min_faves è¿‡æ»¤)
- FxTwitter: èŽ·å–æŽ¨æ–‡è¯¦æƒ…å’Œå›¾ç‰‡
- AI: æå–å’Œåˆ†ç±»æç¤ºè¯

ä½¿ç”¨æ–¹æ³•:
    # æœç´¢çˆ†æ¬¾æç¤ºè¯ (é»˜è®¤ min_faves:500)
    python search_viral_prompts.py

    # è‡ªå®šä¹‰æœ€ä½Žç‚¹èµžæ•°
    python search_viral_prompts.py --min-likes 1000

    # æœç´¢ç‰¹å®šå…³é”®è¯
    python search_viral_prompts.py --keyword "nano banana"

    # é¢„è§ˆæ¨¡å¼
    python search_viral_prompts.py --dry-run

    # æŒç»­æœç´¢ (æ¯ 30 åˆ†é’Ÿ)
    python search_viral_prompts.py --interval 30

çŽ¯å¢ƒå˜é‡:
    DATABASE_URL        - PostgreSQL è¿žæŽ¥å­—ç¬¦ä¸² (å¿…éœ€)
    GITEE_AI_API_KEY    - AI æ¨¡åž‹ API Key
    X_COOKIE            - X è´¦å· cookies JSON (æŽ¨è): '{"auth_token": "xxx", "ct0": "xxx"}'
    X_USERNAME          - X è´¦å·ç”¨æˆ·å (å¤‡ç”¨ç™»å½•æ–¹å¼)
    X_EMAIL             - X è´¦å·é‚®ç®±
    X_PASSWORD          - X è´¦å·å¯†ç 
"""

import argparse
import asyncio
import json
import os
import sys
import tempfile
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional

from dateutil import parser as date_parser

# åŠ è½½çŽ¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"[env] Loaded: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"[env] Loaded: {env_file}")
except ImportError:
    pass

# twikit
try:
    from twikit import Client
    HAS_TWIKIT = True
except ImportError:
    HAS_TWIKIT = False
    print("[Warning] twikit not installed. Run: pip install twikit")

# æ•°æ®åº“
try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("Please install psycopg2: pip install psycopg2-binary")
    sys.exit(1)

# å¯¼å…¥ AI å¤„ç†å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ prompt_utils)
from prompt_utils import DEFAULT_MODEL, process_tweet_for_import

# å¯¼å…¥ Twitter API å‡½æ•°
from fetch_twitter_content import (
    fetch_with_fxtwitter,
    parse_fxtwitter_result,
)

# ========== é…ç½® ==========

DATABASE_URL = os.environ.get("DATABASE_URL", "")
AI_MODEL = os.environ.get("AI_MODEL", DEFAULT_MODEL)

# X è´¦å·å‡­è¯
X_USERNAME = os.environ.get("X_USERNAME", "")
X_EMAIL = os.environ.get("X_EMAIL", "")
X_PASSWORD = os.environ.get("X_PASSWORD", "")

# Cookies é…ç½® (ä¼˜å…ˆä½¿ç”¨çŽ¯å¢ƒå˜é‡)
X_COOKIE = os.environ.get("X_COOKIE", "")  # JSON å­—ç¬¦ä¸²: '{"auth_token": "xxx", "ct0": "xxx"}'
COOKIES_FILE = Path(__file__).parent / "x_cookies.json"

# ä»£ç†é…ç½® (å¦‚æžœéœ€è¦)
PROXY_URL = os.environ.get("X_PROXY", "")

# çŠ¶æ€æ–‡ä»¶
STATE_FILE = Path(__file__).parent / "x_search_state.json"

# ========== æœç´¢å…³é”®è¯ ==========

# Nano Banana ç›¸å…³æœç´¢è¯
SEARCH_KEYWORDS = [
    # Nano Banana ç›´æŽ¥æœç´¢
    '"nano banana"',
    'nanobanana',
    '"Nano Banana Pro"',

    # Gemini å›¾åƒç”Ÿæˆ
    '"gemini image"',
    '"gemini 2.5" image',
    'geminiç”Ÿå›¾',

    # å°é¦™è•‰ (ä¸­æ–‡)
    'å°é¦™è•‰ æç¤ºè¯',
    'å°é¦™è•‰ prompt',
]

# é»˜è®¤æœç´¢é…ç½®
DEFAULT_MIN_LIKES = 100  # Nano Banana å†…å®¹è¾ƒæ–°ï¼Œé™ä½Žé˜ˆå€¼
DEFAULT_MIN_RETWEETS = 20
DEFAULT_RESULTS_PER_KEYWORD = 30
DEFAULT_DAYS_BACK = 1  # åªæœç´¢æœ€è¿‘ N å¤©çš„æŽ¨æ–‡
DEFAULT_HOURS_BACK = 1  # åªå¤„ç†æœ€è¿‘ N å°æ—¶çš„æŽ¨æ–‡ (0=ä¸é™åˆ¶)


# ========== æ•°æ®åº“æ“ä½œ ==========

class Database:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.conn = None

    def connect(self):
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(self.connection_string)
        return self.conn

    def close(self):
        if self.conn and not self.conn.closed:
            self.conn.close()

    def execute_write(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            conn.commit()
            if cur.description:
                result = cur.fetchone()
                return dict(result) if result else None
            return None

    def execute_one(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            result = cur.fetchone()
            return dict(result) if result else None

    def prompt_exists(self, source_link: str) -> bool:
        result = self.execute_one(
            "SELECT id FROM prompts WHERE source_link = %s",
            (source_link,)
        )
        return result is not None

    def save_prompt(self, title: str, prompt: str, category: str,
                    tags: List[str], images: List[str], source_link: str,
                    author: str = None, import_source: str = None) -> Optional[Dict]:
        return self.execute_write(
            """
            INSERT INTO prompts (title, prompt, category, tags, images, source_link, author, import_source)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING *
            """,
            (title, prompt, category, tags or [], images or [], source_link, author, import_source)
        )


# ========== çŠ¶æ€ç®¡ç† ==========

def load_state() -> Dict:
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
        except:
            pass
    return {"processed_tweets": [], "last_search": None}


def save_state(state: Dict):
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)


def is_tweet_processed(state: Dict, tweet_id: str) -> bool:
    return tweet_id in state.get("processed_tweets", [])


def mark_tweet_processed(state: Dict, tweet_id: str):
    if "processed_tweets" not in state:
        state["processed_tweets"] = []
    state["processed_tweets"].append(tweet_id)
    # ä¿ç•™æœ€è¿‘ 10000 æ¡
    if len(state["processed_tweets"]) > 10000:
        state["processed_tweets"] = state["processed_tweets"][-5000:]
    save_state(state)


# ========== åˆ†ç±»æ˜ å°„ ==========

# å¯¼å…¥ç»Ÿä¸€åˆ†ç±»æ˜ å°„ (å®šä¹‰åœ¨ prompt_utils.py)
from prompt_utils import CATEGORY_MAP, map_category


# ========== twikit æœç´¢ ==========

class TwikitSearcher:
    """ä½¿ç”¨ twikit æœç´¢æŽ¨æ–‡"""

    def __init__(self):
        self.client = None
        self.logged_in = False

    async def login(self):
        """ç™»å½• X è´¦å·"""
        if not HAS_TWIKIT:
            raise RuntimeError("twikit not installed")

        # åˆå§‹åŒ–å®¢æˆ·ç«¯ (æ”¯æŒä»£ç†)
        if PROXY_URL:
            print(f"[twikit] Using proxy: {PROXY_URL[:20]}...")
            self.client = Client('en-US', proxy=PROXY_URL)
        else:
            self.client = Client('en-US')

        # å°è¯•ä½¿ç”¨ cookies ç™»å½• (ä¼˜å…ˆä½¿ç”¨çŽ¯å¢ƒå˜é‡)
        if X_COOKIE:
            try:
                cookie_data = json.loads(X_COOKIE)
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶ä¾› twikit åŠ è½½
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json.dump(cookie_data, f)
                    temp_cookie_file = f.name
                self.client.load_cookies(temp_cookie_file)
                os.unlink(temp_cookie_file)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                print("[twikit] Loaded cookies from X_COOKIE env")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Failed to load cookies from env: {e}")

        if COOKIES_FILE.exists():
            try:
                self.client.load_cookies(str(COOKIES_FILE))
                # éªŒè¯ cookies æ˜¯å¦æœ‰æ•ˆ
                print("[twikit] Loaded cookies from file")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Failed to load cookies: {e}")

        # ä½¿ç”¨è´¦å·å¯†ç ç™»å½•
        if X_USERNAME and X_PASSWORD:
            try:
                print("[twikit] Logging in with credentials...")
                await self.client.login(
                    auth_info_1=X_USERNAME,
                    auth_info_2=X_EMAIL,
                    password=X_PASSWORD
                )
                # ä¿å­˜ cookies
                self.client.save_cookies(str(COOKIES_FILE))
                print("[twikit] Login successful, cookies saved")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Login failed: {e}")
                raise

        # æç¤ºå¦‚ä½•èŽ·å– cookies
        print("\n" + "=" * 60)
        print("éœ€è¦ X è´¦å· cookies æ‰èƒ½æœç´¢")
        print("=" * 60)
        print("\næ–¹æ³•: ä»Žæµè§ˆå™¨å¯¼å‡º cookies")
        print("1. åœ¨ Chrome ç™»å½• x.com")
        print("2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("3. åˆ‡æ¢åˆ° Application > Cookies > https://x.com")
        print("4. æ‰¾åˆ°ä»¥ä¸‹ cookies å¹¶å¤åˆ¶å€¼:")
        print("   - auth_token")
        print("   - ct0")
        print("5. åˆ›å»º worker/x_cookies.json æ–‡ä»¶:\n")
        print('''{
    "auth_token": "ä½ çš„auth_tokenå€¼",
    "ct0": "ä½ çš„ct0å€¼"
}''')
        print("\n" + "=" * 60)
        raise RuntimeError("No cookies file found. See instructions above.")

    async def search(self, keyword: str, min_likes: int = 500,
                     min_retweets: int = 0, count: int = 20,
                     days_back: int = 7) -> List[Dict]:
        """
        æœç´¢æŽ¨æ–‡

        Args:
            keyword: æœç´¢å…³é”®è¯
            min_likes: æœ€ä½Žç‚¹èµžæ•°
            min_retweets: æœ€ä½Žè½¬å‘æ•°
            count: ç»“æžœæ•°é‡
            days_back: åªæœç´¢æœ€è¿‘ N å¤©çš„æŽ¨æ–‡

        Returns:
            æŽ¨æ–‡åˆ—è¡¨
        """
        if not self.logged_in:
            await self.login()

        # æž„å»ºæœç´¢æŸ¥è¯¢
        # ä½¿ç”¨ Twitter é«˜çº§æœç´¢è¯­æ³•
        query_parts = [keyword]

        # æ·»åŠ æ—¶é—´è¿‡æ»¤ - åªæœç´¢æœ€è¿‘ N å¤©
        if days_back > 0:
            since_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
            query_parts.append(f"since:{since_date}")

        # æ·»åŠ å›¾ç‰‡è¿‡æ»¤
        query_parts.append("filter:images")

        # æ·»åŠ äº’åŠ¨è¿‡æ»¤
        if min_likes > 0:
            query_parts.append(f"min_faves:{min_likes}")
        if min_retweets > 0:
            query_parts.append(f"min_retweets:{min_retweets}")

        # æŽ’é™¤è½¬æŽ¨
        query_parts.append("-filter:retweets")

        query = " ".join(query_parts)
        print(f"   Query: {query}")

        tweets = []
        try:
            # æœç´¢æœ€æ–°ç»“æžœ (Latest æ¨¡å¼æ›´ç¨³å®š)
            results = await self.client.search_tweet(query, 'Latest', count=count)

            for tweet in results:
                try:
                    # ä¼˜å…ˆä½¿ç”¨ full_text èŽ·å–é•¿æŽ¨æ–‡ (note_tweet) çš„å®Œæ•´å†…å®¹
                    tweet_text = ""
                    try:
                        tweet_text = tweet.full_text or tweet.text or ""
                    except Exception:
                        tweet_text = tweet.text or ""

                    tweet_data = {
                        "id": tweet.id,
                        "text": tweet_text,
                        "username": tweet.user.screen_name if tweet.user else "unknown",
                        "user_name": tweet.user.name if tweet.user else "Unknown",
                        "url": f"https://x.com/{tweet.user.screen_name}/status/{tweet.id}" if tweet.user else "",
                        "likes": tweet.favorite_count or 0,
                        "retweets": tweet.retweet_count or 0,
                        "views": tweet.view_count or 0,
                        "created_at": str(tweet.created_at) if tweet.created_at else None,
                        "images": [],
                    }

                    # æå–å›¾ç‰‡ (twikit 2.3+ ä½¿ç”¨ Photo/Video ç±»)
                    if tweet.media:
                        for media in tweet.media:
                            # ä¼˜å…ˆä½¿ç”¨ media_url (å®žé™…å›¾ç‰‡åœ°å€)
                            if hasattr(media, 'media_url') and media.media_url:
                                img_url = media.media_url
                                # ç¡®ä¿ä½¿ç”¨ https
                                if img_url.startswith('http://'):
                                    img_url = img_url.replace('http://', 'https://')
                                tweet_data["images"].append(img_url)
                            elif hasattr(media, 'media_url_https') and media.media_url_https:
                                tweet_data["images"].append(media.media_url_https)

                    tweets.append(tweet_data)
                except Exception as e:
                    print(f"   [Warning] Failed to parse tweet: {e}")
                    continue

        except Exception as e:
            print(f"   [Error] Search failed: {e}")

        return tweets

    async def search_multiple(self, keywords: List[str], min_likes: int = 500,
                              count_per_keyword: int = 20, days_back: int = 7) -> List[Dict]:
        """æœç´¢å¤šä¸ªå…³é”®è¯"""
        all_tweets = []
        seen_ids = set()

        for keyword in keywords:
            print(f"\n[Search] Keyword: {keyword}")
            tweets = await self.search(keyword, min_likes=min_likes, count=count_per_keyword, days_back=days_back)

            for tweet in tweets:
                if tweet["id"] not in seen_ids:
                    seen_ids.add(tweet["id"])
                    all_tweets.append(tweet)

            print(f"   Found {len(tweets)} tweets, {len(all_tweets)} total unique")

            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(2)

        return all_tweets


# ========== å¤„ç†é€»è¾‘ ==========

async def process_tweet(db: Database, tweet: Dict, state: Dict,
                        dry_run: bool = False, hours_back: int = 0) -> bool:
    """å¤„ç†å•æ¡æŽ¨æ–‡ - ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°"""
    tweet_id = tweet["id"]
    tweet_url = tweet["url"]
    text = tweet["text"]
    images = tweet.get("images", [])
    username = tweet["username"]
    likes = tweet.get("likes", 0)
    retweets = tweet.get("retweets", 0)
    views = tweet.get("views", 0)
    created_at = tweet.get("created_at", "")

    # æ£€æŸ¥æŽ¨æ–‡æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šå°æ—¶å†…
    if hours_back > 0 and created_at:
        try:
            tweet_time = date_parser.parse(created_at)
            if tweet_time.tzinfo is None:
                tweet_time = tweet_time.replace(tzinfo=timezone.utc)
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back * 1.5)
            if tweet_time < cutoff_time:
                print(f"   [Skip] Tweet too old: {created_at} (cutoff: {hours_back * 1.5:.0f}h)")
                return False
        except Exception as e:
            print(f"   [Warning] Failed to parse tweet time: {e}")

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if is_tweet_processed(state, tweet_id):
        return False

    # æ€»æ˜¯å°è¯•ä»Ž FxTwitter èŽ·å–å®Œæ•´å†…å®¹
    try:
        fx_data = fetch_with_fxtwitter(tweet_id, username)
        fx_result = parse_fxtwitter_result(fx_data)
        if fx_result:
            fx_text = fx_result.get("text", "")
            if fx_text and len(fx_text) > len(text):
                print(f"   [FxTwitter] Got longer text: {len(text)} -> {len(fx_text)} chars")
                text = fx_text
            if fx_result.get("images") and not images:
                images = fx_result["images"]
    except Exception as e:
        print(f"   [FxTwitter] Failed to get full text: {e}")

    if not images:
        print(f"   [Skip] No images: @{username}/{tweet_id}")
        mark_tweet_processed(state, tweet_id)
        return False

    # æ˜¾ç¤ºæŽ¨æ–‡ä¿¡æ¯
    print(f"\n   [Tweet] @{username} - {tweet_id}")
    print(f"   Text: {text[:100]}...")
    print(f"   Stats: â¤ï¸ {int(likes or 0):,} | ðŸ” {int(retweets or 0):,} | ðŸ‘ï¸ {int(views or 0):,}")
    print(f"   Images: {len(images)}")

    # ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    result = process_tweet_for_import(
        db=db,
        tweet_url=tweet_url,
        raw_text=text,
        raw_images=images,
        author=username,
        import_source="x-search-viral",
        ai_model=AI_MODEL,
        dry_run=dry_run,
        skip_twitter_fetch=True  # å·²æœ‰ Twitter å›¾ç‰‡
    )

    mark_tweet_processed(state, tweet_id)

    if result["success"]:
        return True
    else:
        error = result.get("error", "")
        if error and error != "Already exists":
            print(f"   [Skip] {error}")
        return False


async def search_viral_prompts(
    keywords: List[str] = None,
    min_likes: int = DEFAULT_MIN_LIKES,
    count_per_keyword: int = DEFAULT_RESULTS_PER_KEYWORD,
    days_back: int = DEFAULT_DAYS_BACK,
    hours_back: int = DEFAULT_HOURS_BACK,
    dry_run: bool = False
) -> Dict:
    """
    æœç´¢çˆ†æ¬¾æç¤ºè¯

    Args:
        keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        min_likes: æœ€ä½Žç‚¹èµžæ•°
        count_per_keyword: æ¯ä¸ªå…³é”®è¯æœç´¢æ•°é‡
        days_back: åªæœç´¢æœ€è¿‘ N å¤©çš„æŽ¨æ–‡ (Twitter API çº§åˆ«)
        hours_back: åªå¤„ç†æœ€è¿‘ N å°æ—¶çš„æŽ¨æ–‡ (0=ä¸é™åˆ¶)
        dry_run: é¢„è§ˆæ¨¡å¼
    """
    keywords = keywords or SEARCH_KEYWORDS

    print("=" * 60)
    print("X/Twitter Viral Prompt Search")
    print("=" * 60)
    print(f"Keywords: {len(keywords)}")
    print(f"Min Likes: {min_likes:,}")
    print(f"Days Back: {days_back}")
    print(f"Hours Back: {hours_back}" + (" (filter enabled)" if hours_back > 0 else " (no filter)"))
    print(f"Count per keyword: {count_per_keyword}")
    print(f"Dry Run: {dry_run}")
    print(f"AI Model: {AI_MODEL}")
    print("=" * 60)

    if not DATABASE_URL:
        print("[Error] DATABASE_URL not set")
        return {"error": "DATABASE_URL not set"}

    if not HAS_TWIKIT:
        print("[Error] twikit not installed")
        return {"error": "twikit not installed"}

    # åˆå§‹åŒ–
    db = Database(DATABASE_URL)
    searcher = TwikitSearcher()
    state = load_state()

    stats = {
        "keywords_searched": 0,
        "tweets_found": 0,
        "prompts_saved": 0,
        "skipped": 0,
        "errors": 0,
    }

    try:
        db.connect()
        print("[DB] Connected")

        await searcher.login()
        print("[twikit] Ready")

        # æœç´¢æ‰€æœ‰å…³é”®è¯
        all_tweets = await searcher.search_multiple(
            keywords,
            min_likes=min_likes,
            count_per_keyword=count_per_keyword,
            days_back=days_back
        )

        stats["keywords_searched"] = len(keywords)
        stats["tweets_found"] = len(all_tweets)

        print(f"\n[Processing] {len(all_tweets)} unique tweets...")

        # æŒ‰ç‚¹èµžæ•°æŽ’åº
        all_tweets.sort(key=lambda x: x.get("likes", 0), reverse=True)

        for i, tweet in enumerate(all_tweets, 1):
            print(f"\n[{i}/{len(all_tweets)}]", end="")
            result = await process_tweet(db, tweet, state, dry_run=dry_run, hours_back=hours_back)
            if result:
                stats["prompts_saved"] += 1
            else:
                stats["skipped"] += 1

        # æ›´æ–°çŠ¶æ€
        state["last_search"] = datetime.now(timezone.utc).isoformat()
        save_state(state)

    except Exception as e:
        print(f"[Error] {e}")
        stats["errors"] += 1
    finally:
        db.close()

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    print(f"Keywords searched: {stats['keywords_searched']}")
    print(f"Tweets found: {stats['tweets_found']}")
    print(f"Prompts saved: {stats['prompts_saved']}")
    print(f"Skipped: {stats['skipped']}")
    print(f"Errors: {stats['errors']}")
    print("=" * 60)

    return stats


async def run_continuous(
    keywords: List[str] = None,
    min_likes: int = DEFAULT_MIN_LIKES,
    days_back: int = DEFAULT_DAYS_BACK,
    hours_back: int = DEFAULT_HOURS_BACK,
    interval_minutes: int = 30,
    dry_run: bool = False
):
    """æŒç»­æœç´¢æ¨¡å¼"""
    print(f"Starting continuous search (interval: {interval_minutes} min)")
    print("Press Ctrl+C to stop\n")

    while True:
        try:
            await search_viral_prompts(
                keywords=keywords,
                min_likes=min_likes,
                days_back=days_back,
                hours_back=hours_back,
                dry_run=dry_run
            )

            print(f"\nNext search in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)

        except KeyboardInterrupt:
            print("\nStopped by user")
            break
        except Exception as e:
            print(f"\n[Error] {e}")
            print(f"Retrying in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)


# ========== CLI ==========

def main():
    parser = argparse.ArgumentParser(
        description="Search viral AI prompts on X/Twitter using twikit",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Search with default settings (min_likes=500)
  python search_viral_prompts.py

  # Higher threshold for more viral content
  python search_viral_prompts.py --min-likes 1000

  # Search specific keyword
  python search_viral_prompts.py --keyword "nano banana prompt"

  # Dry run (don't save to database)
  python search_viral_prompts.py --dry-run

  # Continuous search (every 30 min)
  python search_viral_prompts.py --interval 30

  # Custom keywords (comma-separated)
  python search_viral_prompts.py --keywords "midjourney prompt,AI art prompt"

Environment Variables:
  DATABASE_URL     - PostgreSQL connection string
  X_USERNAME       - X account username
  X_EMAIL          - X account email
  X_PASSWORD       - X account password
        """
    )

    parser.add_argument("--keyword", "-k", type=str,
                        help="Single search keyword")
    parser.add_argument("--keywords", type=str,
                        help="Comma-separated keywords")
    parser.add_argument("--min-likes", "-l", type=int, default=DEFAULT_MIN_LIKES,
                        help=f"Minimum likes (default: {DEFAULT_MIN_LIKES})")
    parser.add_argument("--days", type=int, default=DEFAULT_DAYS_BACK,
                        help=f"Only search tweets from last N days (default: {DEFAULT_DAYS_BACK})")
    parser.add_argument("--hours", type=int, default=DEFAULT_HOURS_BACK,
                        help=f"Only process tweets from last N hours, 0=no filter (default: {DEFAULT_HOURS_BACK})")
    parser.add_argument("--count", "-c", type=int, default=DEFAULT_RESULTS_PER_KEYWORD,
                        help=f"Results per keyword (default: {DEFAULT_RESULTS_PER_KEYWORD})")
    parser.add_argument("--interval", "-i", type=int, default=0,
                        help="Continuous mode interval in minutes (0=run once)")
    parser.add_argument("--dry-run", "-d", action="store_true",
                        help="Dry run mode - don't save to database")

    args = parser.parse_args()

    # è§£æžå…³é”®è¯
    keywords = None
    if args.keyword:
        keywords = [args.keyword]
    elif args.keywords:
        keywords = [k.strip() for k in args.keywords.split(",") if k.strip()]

    # è¿è¡Œ
    if args.interval > 0:
        asyncio.run(run_continuous(
            keywords=keywords,
            min_likes=args.min_likes,
            days_back=args.days,
            hours_back=args.hours,
            interval_minutes=args.interval,
            dry_run=args.dry_run
        ))
    else:
        asyncio.run(search_viral_prompts(
            keywords=keywords,
            min_likes=args.min_likes,
            days_back=args.days,
            hours_back=args.hours,
            count_per_keyword=args.count,
            dry_run=args.dry_run
        ))


if __name__ == "__main__":
    main()
