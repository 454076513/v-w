#!/usr/bin/env python3
"""
OpenNana Prompt Gallery å¯¼å…¥è„šæœ¬

å·¥ä½œæµç¨‹:
1. ä» opennana.com è·å– prompts.json
2. è§£æå‡ºæœ‰ Twitter/X æ¥æºçš„ prompts
3. å°è¯•é€šè¿‡ main.py çš„ process_twitter_url æµç¨‹å¤„ç†
4. å¦‚æœ Twitter å¤„ç†å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ JSON æ•°æ®å…¥åº“

ç¯å¢ƒå˜é‡:
  DATABASE_URL - PostgreSQL è¿æ¥å­—ç¬¦ä¸² (å¿…éœ€)
  AI_MODEL     - AI æ¨¡å‹ (é»˜è®¤: openai)

ç”¨æ³•:
  python import_opennana.py                    # å¯¼å…¥æ‰€æœ‰æœ‰ X æ¥æºçš„ prompts
  python import_opennana.py --limit 10         # é™åˆ¶å¯¼å…¥æ•°é‡
  python import_opennana.py --skip-twitter     # è·³è¿‡ Twitter å¤„ç†ï¼Œç›´æ¥ç”¨åŸå§‹æ•°æ®
  python import_opennana.py --dry-run          # é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv

    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"
    
    if env_local.exists():
        load_dotenv(env_local)
        print(f"âœ“ å·²åŠ è½½: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"âœ“ å·²åŠ è½½: {env_file}")
except ImportError:
    pass

# å¯¼å…¥ä¸»æ¨¡å—çš„æ•°æ®åº“ç±»å’Œå¤„ç†å‡½æ•°
from main import Database, AI_MODEL

# AI å¤„ç†é€‚é…å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ prompt_utils)
from prompt_utils import process_tweet_for_import

# ========== é…ç½® ==========
# æ–° API ç«¯ç‚¹
OPENNANA_API_BASE = "https://api.opennana.com/api/prompts"
OPENNANA_LIST_API = OPENNANA_API_BASE  # GET ?page=1&limit=20&sort=created_at&order=DESC
# è¯¦æƒ… API: GET https://api.opennana.com/api/prompts/{slug}

DATABASE_URL = os.environ.get("DATABASE_URL", "")

# æ•°æ®ç¼“å­˜ç›®å½•
CACHE_DIR = Path(__file__).parent / "cache"
PROMPTS_CACHE_FILE = CACHE_DIR / "prompts.json"
PROGRESS_FILE = CACHE_DIR / "import_progress.json"

# å¤±è´¥è®°å½•è¾“å‡ºç›®å½•
FAILED_OUTPUT_DIR = Path(__file__).parent / "failed_imports"

# åˆ†ç±»ç”± process_tweet_for_import ç»Ÿä¸€å¤„ç†ï¼Œæ— éœ€å•ç‹¬å¯¼å…¥æ ‡ç­¾æ˜ å°„


def fetch_prompt_list(page: int = 1, limit: int = 100) -> Optional[Dict]:
    """
    è·å– prompt åˆ—è¡¨ï¼ˆå•é¡µï¼‰

    Args:
        page: é¡µç 
        limit: æ¯é¡µæ•°é‡

    Returns:
        API å“åº”æ•°æ®æˆ– None
    """
    url = f"{OPENNANA_LIST_API}?page={page}&limit={limit}&sort=created_at&order=DESC"

    try:
        response = requests.get(url, timeout=30, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Referer": "https://opennana.com/",
            "Origin": "https://opennana.com"
        })
        response.raise_for_status()
        data = response.json()

        if data.get("status") == 200:
            return data.get("data", {})
        else:
            print(f"âŒ API è¿”å›é”™è¯¯: {data.get('msg', 'Unknown error')}")
            return None
    except Exception as e:
        print(f"âŒ è·å–åˆ—è¡¨å¤±è´¥ (page={page}): {e}")
        return None


def fetch_prompt_detail(slug: str) -> Optional[Dict]:
    """
    è·å–å•ä¸ª prompt çš„è¯¦æƒ…

    Args:
        slug: prompt çš„ slugï¼Œå¦‚ "prompt-1128"

    Returns:
        è¯¦æƒ…æ•°æ®æˆ– None
    """
    url = f"{OPENNANA_API_BASE}/{slug}"

    try:
        response = requests.get(url, timeout=30, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Referer": "https://opennana.com/",
            "Origin": "https://opennana.com"
        })
        response.raise_for_status()
        data = response.json()

        if data.get("status") == 200:
            return data.get("data", {})
        else:
            return None
    except Exception as e:
        print(f"âš ï¸ è·å–è¯¦æƒ…å¤±è´¥ ({slug}): {e}")
        return None


def fetch_opennana_data(force_refresh: bool = False, fetch_details: bool = True, max_items: int = None, max_pages: int = 2, page_size: int = 20) -> Optional[Dict]:
    """
    ä» OpenNana æ–° API è·å–æ•°æ®ï¼Œæ”¯æŒæœ¬åœ°ç¼“å­˜

    Args:
        force_refresh: å¼ºåˆ¶ä»è¿œç¨‹è·å–ï¼Œå¿½ç•¥æœ¬åœ°ç¼“å­˜
        fetch_details: æ˜¯å¦è·å–è¯¦æƒ…ï¼ˆç”¨äºå®Œæ•´å¯¼å…¥ï¼‰
        max_items: æœ€å¤§è·å–æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
        max_pages: æœ€å¤§è·å–é¡µæ•°ï¼ˆé»˜è®¤ 2ï¼‰
        page_size: æ¯é¡µè·å–æ•°é‡ï¼ˆé»˜è®¤ 20ï¼‰

    Returns:
        æ ¼å¼åŒ–çš„æ•°æ®: {"total": int, "items": [...]}
    """
    # åˆ›å»ºç¼“å­˜ç›®å½•
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    if not force_refresh and PROMPTS_CACHE_FILE.exists():
        try:
            with open(PROMPTS_CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)

            total = data.get("total", len(data.get("items", [])))
            cache_time = PROMPTS_CACHE_FILE.stat().st_mtime
            cache_date = datetime.fromtimestamp(cache_time).strftime("%Y-%m-%d %H:%M:%S")

            print(f"ğŸ“¦ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {PROMPTS_CACHE_FILE}")
            print(f"   ç¼“å­˜æ—¶é—´: {cache_date}")
            print(f"   å…± {total} æ¡è®°å½•")
            print(f"   (ä½¿ç”¨ --refresh å¼ºåˆ¶æ›´æ–°ç¼“å­˜)")

            return data
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°è·å–...")

    # ä»æ–° API è·å–æ•°æ®
    print(f"ğŸ“¡ æ­£åœ¨ä»æ–° API è·å–æ•°æ®...")
    print(f"   åˆ—è¡¨ API: {OPENNANA_LIST_API}")
    print(f"   é…ç½®: max_pages={max_pages}, page_size={page_size}")

    all_items = []
    page = 1
    limit = page_size  # æ¯é¡µè·å–æ•°é‡

    # 1. å…ˆè·å–æ‰€æœ‰åˆ—è¡¨æ•°æ®
    while True:
        print(f"   ğŸ“„ è·å–åˆ—è¡¨ç¬¬ {page} é¡µ...")
        list_data = fetch_prompt_list(page=page, limit=limit)

        if not list_data:
            break

        items = list_data.get("items", [])
        pagination = list_data.get("pagination", {})

        if not items:
            break

        all_items.extend(items)

        total_pages = pagination.get("total_pages", 1)
        has_more = pagination.get("has_more", False)

        print(f"      è·å–åˆ° {len(items)} æ¡ï¼Œå…± {pagination.get('total', '?')} æ¡")

        # å¦‚æœè®¾ç½®äº†æœ€å¤§æ•°é‡é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è¾¾åˆ°
        if max_items and len(all_items) >= max_items:
            all_items = all_items[:max_items]
            print(f"   âš¡ è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ ({max_items})ï¼Œåœæ­¢è·å–åˆ—è¡¨")
            break

        # å¦‚æœè®¾ç½®äº†æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è¾¾åˆ°
        if max_pages and page >= max_pages:
            print(f"   âš¡ è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ ({max_pages} é¡µ)ï¼Œåœæ­¢è·å–åˆ—è¡¨")
            break

        if not has_more or page >= total_pages:
            break

        page += 1

    if not all_items:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
        return None

    print(f"âœ… åˆ—è¡¨è·å–å®Œæˆ: å…± {len(all_items)} æ¡")

    # 2. è·å–è¯¦æƒ…ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if fetch_details:
        print(f"ğŸ“¡ æ­£åœ¨è·å–è¯¦æƒ…...")
        detailed_items = []

        for i, item in enumerate(all_items, 1):
            slug = item.get("slug")
            if not slug:
                continue

            if i % 50 == 0 or i == len(all_items):
                print(f"   è¿›åº¦: {i}/{len(all_items)}")

            detail = fetch_prompt_detail(slug)
            if detail:
                # è½¬æ¢ä¸ºå…¼å®¹æ—§æ ¼å¼çš„æ•°æ®ç»“æ„
                converted = convert_to_legacy_format(detail)
                detailed_items.append(converted)
            else:
                # è¯¦æƒ…è·å–å¤±è´¥ï¼Œä½¿ç”¨åˆ—è¡¨ä¸­çš„åŸºç¡€æ•°æ®
                detailed_items.append({
                    "id": item.get("id"),
                    "slug": slug,
                    "title": item.get("title", "Untitled"),
                    "images": [item.get("cover_image")] if item.get("cover_image") else [],
                    "prompts": [],
                    "tags": [],
                    "source": None
                })

        all_items = detailed_items
        print(f"âœ… è¯¦æƒ…è·å–å®Œæˆ: {len(detailed_items)} æ¡")

    # æ„å»ºè¿”å›æ•°æ®
    result = {
        "total": len(all_items),
        "items": all_items
    }

    # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
    try:
        with open(PROMPTS_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ç¼“å­˜åˆ°: {PROMPTS_CACHE_FILE}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    return result


def convert_to_legacy_format(detail: Dict) -> Dict:
    """
    å°†æ–° API çš„è¯¦æƒ…æ•°æ®è½¬æ¢ä¸ºå…¼å®¹æ—§æ ¼å¼çš„æ•°æ®ç»“æ„

    æ–° API å­—æ®µ:
        - source_url: Twitter/X é“¾æ¥
        - source_name: ä½œè€…å
        - prompts: [{text, type}] æ•°ç»„
        - images: å›¾ç‰‡ URL æ•°ç»„
        - tags: æ ‡ç­¾æ•°ç»„

    æ—§æ ¼å¼å­—æ®µ:
        - source: {url, name}
        - prompts: [string] æ•°ç»„
        - images: [string] æ•°ç»„ (ç›¸å¯¹è·¯å¾„)
        - tags: [string] æ•°ç»„
    """
    # æå–æç¤ºè¯æ–‡æœ¬ï¼ˆä¼˜å…ˆè‹±æ–‡ï¼Œå¦åˆ™ä¸­æ–‡ï¼‰
    prompts_data = detail.get("prompts", [])
    prompt_texts = []

    # ä¼˜å…ˆä½¿ç”¨è‹±æ–‡æç¤ºè¯
    for p in prompts_data:
        if p.get("type") == "en" and p.get("text"):
            prompt_texts.append(p["text"])
            break

    # å¦‚æœæ²¡æœ‰è‹±æ–‡ï¼Œä½¿ç”¨ä¸­æ–‡
    if not prompt_texts:
        for p in prompts_data:
            if p.get("text"):
                prompt_texts.append(p["text"])
                break

    # æ„å»º source å¯¹è±¡
    source = None
    if detail.get("source_url"):
        source = {
            "url": detail.get("source_url"),
            "name": detail.get("source_name", "")
        }

    return {
        "id": detail.get("id"),
        "slug": detail.get("slug"),
        "title": detail.get("title", "Untitled"),
        "prompts": prompt_texts,
        "images": detail.get("images", []),
        "tags": detail.get("tags", []),
        "source": source,
        "model": detail.get("model"),
        # ä¿ç•™åŸå§‹æ•°æ®ä¾›éœ€è¦æ—¶ä½¿ç”¨
        "_raw_prompts": prompts_data
    }


def load_progress() -> Dict:
    """åŠ è½½å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {"processed_ids": [], "last_updated": None}


def save_progress(progress: Dict):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    progress["last_updated"] = datetime.now(timezone.utc).isoformat()
    
    try:
        with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")


def clear_progress():
    """æ¸…é™¤å¤„ç†è¿›åº¦"""
    if PROGRESS_FILE.exists():
        PROGRESS_FILE.unlink()
        print("ğŸ—‘ï¸ å·²æ¸…é™¤å¤„ç†è¿›åº¦")


def extract_twitter_url(source: Dict) -> Optional[str]:
    """ä» source å¯¹è±¡ä¸­æå– Twitter/X URL"""
    if not source:
        return None
    
    url = source.get("url", "")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ Twitter/X é“¾æ¥
    if re.match(r"https?://(?:www\.)?(?:twitter\.com|x\.com)/\w+/status/\d+", url):
        # æ ‡å‡†åŒ–ä¸º x.com
        return url.replace("twitter.com", "x.com")
    
    return None


def process_opennana_item(db: Database, item: Dict, skip_twitter: bool = False, dry_run: bool = False) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ª OpenNana æ¡ç›® - ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°

    ç­–ç•¥:
    - å¿…é¡»æœ‰ Twitter URL ä¸”èƒ½è·å–å›¾ç‰‡æ‰å…¥åº“
    - ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•° process_tweet_for_import

    è¿”å›: {"success": bool, "method": str, "error": str or None, "twitter_failed": bool}
    """
    source = item.get("source") or {}

    # æå– Twitter URL
    twitter_url = extract_twitter_url(source)

    # è·å–åŸå§‹æç¤ºè¯
    prompts = item.get("prompts", [])
    raw_prompt = prompts[0] if prompts else ""

    if not raw_prompt:
        return {"success": False, "method": "skipped", "error": "No prompt text", "twitter_failed": False}

    # å¿…é¡»æœ‰ Twitter URL
    if not twitter_url:
        return {"success": False, "method": "skipped", "error": "No Twitter URL", "twitter_failed": False}

    # ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    result = process_tweet_for_import(
        db=db,
        tweet_url=twitter_url,
        raw_text=raw_prompt,
        import_source="opennana",
        ai_model=AI_MODEL,
        dry_run=dry_run
    )

    return result


def save_failed_twitter_items(failed_twitter_items: List[Dict], timestamp: str):
    """ä¿å­˜ Twitter å¤„ç†å¤±è´¥çš„æ¡ç›®åˆ°æ–‡ä»¶ä¾›äººå·¥å¤„ç†"""
    if not failed_twitter_items:
        return None
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    FAILED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"twitter_failed_{timestamp}.json"
    filepath = FAILED_OUTPUT_DIR / filename
    
    # ä¿å­˜ä¸º JSON æ–‡ä»¶
    output_data = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "total": len(failed_twitter_items),
        "description": "Twitter å›¾ç‰‡è·å–å¤±è´¥çš„æ¡ç›®ï¼ˆæœªå…¥åº“ï¼‰",
        "instructions": [
            "è¿™äº›æ¡ç›®çš„ Twitter å›¾ç‰‡è·å–å¤±è´¥ï¼Œæœªå…¥åº“",
            "äººå·¥å¤„ç†æ­¥éª¤:",
            "1. è®¿é—® twitter_url è·å–é«˜æ¸…å›¾ç‰‡ URL",
            "2. æ‰‹åŠ¨å…¥åº“æˆ–ä½¿ç”¨è„šæœ¬å¤„ç†",
            "3. æˆ–ä½¿ç”¨ --skip-twitter è·³è¿‡ Twitter å¤„ç†ï¼Œç›´æ¥ç”¨ OpenNana å›¾ç‰‡å…¥åº“"
        ],
        "items": failed_twitter_items
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    return filepath


def run_import(limit: int = None, skip_twitter: bool = False, dry_run: bool = False,
               only_twitter: bool = False, start_id: int = None, force_refresh: bool = False,
               resume: bool = True, reset_progress: bool = False,
               max_pages: int = 2, page_size: int = 20):
    """
    è¿è¡Œå¯¼å…¥æµç¨‹

    Args:
        limit: é™åˆ¶å¤„ç†æ•°é‡
        skip_twitter: è·³è¿‡ Twitter å¤„ç†
        dry_run: é¢„è§ˆæ¨¡å¼
        only_twitter: ä»…å¤„ç†æœ‰ X æ¥æºçš„
        start_id: ä»æŒ‡å®š ID å¼€å§‹
        force_refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        resume: æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤å¼€å¯ï¼‰
        reset_progress: é‡ç½®è¿›åº¦
        max_pages: æœ€å¤§è·å–é¡µæ•°
        page_size: æ¯é¡µè·å–æ•°é‡
    """
    print("=" * 70)
    print("ğŸ“¦ OpenNana Prompt Gallery å¯¼å…¥")
    print("=" * 70)
    print(f"æ•°æ®æº: {OPENNANA_API_BASE}")
    print(f"è·å–é…ç½®: max_pages={max_pages}, page_size={page_size}")
    print(f"è·³è¿‡ Twitter å¤„ç†: {skip_twitter}")
    print(f"ä»…å¤„ç†æœ‰ X æ¥æºçš„: {only_twitter}")
    print(f"é¢„è§ˆæ¨¡å¼: {dry_run}")
    print(f"æ–­ç‚¹ç»­ä¼ : {resume}")
    if limit:
        print(f"é™åˆ¶æ•°é‡: {limit}")
    if start_id:
        print(f"èµ·å§‹ ID: {start_id}")
    print("=" * 70)
    
    # é‡ç½®è¿›åº¦
    if reset_progress:
        clear_progress()
    
    # æ£€æŸ¥é…ç½®
    if not DATABASE_URL:
        print("âŒ ç¼ºå°‘ DATABASE_URL ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # è·å–æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
    data = fetch_opennana_data(force_refresh=force_refresh, max_pages=max_pages, page_size=page_size)
    if not data:
        sys.exit(1)
    
    items = data.get("items", [])
    
    # å¦‚æœåªå¤„ç†æœ‰ Twitter æ¥æºçš„
    if only_twitter:
        items = [item for item in items if extract_twitter_url(item.get("source"))]
        print(f"ğŸ“Š æœ‰ X æ¥æºçš„æ¡ç›®: {len(items)}")
    
    # æŒ‰ ID ä»å°åˆ°å¤§æ’åº
    items = sorted(items, key=lambda x: x.get("id", 0))
    print(f"ğŸ“Š æŒ‰ ID å‡åºæ’åˆ—")
    
    # å¦‚æœæŒ‡å®šäº†èµ·å§‹ ID
    if start_id:
        items = [item for item in items if item.get("id", 0) >= start_id]
        print(f"ğŸ“Š ID >= {start_id} çš„æ¡ç›®: {len(items)}")
    
    # åŠ è½½è¿›åº¦ï¼Œè¿‡æ»¤å·²å¤„ç†çš„æ¡ç›®
    progress = load_progress()
    processed_ids = set(progress.get("processed_ids", []))
    
    if resume and processed_ids:
        original_count = len(items)
        items = [item for item in items if item.get("id") not in processed_ids]
        skipped_count = original_count - len(items)
        
        if skipped_count > 0:
            print(f"ğŸ“Š å·²å¤„ç†ï¼ˆè·³è¿‡ï¼‰: {skipped_count}")
            print(f"   ä¸Šæ¬¡æ›´æ–°: {progress.get('last_updated', 'N/A')}")
    
    # é™åˆ¶æ•°é‡
    if limit:
        items = items[:limit]
    
    total_items = len(items)
    print(f"\nğŸ”„ å‡†å¤‡å¤„ç† {total_items} æ¡è®°å½•...\n")
    
    if total_items == 0:
        print("âœ… æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•")
        return
    
    # è¿æ¥æ•°æ®åº“
    db = Database(DATABASE_URL)
    
    try:
        db.connect()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        sys.exit(1)
    
    # ç»Ÿè®¡
    stats = {
        "total": len(items),
        "processed": 0,
        "success_twitter": 0,
        "success_json": 0,
        "skipped": 0,
        "failed": 0,
        "twitter_failed": 0,
    }
    
    failed_items = []
    failed_twitter_items = []  # Twitter å¤„ç†å¤±è´¥çš„æ¡ç›®
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å®šæœŸåˆ·æ–°å¤±è´¥æ–‡ä»¶çš„é—´éš”ï¼ˆæ¯ 20 æ¡ï¼‰
    FLUSH_INTERVAL = 20
    failed_file = None
    
    try:
        for i, item in enumerate(items, 1):
            item_id = item.get("id", "?")
            title = item.get("title", "Untitled")[:40]
            source = item.get("source") or {}
            twitter_url = extract_twitter_url(source)
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            progress_pct = (i / total_items) * 100
            print(f"[{i}/{total_items}] ({progress_pct:.1f}%) ID={item_id}: {title}")
            
            if twitter_url:
                print(f"   ğŸ”— X: {twitter_url}")
            
            result = process_opennana_item(db, item, skip_twitter=skip_twitter, dry_run=dry_run)
            stats["processed"] += 1
            
            # è®°å½• Twitter å¤„ç†å¤±è´¥çš„æ¡ç›®
            if result.get("twitter_failed"):
                stats["twitter_failed"] += 1

                # æ–° API è¿”å›çš„æ˜¯å®Œæ•´å›¾ç‰‡ URL
                images = item.get("images", [])
                
                failed_twitter_items.append({
                    "id": item_id,
                    "title": item.get("title", "Untitled"),
                    "twitter_url": twitter_url,
                    "error": result.get("twitter_error", "Unknown error"),
                    "saved_to_db": result.get("success", False),  # æ˜¯å¦å·²å…¥åº“ï¼ˆä½¿ç”¨å¤‡ç”¨å›¾ç‰‡ï¼‰
                    # ç”¨äºäººå·¥å¤„ç†çš„å…³é”®æ•°æ®
                    "prompt_preview": (item.get("prompts", [""])[0][:200] + "...") if item.get("prompts") else None,
                    "full_prompt": item.get("prompts", [""])[0] if item.get("prompts") else None,  # å®Œæ•´æç¤ºè¯
                    "images": images[:5],  # ä¿ç•™å‰5å¼ å›¾ç‰‡URL
                    "tags": item.get("tags", []),
                    "model": item.get("model"),
                    "source_name": (item.get("source") or {}).get("name"),
                })
            
            if result["success"]:
                if result["method"] == "hybrid":
                    stats["success_twitter"] += 1
                    print(f"   âœ… æˆåŠŸå…¥åº“ (Twitterå›¾ç‰‡+AIåˆ†ç±»)")
                elif result["method"] == "json_direct":
                    stats["success_json"] += 1
                    print(f"   âœ… æˆåŠŸå…¥åº“ (OpenNanaå›¾ç‰‡+AIåˆ†ç±»)")
                elif result["method"] == "dry_run":
                    print(f"   âœ… é¢„è§ˆé€šè¿‡")
            else:
                if result["method"] == "skipped":
                    stats["skipped"] += 1
                    print(f"   â­ï¸ è·³è¿‡: {result['error']}")
                elif result["method"] == "twitter_failed":
                    # Twitter å¤±è´¥ï¼Œä¸å…¥åº“ï¼Œè®°å½•åˆ°æ–‡ä»¶
                    print(f"   ğŸ“ è®°å½•åˆ°å¤±è´¥æ–‡ä»¶ (Twitterå›¾ç‰‡è·å–å¤±è´¥)")
                elif result["method"] == "save_failed":
                    stats["failed"] += 1
                    failed_items.append({"id": item_id, "title": title, "error": result["error"]})
                    print(f"   âŒ ä¿å­˜å¤±è´¥: {result['error']}")
                else:
                    stats["failed"] += 1
                    failed_items.append({"id": item_id, "title": title, "error": result["error"]})
                    print(f"   âŒ å¤±è´¥: {result['error']}")
            
            # ä¿å­˜è¿›åº¦ï¼ˆæ¯å¤„ç†ä¸€æ¡å°±ä¿å­˜ï¼Œæ”¯æŒä¸­æ–­ç»­ä¼ ï¼‰
            if not dry_run and item_id != "?":
                processed_ids.add(item_id)
                # æ¯å¤„ç† 10 æ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦ï¼Œå‡å°‘ IO
                if i % 10 == 0 or i == total_items:
                    save_progress({"processed_ids": list(processed_ids)})
            
            # å®šæœŸåˆ·æ–°å¤±è´¥æ–‡ä»¶ï¼ˆæ¯ FLUSH_INTERVAL æ¡æˆ–æœ€åä¸€æ¡ï¼‰
            if not dry_run and failed_twitter_items and (i % FLUSH_INTERVAL == 0 or i == total_items):
                failed_file = save_failed_twitter_items(failed_twitter_items, timestamp)
                print(f"   ğŸ’¾ å·²åˆ·æ–°å¤±è´¥è®°å½•åˆ°æ–‡ä»¶ ({len(failed_twitter_items)} æ¡)")
            
            print()
        
        # æœ€ç»ˆä¿å­˜ Twitter å¤„ç†å¤±è´¥çš„æ¡ç›®åˆ°æ–‡ä»¶
        if failed_twitter_items and not dry_run:
            failed_file = save_failed_twitter_items(failed_twitter_items, timestamp)
        
        # è¾“å‡ºç»Ÿè®¡
        print("=" * 70)
        print("ğŸ“Š å¯¼å…¥å®Œæˆ - ç»Ÿè®¡æ±‡æ€»")
        print("=" * 70)
        print(f"\næ€»è®¡: {stats['total']}")
        print(f"å·²å¤„ç†: {stats['processed']}")
        print(f"âœ… æˆåŠŸ (Twitter): {stats['success_twitter']}")
        print(f"âœ… æˆåŠŸ (JSON): {stats['success_json']}")
        print(f"â­ï¸ è·³è¿‡: {stats['skipped']}")
        print(f"âŒ å¤±è´¥: {stats['failed']}")
        print(f"âš ï¸ Twitter å¤„ç†å¤±è´¥: {stats['twitter_failed']}")
        
        if failed_items:
            print("\n" + "=" * 70)
            print("âŒ å®Œå…¨å¤±è´¥çš„æ¡ç›®:")
            print("=" * 70)
            for item in failed_items[:10]:
                print(f"   ID={item['id']}: {item['title']}")
                print(f"   é”™è¯¯: {item['error']}")
                print()
            if len(failed_items) > 10:
                print(f"   ... è¿˜æœ‰ {len(failed_items) - 10} æ¡å¤±è´¥è®°å½•")
        
        # æ˜¾ç¤ºå¤±è´¥æ–‡ä»¶ä¿¡æ¯
        if failed_file:
            print("\n" + "=" * 70)
            print("ğŸ“ Twitter å›¾ç‰‡è·å–å¤±è´¥çš„æ¡ç›®å·²ä¿å­˜:")
            print("=" * 70)
            print(f"   æ–‡ä»¶: {failed_file}")
            print(f"   æ•°é‡: {len(failed_twitter_items)}")
            print(f"   è¯´æ˜: è¿™äº›æ¡ç›®æœªå…¥åº“ï¼Œéœ€è¦äººå·¥å¤„ç†")
            print(f"         æˆ–ä½¿ç”¨ --skip-twitter è·³è¿‡ Twitter ç›´æ¥å…¥åº“")
        
        print("\n" + "=" * 70)
        
    finally:
        db.close()


def main():
    parser = argparse.ArgumentParser(
        description="ä» OpenNana Prompt Gallery å¯¼å…¥æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å…¥æ‰€æœ‰æ•°æ®ï¼ˆè‡ªåŠ¨æ–­ç‚¹ç»­ä¼ ï¼‰
  python import_opennana.py
  
  # ä»…å¯¼å…¥æœ‰ X æ¥æºçš„ï¼Œé™åˆ¶ 10 æ¡
  python import_opennana.py --only-twitter --limit 10
  
  # è·³è¿‡ Twitter å¤„ç†ï¼Œç›´æ¥ç”¨ JSON æ•°æ®
  python import_opennana.py --skip-twitter
  
  # å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
  python import_opennana.py --refresh
  
  # é‡ç½®è¿›åº¦ï¼Œä»å¤´å¼€å§‹
  python import_opennana.py --reset
  
  # ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ 
  python import_opennana.py --no-resume
  
  # é¢„è§ˆæ¨¡å¼
  python import_opennana.py --dry-run --limit 5

ç¼“å­˜æ–‡ä»¶:
  worker/cache/prompts.json        - JSON æ•°æ®ç¼“å­˜
  worker/cache/import_progress.json - å¤„ç†è¿›åº¦
        """
    )
    
    parser.add_argument("--limit", "-l", type=int, help="é™åˆ¶å¯¼å…¥æ•°é‡")
    parser.add_argument("--skip-twitter", "-s", action="store_true", 
                        help="è·³è¿‡ Twitter å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ JSON æ•°æ®")
    parser.add_argument("--only-twitter", "-t", action="store_true",
                        help="ä»…å¤„ç†æœ‰ X æ¥æºçš„æ¡ç›®")
    parser.add_argument("--dry-run", "-d", action="store_true",
                        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“")
    parser.add_argument("--start-id", type=int,
                        help="ä»æŒ‡å®š ID å¼€å§‹å¤„ç† (ID ä»å¤§åˆ°å°)")
    parser.add_argument("--refresh", "-r", action="store_true",
                        help="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼Œé‡æ–°ä¸‹è½½ prompts.json")
    parser.add_argument("--no-resume", action="store_true",
                        help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œå¤„ç†æ‰€æœ‰æ¡ç›®")
    parser.add_argument("--reset", action="store_true",
                        help="é‡ç½®è¿›åº¦ï¼Œä»å¤´å¼€å§‹å¤„ç†")
    parser.add_argument("--max-pages", type=int, default=2,
                        help="æœ€å¤§è·å–é¡µæ•° (é»˜è®¤: 2)")
    parser.add_argument("--page-size", type=int, default=20,
                        help="æ¯é¡µè·å–æ•°é‡ (é»˜è®¤: 20)")

    args = parser.parse_args()

    run_import(
        limit=args.limit,
        skip_twitter=args.skip_twitter,
        dry_run=args.dry_run,
        only_twitter=args.only_twitter,
        start_id=args.start_id,
        force_refresh=args.refresh,
        resume=not args.no_resume,
        reset_progress=args.reset,
        max_pages=args.max_pages,
        page_size=args.page_size
    )


if __name__ == "__main__":
    main()

