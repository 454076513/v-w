#!/usr/bin/env python3
"""
X/Twitter AI Art Account Monitor
ç›‘å¬ AI è‰ºæœ¯è´¦å·ï¼Œè‡ªåŠ¨æå–æç¤ºè¯å¹¶å…¥åº“

æŠ€æœ¯æ–¹æ¡ˆ:
- twikit: ä½¿ç”¨ X è´¦å· cookies è·å–ç”¨æˆ·æ—¶é—´çº¿
- FxTwitter/VxTwitter API: è·å–æ¨æ–‡è¯¦æƒ…å’Œäº’åŠ¨æ•°æ® (å¤‡ç”¨)

ä½¿ç”¨æ–¹æ³•:
    # è¿è¡Œç›‘å¬ (å•æ¬¡)
    python fetch_x_accounts.py

    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„é«˜é¢‘ä½œè€…
    python fetch_x_accounts.py --top 20

    # æŒç»­ç›‘å¬ (æ¯ N åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡)
    python fetch_x_accounts.py --interval 30

    # åªå¤„ç†çˆ†æ¬¾æ¨æ–‡
    python fetch_x_accounts.py --viral-only

    # é¢„è§ˆæ¨¡å¼ (ä¸å†™å…¥æ•°æ®åº“)
    python fetch_x_accounts.py --dry-run

    # ç›‘å¬ç‰¹å®šè´¦å·
    python fetch_x_accounts.py --accounts midjourney,openai

ç¯å¢ƒå˜é‡:
    DATABASE_URL        - PostgreSQL è¿æ¥å­—ç¬¦ä¸² (å¿…éœ€)
    AI_MODEL            - AI æ¨¡å‹ (é»˜è®¤: openai)
    X_COOKIE            - X è´¦å· cookies JSON (æ¨è): '{"auth_token": "xxx", "ct0": "xxx"}'
    X_USERNAME          - X è´¦å·ç”¨æˆ·å (å¤‡ç”¨ç™»å½•æ–¹å¼)
    X_EMAIL             - X è´¦å·é‚®ç®±
    X_PASSWORD          - X è´¦å·å¯†ç 
"""

import argparse
import asyncio
import json
import os
import random
import re
import sys
import tempfile
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"[env] Loaded: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"[env] Loaded: {env_file}")
except ImportError:
    pass

# twikit
try:
    from twikit import Client
    HAS_TWIKIT = True
except ImportError:
    HAS_TWIKIT = False
    print("[Warning] twikit not installed. Run: pip install twikit")

# HTTP è¯·æ±‚
import requests
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

# å¯¼å…¥ AI å¤„ç†å‡½æ•° (ç»Ÿä¸€ä½¿ç”¨ prompt_utils)
from prompt_utils import DEFAULT_MODEL, process_tweet_for_import

# å¯¼å…¥ Twitter API å‡½æ•°
from fetch_twitter_content import (
    fetch_with_fxtwitter,
    fetch_with_vxtwitter,
    parse_fxtwitter_result,
    parse_vxtwitter_result,
)

# æ•°æ®åº“
try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("Please install psycopg2: pip install psycopg2-binary")
    sys.exit(1)

# ========== é…ç½® ==========

DATABASE_URL = os.environ.get("DATABASE_URL", "")
AI_MODEL = os.environ.get("AI_MODEL", DEFAULT_MODEL)

# X è´¦å·å‡­è¯
X_USERNAME = os.environ.get("X_USERNAME", "")
X_EMAIL = os.environ.get("X_EMAIL", "")
X_PASSWORD = os.environ.get("X_PASSWORD", "")

# Cookies é…ç½® (ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡)
X_COOKIE = os.environ.get("X_COOKIE", "")  # JSON å­—ç¬¦ä¸²: '{"auth_token": "xxx", "ct0": "xxx"}'
COOKIES_FILE = Path(__file__).parent / "x_cookies.json"

# ä»£ç†é…ç½® (å¦‚æœéœ€è¦)
PROXY_URL = os.environ.get("X_PROXY", "")

# çŠ¶æ€æ–‡ä»¶ (è®°å½•å·²å¤„ç†çš„æ¨æ–‡ ID)
STATE_FILE = Path(__file__).parent / "x_monitor_state.json"

# ========== é˜²åˆ·é…ç½® (Rate Limit) ==========
# å‚è€ƒ: https://github.com/d60/twikit/blob/main/ratelimits.md

# è¯·æ±‚é—´éš”é…ç½® (ç§’)
DELAY_BETWEEN_TWEETS = (2, 5)       # å¤„ç†æ¯æ¡æ¨æ–‡åçš„éšæœºå»¶è¿Ÿ
DELAY_BETWEEN_ACCOUNTS = (5, 10)    # åˆ‡æ¢è´¦å·æ—¶çš„éšæœºå»¶è¿Ÿ
DELAY_ON_RATE_LIMIT = 60            # é‡åˆ°é™æµæ—¶çš„åŸºç¡€ç­‰å¾…æ—¶é—´
MAX_RETRIES_ON_RATE_LIMIT = 3       # é™æµé‡è¯•æ¬¡æ•°
DELAY_BETWEEN_API_CALLS = (1, 3)    # API è°ƒç”¨é—´éš”

# é»˜è®¤ç›‘å¬çš„ AI è‰ºæœ¯è´¦å· (åŸºäºæ•°æ®åº“é«˜é¢‘ç»Ÿè®¡ + twitterhot.vercel.app)
DEFAULT_ACCOUNTS = [
    # Top 20 é«˜é¢‘è´¦å· (ä»æ•°æ®åº“ç»Ÿè®¡)
    "songguoxiansen",    # #1 - 144 prompts
    "Gdgtify",           # #2 - 123 prompts
    "Ankit_patel211",    # #3 - 99 prompts
    "dotey",             # #4 - 96 prompts
    "azed_ai",           # #5 - 85 prompts
    "lexx_aura",         # #6 - 79 prompts
    "YaseenK7212",       # #7 - 79 prompts
    "saniaspeaks_",      # #8 - 78 prompts
    "ZaraIrahh",         # #9 - 75 prompts
    "Just_sharon7",      # #10 - 74 prompts
    "xmliisu",           # #11 - 71 prompts
    "Vivekhy",           # #12 - 64 prompts
    "astronomerozge1",   # #13 - 64 prompts
    "siennalovesai",     # #14 - 61 prompts
    "Strength04_X",      # #15 - 60 prompts
    "aleenaamiir",       # #16 - 57 prompts
    "SimplyAnnisa",      # #17 - 54 prompts
    "umesh_ai",          # #18 - 53 prompts
    "oggii_0",           # #19 - 44 prompts
    "xmiiru_",           # #20 - 43 prompts
    # === ä»¥ä¸‹è´¦å·æ¥è‡ª twitterhot.vercel.app (2026-01-10 å¯¼å…¥) ===
    "0xInk_",
    "0xbisc",
    "369labsx",
    "3DVR3",
    "4on_yon_x",
    "94vanAI",
    "AIFrontliner",
    "AIMevzulari",
    "AIwithSynthia",
    "AIwithkhan",
    "Adam38363368936",
    "AdemVessell",
    "AleRVG",
    "AllaAisling",
    "AllarHaltsonen",
    "AltugAkgul",
    "AmirMushich",
    "Angaisb_",
    "Arminn_Ai",
    "BeanieBlossom",
    "BeautyVerse_Lab",
    "Bitturing",
    "BrettFromDJ",
    "CaptainHaHaa",
    "CharaspowerAI",
    "ChatgptAIskill",
    "ChillaiKalan__",
    "Citrini7",
    "ClaireSilver12",
    "Creatify_AI",
    "Cydiar404",
    "D_studioproject",
    "Dari_Designs",
    "David_eficaz",
    "DilumSanjaya",
    "DmitryLepisov",
    "DrFonts",
    "EHuanglu",
    "FitzGPT",
    "FlowbyGoogle",
    "FuSheng_0306",
    "GammaApp",
    "GeminiApp",
    "GlitterPixely",
    "GoogleLabs",
    "Gorden_Sun",
    "HBCoop_",
    "Harboris_27",
    "IamEmily2050",
    "Ibrahim56072637",
    "IqraSaifiii",
    "JZhen72937",
    "JasonBud",
    "JefferyTatsuya",
    "Jimmy_JingLv",
    "KanaWorks_AI",
    "KeorUnreal",
    "Kerroudjm",
    "KusoPhoto",
    "LZhou15365",
    "LiEvanna85716",
    "Limorio_",
    "LinusEkenstam",
    "LudovicCreator",
    "LufzzLiz",
    "MANISH1027512",
    "Me_Rock369",
    "Mho_23",
    "MissMi1973",
    "MrDavids1",
    "Mr_AllenT",
    "NahFlo2n",
    "Naiknelofar788",
    "NanoBanana",
    "NanoBanana_labs",
    "Noguma_Morino",
    "OTFHD",
    "OdinLovis",
    "Ok_shuai",
    "OkunevUA",
    "PolymarketMoney",
    "RAVIKUMARSAHU78",
    "Raylan89",
    "ReflctWillie",
    "RobotCleopatra",
    "Ror_Fly",
    "SDT_side",
    "SSSS_CRYPTOMAN",
    "Saboo_Shubham_",
    "Saccc_c",
    "Samann_ai",
    "Sheldon056",
    "Shimayus",
    "ShreyaYadav___",
    "SiboEsenkova",
    "Taaruk_",
    "TechByMarkandey",
    "TechieBySA",
    "TheMattBerman",
    "The_Sycomore",
    "Tz_2022",
    "VibeMarketer_",
    "Whizz_ai",
    "WuxiaRocks",
    "YZOkulu",
    "ZHO_ZHO_ZHO",
    "Zar_xplorer",
    "_3912657840",
    "_MehdiSharifi_",
    "_smcf",
    "aaliya_va",
    "ai_for_success",
    "aiwarts",
    "anandh_ks_",
    "anvishapai",
    "anzedetn",
    "archi_reum",
    "artisin_ai",
    "asdfghdevv",
    "avstudiosng",
    "ayami_marketing",
    "aziz4ai",
    "bananababydoll",
    "beechinour",
    "beginnersblog1",
    "berryxia",
    "berryxia_ai",
    "bindureddy",
    "bobbykun_banana",
    "bozhou_ai",
    "brad_zhang2024",
    "canghecode",
    "cartunmafia",
    "cfryant",
    "cheerselflin",
    "cheese_ai07",
    "chengzi_95330",
    "chillhousedev",
    "cnyzgkc",
    "condzxyz",
    "craftian_keskin",
    "design_with_ayo",
    "develogon0",
    "dhumann",
    "dr_cintas",
    "ducktheaff",
    "ecommartinez",
    "egeberkina",
    "elCarlosVega",
    "emollick",
    "eveningbtc",
    "eviljer",
    "excel_niisan",
    "fAIkout",
    "felo_ai",
    "firatbilal",
    "firemadeher",
    "fofrAI",
    "freddier",
    "futamen_0308",
    "gaucheai",
    "genel_ai",
    "genspark_ai",
    "genspark_japan",
    "ghumare64",
    "gisellaesthetic",
    "gizakdag",
    "glennwrites1",
    "gokayfem",
    "goo_vision",
    "guicastellanos1",
    "hAru_mAki_ch",
    "harboriis",
    "hckmstrrahul",
    "helinvision",
    "henrydaubrez",
    "higgsfield_ai",
    "hx831126",
    "iX00AI",
    "iam_vampire_0",
    "iamsofiaijaz",
    "iamtonyzhu",
    "icreatelife",
    "imxiaohu",
    "itis_Jarvo33",
    "jamesyeung18",
    "john_my07",
    "kaanakz",
    "kabu_st0ck",
    "karim_yourself",
    "kashmir_ki_lark",
    "kei31",
    "kingofdairyque",
    "kohaku_00",
    "ksmhope",
    "langzihan",
    "learn2vibe",
    "linxiaobei888",
    "loveko28516",
    "madebygoogle",
    "manerun_",
    "mattiapomelli",
    "maxescu",
    "med3bbas",
    "meng_dagg695",
    "michaelrabone",
    "miilesus",
    "milbon_",
    "mimi_aiart",
    "minchoi",
    "miyabi_foxx",
    "mmmiyama_D",
    "monicamoonx",
    "moshimoshi_ai",
    "msjiaozhu",
    "munou_ac",
    "nabe1975",
    "nagano_yoh",
    "nimentrix",
    "ninohut",
    "notoro_ai",
    "old_pgmrs_will",
    "op7418",
    "oreno_musume",
    "osanseviero",
    "ozan_sihay",
    "paularambles",
    "qisi_ai",
    "r4jjesh",
    "ratman_aiillust",
    "rionaifantasy",
    "rovvmut_",
    "ryosan1904",
    "s_tiva",
    "sasakitoshinao",
    "schnapoon",
    "serena_ailab",
    "sergeantsref",
    "shota7180",
    "showheyohtaki",
    "sidharthgehlot",
    "so_ainsight",
    "sodaguyx",
    "sonucnc2",
    "ss_uulq09",
    "stitchbygoogle",
    "studio_veco",
    "sudharps",
    "sundarpichai",
    "sundyme",
    "taiyo_ai_gakuse",
    "techhalla",
    "tegnike",
    "testingcatalog",
    "threeaus",
    "tisch_eins",
    "tkm_hmng8",
    "treydtw",
    "trxuanxw",
    "tsyn18",
    "ttmouse",
    "tuzi_ai",
    "underwoodxie96",
    "venturetwins",
    "wad0427",
    "wanerfu",
    "xiaojietongxue",
    "yachimat_manga",
    "yammmy_hedgehog",
    "yanhua1010",
    "youraipulse",
    "yuanzhe68949664",
    "yyyole",
]


# ========== æ•°æ®åº“æ“ä½œ ==========

class Database:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.conn = None

    def connect(self):
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(self.connection_string)
        return self.conn

    def close(self):
        if self.conn and not self.conn.closed:
            self.conn.close()

    def execute_write(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            conn.commit()
            if cur.description:
                result = cur.fetchone()
                return dict(result) if result else None
            return None

    def execute_one(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            result = cur.fetchone()
            return dict(result) if result else None

    def prompt_exists(self, source_link: str) -> bool:
        result = self.execute_one(
            "SELECT id FROM prompts WHERE source_link = %s",
            (source_link,)
        )
        return result is not None

    def save_prompt(self, title: str, prompt: str, category: str,
                    tags: List[str], images: List[str], source_link: str,
                    author: str = None, import_source: str = None) -> Optional[Dict]:
        return self.execute_write(
            """
            INSERT INTO prompts (title, prompt, category, tags, images, source_link, author, import_source)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING *
            """,
            (title, prompt, category, tags or [], images or [], source_link, author, import_source)
        )

    def get_top_authors(self, limit: int = 30) -> List[Dict]:
        """è·å–é«˜é¢‘ä½œè€…åˆ—è¡¨"""
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute("""
                SELECT author, COUNT(*) as count
                FROM prompts
                WHERE author IS NOT NULL
                  AND author != ''
                  AND deleted_at IS NULL
                GROUP BY author
                ORDER BY count DESC
                LIMIT %s
            """, (limit,))
            return [dict(row) for row in cur.fetchall()]


# ========== çŠ¶æ€ç®¡ç† ==========

def load_state() -> Dict:
    """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
        except:
            pass
    return {"processed_tweets": [], "last_check": None}


def save_state(state: Dict):
    """ä¿å­˜çŠ¶æ€æ–‡ä»¶"""
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)


def is_tweet_processed(state: Dict, tweet_id: str) -> bool:
    """æ£€æŸ¥æ¨æ–‡æ˜¯å¦å·²å¤„ç†"""
    return tweet_id in state.get("processed_tweets", [])


def mark_tweet_processed(state: Dict, tweet_id: str):
    """æ ‡è®°æ¨æ–‡ä¸ºå·²å¤„ç†"""
    if "processed_tweets" not in state:
        state["processed_tweets"] = []

    state["processed_tweets"].append(tweet_id)

    # åªä¿ç•™æœ€è¿‘ 10000 æ¡è®°å½•
    if len(state["processed_tweets"]) > 10000:
        state["processed_tweets"] = state["processed_tweets"][-5000:]

    save_state(state)


# ========== æç¤ºè¯ç‰¹å¾åŒ¹é… ==========

# Nano Banana ç›¸å…³å…³é”®è¯
PROMPT_KEYWORDS = [
    # äº§å“åç§°
    "nano banana", "nanobanana", "å°é¦™è•‰", "é¦™è•‰",
    "nano banana pro", "gemini", "gemini 2.5", "gemini 3",
    "gemini image", "gemini pro",

    # å…¶ä»– AI å›¾åƒå·¥å…·
    "midjourney", "mj", "stable diffusion", "sd", "dall-e", "dalle",
    "flux", "comfyui", "leonardo", "ideogram", "runway",
    "å¯çµ", "kling", "å³æ¢¦", "é€šä¹‰ä¸‡ç›¸", "æ–‡å¿ƒä¸€æ ¼",

    # æç¤ºè¯æ ‡è¯†
    "æç¤ºè¯", "å’’è¯­", "prompt", "prompts",

    # å¸¸è§åŠ¨ä½œå¼€å¤´ (è‡ªç„¶è¯­è¨€æè¿°é£æ ¼)
    "åˆ›å»ºä¸€ä¸ª", "ç”Ÿæˆä¸€ä¸ª", "è®¾è®¡ä¸€ä¸ª", "åˆ¶ä½œä¸€ä¸ª", "ç”»ä¸€ä¸ª",
    "create a", "generate a", "design a", "make a", "draw a",

    # Midjourney ç‰¹æœ‰å‚æ•°
    "--ar", "--v ", "--style", "--s ", "--c ", "--q ",

    # SD/ComfyUI ç‰¹æœ‰
    "(masterpiece", "best quality", "8k uhd", "highly detailed",
]

# éœ€è¦åŒ…å«å›¾ç‰‡çš„æ¨æ–‡æ‰è€ƒè™‘
MIN_IMAGES = 1

# æ–‡æœ¬æœ€å°é•¿åº¦ (è¿‡æ»¤å¤ªçŸ­çš„æ¨æ–‡)
MIN_TEXT_LENGTH = 30


def is_likely_prompt_tweet(tweet: Dict) -> tuple[bool, str]:
    """
    ç¬¬ä¸€é˜¶æ®µè¿‡æ»¤: åŸºäºå…³é”®è¯å’Œç‰¹å¾åˆ¤æ–­æ˜¯å¦å¯èƒ½åŒ…å«æç¤ºè¯

    Args:
        tweet: æ¨æ–‡æ•°æ®

    Returns:
        (is_likely, reason): æ˜¯å¦å¯èƒ½æ˜¯æç¤ºè¯æ¨æ–‡åŠåŸå› 
    """
    text = tweet.get("text", "").lower()
    images = tweet.get("images", [])

    # å¿…é¡»æœ‰å›¾ç‰‡
    if len(images) < MIN_IMAGES:
        return False, "no_images"

    # æ–‡æœ¬å¤ªçŸ­
    if len(text) < MIN_TEXT_LENGTH:
        return False, "text_too_short"

    # æ£€æŸ¥å…³é”®è¯
    matched_keywords = []
    for keyword in PROMPT_KEYWORDS:
        if keyword.lower() in text:
            matched_keywords.append(keyword)

    if matched_keywords:
        return True, f"keywords: {', '.join(matched_keywords[:3])}"

    # æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ®µè½æè¿° (Nano Banana é£æ ¼çš„è‡ªç„¶è¯­è¨€æç¤ºè¯)
    # é€šå¸¸æç¤ºè¯ä¼šæœ‰è¾ƒé•¿çš„è¿ç»­æè¿°
    if len(text) > 200:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æè¿°æ€§å†…å®¹
        chinese_descriptors = ["é£æ ¼", "åœºæ™¯", "èƒŒæ™¯", "äººç‰©", "é¢œè‰²", "å…‰çº¿", "æ°›å›´", "æ„å›¾"]
        for desc in chinese_descriptors:
            if desc in text:
                return True, f"descriptive: {desc}"

    return False, "no_match"


# ========== çˆ†æ¬¾å®šä¹‰ ==========

# çˆ†æ¬¾é˜ˆå€¼é…ç½®
VIRAL_THRESHOLDS = {
    "likes_min": 1000,        # ç‚¹èµ >= 1000
    "retweets_min": 500,      # è½¬å‘ >= 500
    "views_min": 100000,      # æµè§ˆé‡ >= 100k
    "likes_small_account": 500,  # å°è´¦å·ï¼ˆ<10kç²‰ï¼‰ç‚¹èµé˜ˆå€¼
    "engagement_rate_min": 0.01,  # äº’åŠ¨ç‡ >= 1%
}


def is_viral_tweet(tweet: Dict, follower_count: int = 0) -> tuple[bool, str]:
    """
    åˆ¤æ–­æ¨æ–‡æ˜¯å¦ä¸ºçˆ†æ¬¾

    Args:
        tweet: æ¨æ–‡æ•°æ®å­—å…¸
        follower_count: å‘å¸ƒè€…ç²‰ä¸æ•°ï¼ˆç”¨äºè®¡ç®—äº’åŠ¨ç‡ï¼‰

    Returns:
        (is_viral, reason): æ˜¯å¦çˆ†æ¬¾åŠåŸå› 
    """
    likes = tweet.get("likes", 0) or 0
    retweets = tweet.get("retweets", 0) or 0
    views = tweet.get("views", 0) or 0

    reasons = []

    # ç»å¯¹æ•°å€¼åˆ¤æ–­
    if likes >= VIRAL_THRESHOLDS["likes_min"]:
        reasons.append(f"likes={likes}")

    if retweets >= VIRAL_THRESHOLDS["retweets_min"]:
        reasons.append(f"retweets={retweets}")

    if views >= VIRAL_THRESHOLDS["views_min"]:
        reasons.append(f"views={views}")

    # å°è´¦å·åˆ¤æ–­ï¼ˆç²‰ä¸æ•° < 10kï¼‰
    if follower_count > 0 and follower_count < 10000:
        if likes >= VIRAL_THRESHOLDS["likes_small_account"]:
            reasons.append(f"small_account_viral(likes={likes})")

    # äº’åŠ¨ç‡åˆ¤æ–­ï¼ˆå¦‚æœæœ‰ç²‰ä¸æ•°ï¼‰
    if follower_count > 0:
        engagement = (likes + retweets) / follower_count
        if engagement >= VIRAL_THRESHOLDS["engagement_rate_min"]:
            reasons.append(f"engagement_rate={engagement:.1%}")

    is_viral = len(reasons) > 0
    reason = ", ".join(reasons) if reasons else "not_viral"

    return is_viral, reason


def get_viral_score(tweet: Dict) -> int:
    """
    è®¡ç®—æ¨æ–‡çš„çˆ†æ¬¾è¯„åˆ†ï¼ˆç”¨äºæ’åºï¼‰

    åŸºäº X ç®—æ³•æƒé‡ï¼š
    - ç‚¹èµ: +30 åˆ†/ä¸ª
    - è½¬å‘: +20 åˆ†/ä¸ª
    - æµè§ˆé‡: +0.001 åˆ†/ä¸ª

    Returns:
        çˆ†æ¬¾è¯„åˆ†
    """
    likes = tweet.get("likes", 0) or 0
    retweets = tweet.get("retweets", 0) or 0
    views = tweet.get("views", 0) or 0

    score = (likes * 30) + (retweets * 20) + (views * 0.001)
    return int(score)


# ========== åˆ†ç±»æ˜ å°„ ==========

# å¯¼å…¥ç»Ÿä¸€åˆ†ç±»æ˜ å°„ (å®šä¹‰åœ¨ prompt_utils.py)
from prompt_utils import CATEGORY_MAP, map_category


# ========== Nitter/RSS æ–¹å¼è·å–æ—¶é—´çº¿ ==========

# Nitter å®ä¾‹åˆ—è¡¨ (å…¬å¼€å¯ç”¨)
NITTER_INSTANCES = [
    "https://nitter.poast.org",
    "https://nitter.privacydev.net",
    "https://nitter.1d4.us",
    "https://nitter.net",
    "https://nitter.cz",
]

# RSSHub å®ä¾‹ (å¦ä¸€ç§è·å– Twitter Timeline çš„æ–¹å¼)
RSSHUB_INSTANCES = [
    "https://rsshub.app",
    "https://rsshub.rssforever.com",
]


def fetch_user_timeline_nitter(username: str, count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ Nitter RSS è·å–ç”¨æˆ·æ—¶é—´çº¿

    Args:
        username: Twitter ç”¨æˆ·å (ä¸å« @)
        count: è·å–æ•°é‡

    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    tweets = []

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml',
    }

    for instance in NITTER_INSTANCES:
        try:
            rss_url = f"{instance}/{username}/rss"
            response = requests.get(rss_url, headers=headers, timeout=15)

            if response.status_code == 200:
                # è§£æ RSS
                tweets = parse_nitter_rss(response.text, username, count)
                if tweets:
                    print(f"   [Nitter] Got {len(tweets)} tweets from {instance}")
                    return tweets
        except Exception as e:
            print(f"   [Nitter] {instance} failed: {e}")
            continue

    return tweets


def parse_nitter_rss(xml_content: str, username: str, count: int = 20) -> List[Dict]:
    """è§£æ Nitter RSS å†…å®¹"""
    tweets = []

    if not HAS_BS4:
        print("   [Nitter] BeautifulSoup not available")
        return tweets

    soup = BeautifulSoup(xml_content, 'xml')
    items = soup.find_all('item')[:count]

    for item in items:
        try:
            # æå–é“¾æ¥å’Œæ¨æ–‡ ID
            link = item.find('link')
            if not link:
                continue
            link_text = link.get_text()

            # ä»é“¾æ¥æå– tweet_id
            # æ ¼å¼: https://nitter.xxx/username/status/123456789#m
            match = re.search(r'/status/(\d+)', link_text)
            if not match:
                continue
            tweet_id = match.group(1)

            # æå–æ­£æ–‡
            description = item.find('description')
            text = ""
            images = []

            if description:
                desc_html = description.get_text()
                desc_soup = BeautifulSoup(desc_html, 'html.parser')

                # æå–æ–‡æœ¬ (å»é™¤å›¾ç‰‡æè¿°)
                for img in desc_soup.find_all('img'):
                    img_src = img.get('src', '')
                    if 'pbs.twimg.com' in img_src or 'twimg.com' in img_src:
                        images.append(img_src)
                    img.decompose()

                text = desc_soup.get_text(separator=' ').strip()

            # æå–å‘å¸ƒæ—¶é—´
            pub_date = item.find('pubDate')
            created_at = pub_date.get_text() if pub_date else None

            tweets.append({
                "id": tweet_id,
                "text": text,
                "created_at": created_at,
                "username": username,
                "url": f"https://x.com/{username}/status/{tweet_id}",
                "images": images,
                "likes": 0,  # RSS ä¸æä¾›è¿™äº›æ•°æ®
                "retweets": 0,
                "views": 0,
            })
        except Exception as e:
            continue

    return tweets


def fetch_user_timeline_syndication(username: str, count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ Twitter Syndication API è·å–ç”¨æˆ·æ—¶é—´çº¿
    è¿™æ˜¯ Twitter å®˜æ–¹çš„åµŒå…¥ APIï¼Œä¸éœ€è¦è®¤è¯
    """
    tweets = []

    # Twitter Syndication Timeline API
    url = f"https://syndication.twitter.com/srv/timeline-profile/screen-name/{username}"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml',
    }

    try:
        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code == 200:
            # ä» JSON æ•°æ®ä¸­æå– tweet IDs
            tweet_ids = re.findall(r'"id_str":"(\d+)"', response.text)

            seen_ids = set()
            for tweet_id in tweet_ids:
                if tweet_id not in seen_ids:
                    seen_ids.add(tweet_id)
                    tweets.append({
                        "id": tweet_id,
                        "username": username,
                        "url": f"https://x.com/{username}/status/{tweet_id}",
                    })
                    if len(tweets) >= count:
                        break

            if tweets:
                print(f"   [Syndication] Got {len(tweets)} tweet IDs")
            else:
                # æ£€æŸ¥æ˜¯å¦è´¦å·ä¸å­˜åœ¨æˆ–è¢«é™åˆ¶
                if "UserUnavailable" in response.text or "This account doesn" in response.text:
                    print(f"   [Syndication] Account unavailable or suspended")
                elif len(response.text) < 1000:
                    print(f"   [Syndication] Empty or minimal response")
        else:
            print(f"   [Syndication] HTTP {response.status_code}")

    except Exception as e:
        print(f"   [Syndication] Error: {e}")

    return tweets


def fetch_user_timeline_rsshub(username: str, count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ RSSHub è·å–ç”¨æˆ·æ—¶é—´çº¿

    Args:
        username: Twitter ç”¨æˆ·å (ä¸å« @)
        count: è·å–æ•°é‡

    Returns:
        æ¨æ–‡åˆ—è¡¨ (åªåŒ…å« tweet_idï¼Œéœ€è¦åç»­è·å–è¯¦æƒ…)
    """
    tweets = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml',
    }

    for instance in RSSHUB_INSTANCES:
        try:
            # RSSHub Twitter è·¯ç”±: /twitter/user/:id
            rss_url = f"{instance}/twitter/user/{username}"
            response = requests.get(rss_url, headers=headers, timeout=15)

            if response.status_code == 200:
                # è§£æ RSS æå– tweet IDs
                if HAS_BS4:
                    soup = BeautifulSoup(response.text, 'xml')
                    items = soup.find_all('item')[:count]

                    for item in items:
                        link = item.find('link')
                        if link:
                            link_text = link.get_text()
                            # æå– tweet_id
                            match = re.search(r'/status/(\d+)', link_text)
                            if match:
                                tweet_id = match.group(1)
                                tweets.append({
                                    "id": tweet_id,
                                    "username": username,
                                    "url": f"https://x.com/{username}/status/{tweet_id}",
                                })

                if tweets:
                    print(f"   [RSSHub] Got {len(tweets)} tweet IDs from {instance}")
                    return tweets
        except Exception as e:
            print(f"   [RSSHub] {instance} failed: {e}")
            continue

    return tweets


def fetch_tweet_details(tweet_id: str, username: str) -> Optional[Dict]:
    """
    è·å–å•æ¡æ¨æ–‡çš„è¯¦ç»†ä¿¡æ¯ (åŒ…æ‹¬å›¾ç‰‡ã€äº’åŠ¨æ•°æ®)
    ä½¿ç”¨ FxTwitter/VxTwitter API
    """
    try:
        data = fetch_with_fxtwitter(tweet_id, username)
        result = parse_fxtwitter_result(data)
        if result and result.get("text"):
            stats = result.get("stats", {})
            return {
                "id": tweet_id,
                "text": result.get("text", ""),
                "images": result.get("images", []),
                "username": username,
                "url": f"https://x.com/{username}/status/{tweet_id}",
                "likes": stats.get("likes", 0),
                "retweets": stats.get("retweets", 0),
                "views": stats.get("views", 0),
                "created_at": result.get("created_at"),
            }
    except Exception:
        pass

    try:
        data = fetch_with_vxtwitter(tweet_id, username)
        result = parse_vxtwitter_result(data)
        if result and result.get("text"):
            stats = result.get("stats", {})
            return {
                "id": tweet_id,
                "text": result.get("text", ""),
                "images": result.get("images", []),
                "username": username,
                "url": f"https://x.com/{username}/status/{tweet_id}",
                "likes": stats.get("likes", 0),
                "retweets": stats.get("retweets", 0),
                "views": stats.get("views", 0),
                "created_at": result.get("created_at"),
            }
    except Exception:
        pass

    return None


# ========== X/Twitter å®¢æˆ·ç«¯ ==========

def random_delay(delay_range: tuple, description: str = ""):
    """æ·»åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
    delay = random.uniform(delay_range[0], delay_range[1])
    if description:
        print(f"   [Delay] {description}: {delay:.1f}s")
    time.sleep(delay)


class XMonitor:
    """X/Twitter ç›‘æ§å™¨ - ä½¿ç”¨ twikit è·å–ç”¨æˆ·æ—¶é—´çº¿"""

    def __init__(self, use_guest: bool = False):
        # use_guest å‚æ•°ä¿ç•™ä»¥å…¼å®¹ CLIï¼Œä½†ä¸å†ä½¿ç”¨
        self.client = None
        self.logged_in = False
        self.request_count = 0
        self.last_request_time = 0

    def _clear_cf_cookies(self):
        """æ¸…ç† Cloudflare cookies é¿å…å†²çª"""
        if self.client and hasattr(self.client, '_session') and self.client._session:
            try:
                # è·å– session çš„ cookies jar
                session = self.client._session
                if hasattr(session, 'cookies') and hasattr(session.cookies, 'jar'):
                    # åˆ é™¤æ‰€æœ‰ __cf_bm cookies
                    cookies_to_remove = []
                    for cookie in session.cookies.jar:
                        if cookie.name == '__cf_bm':
                            cookies_to_remove.append((cookie.name, cookie.domain))
                    for name, domain in cookies_to_remove:
                        try:
                            session.cookies.delete(name, domain=domain)
                        except:
                            pass
                    if cookies_to_remove:
                        print(f"   [twikit] Cleared {len(cookies_to_remove)} __cf_bm cookies")
            except Exception as e:
                print(f"   [twikit] Failed to clear cookies: {e}")

    async def init_client(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯ - ä½¿ç”¨ twikit + cookies"""
        if not HAS_TWIKIT:
            raise RuntimeError("twikit not installed. Run: pip install twikit")

        # è¿è¡Œæ—¶è·å–ç¯å¢ƒå˜é‡ (ç¡®ä¿èƒ½è¯»åˆ° GitHub Actions è®¾ç½®çš„ secrets)
        x_cookie = os.environ.get("X_COOKIE", "")
        x_username = os.environ.get("X_USERNAME", "")
        x_email = os.environ.get("X_EMAIL", "")
        x_password = os.environ.get("X_PASSWORD", "")
        proxy_url = os.environ.get("X_PROXY", "")

        # è°ƒè¯•ä¿¡æ¯
        print(f"[twikit] X_COOKIE env: {'set (' + str(len(x_cookie)) + ' chars)' if x_cookie else 'not set'}")
        print(f"[twikit] X_USERNAME env: {'set' if x_username else 'not set'}")
        print(f"[twikit] COOKIES_FILE: {COOKIES_FILE} (exists: {COOKIES_FILE.exists()})")

        # åˆå§‹åŒ–å®¢æˆ·ç«¯ (æ”¯æŒä»£ç†)
        if proxy_url:
            print(f"[twikit] Using proxy: {proxy_url[:20]}...")
            self.client = Client('en-US', proxy=proxy_url)
        else:
            self.client = Client('en-US')

        # å°è¯•ä½¿ç”¨ cookies ç™»å½• (ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡)
        if x_cookie:
            try:
                # æ”¯æŒå¤šç§æ ¼å¼
                cookie_str = x_cookie.strip()
                # å¦‚æœæ˜¯å•å¼•å·åŒ…è£¹ï¼Œè½¬æ¢ä¸ºåŒå¼•å·
                if cookie_str.startswith("'") and cookie_str.endswith("'"):
                    cookie_str = cookie_str[1:-1]
                cookie_data = json.loads(cookie_str)
                print(f"[twikit] Parsed cookie keys: {list(cookie_data.keys())}")

                # å†™å…¥ä¸´æ—¶æ–‡ä»¶ä¾› twikit åŠ è½½
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                    json.dump(cookie_data, f)
                    temp_cookie_file = f.name
                self.client.load_cookies(temp_cookie_file)
                os.unlink(temp_cookie_file)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                print("[twikit] Loaded cookies from X_COOKIE env")
                self.logged_in = True
                return
            except json.JSONDecodeError as e:
                print(f"[twikit] Failed to parse X_COOKIE JSON: {e}")
                print(f"[twikit] X_COOKIE value (first 50 chars): {x_cookie[:50]}...")
            except Exception as e:
                print(f"[twikit] Failed to load cookies from env: {e}")

        if COOKIES_FILE.exists():
            try:
                self.client.load_cookies(str(COOKIES_FILE))
                print("[twikit] Loaded cookies from file")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Failed to load cookies from file: {e}")

        # ä½¿ç”¨è´¦å·å¯†ç ç™»å½•
        if x_username and x_password:
            try:
                print("[twikit] Logging in with credentials...")
                await self.client.login(
                    auth_info_1=x_username,
                    auth_info_2=x_email,
                    password=x_password
                )
                # ä¿å­˜ cookies
                self.client.save_cookies(str(COOKIES_FILE))
                print("[twikit] Login successful, cookies saved")
                self.logged_in = True
                return
            except Exception as e:
                print(f"[twikit] Login failed: {e}")
                raise

        # æç¤ºå¦‚ä½•è·å– cookies
        print("\n" + "=" * 60)
        print("éœ€è¦ X è´¦å· cookies æ‰èƒ½è·å–ç”¨æˆ·æ—¶é—´çº¿")
        print("=" * 60)
        print("\nè®¾ç½®æ–¹æ³•:")
        print("1. åœ¨ GitHub Secrets ä¸­æ·»åŠ  X_COOKIE:")
        print('   å€¼æ ¼å¼: {"auth_token": "xxx", "ct0": "xxx"}')
        print("\n2. æˆ–è€…ä»æµè§ˆå™¨å¯¼å‡º cookies:")
        print("   - åœ¨ Chrome ç™»å½• x.com")
        print("   - F12 > Application > Cookies > https://x.com")
        print("   - å¤åˆ¶ auth_token å’Œ ct0 çš„å€¼")
        print("\n" + "=" * 60)
        raise RuntimeError("No X_COOKIE env or cookies file found.")

    async def _handle_rate_limit(self, e: Exception, retry_count: int) -> bool:
        """å¤„ç†é™æµï¼Œè¿”å›æ˜¯å¦åº”è¯¥é‡è¯•"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ TooManyRequests å¼‚å¸¸
        if 'TooManyRequests' in str(type(e).__name__) or '429' in str(e):
            if retry_count < MAX_RETRIES_ON_RATE_LIMIT:
                # å°è¯•è·å–é‡ç½®æ—¶é—´
                wait_time = DELAY_ON_RATE_LIMIT * (retry_count + 1)  # æŒ‡æ•°é€€é¿
                if hasattr(e, 'rate_limit_reset') and e.rate_limit_reset:
                    wait_time = max(wait_time, e.rate_limit_reset - time.time())

                # æ·»åŠ éšæœºæŠ–åŠ¨ (jitter)
                jitter = random.uniform(0, 10)
                wait_time += jitter

                print(f"   [Rate Limit] Waiting {wait_time:.0f}s before retry ({retry_count + 1}/{MAX_RETRIES_ON_RATE_LIMIT})...")
                await asyncio.sleep(wait_time)
                return True
        return False

    async def get_user_tweets(self, username: str, count: int = 20) -> List[Dict]:
        """è·å–ç”¨æˆ·æœ€æ–°æ¨æ–‡ (å¸¦é‡è¯•å’Œé™æµå¤„ç†)"""
        if not self.logged_in:
            await self.init_client()

        tweets = []
        retry_count = 0

        while retry_count <= MAX_RETRIES_ON_RATE_LIMIT:
            try:
                # æ¸…ç†å¯èƒ½å†²çªçš„ Cloudflare cookies
                self._clear_cf_cookies()

                # API è°ƒç”¨é—´éš”
                time_since_last = time.time() - self.last_request_time
                if time_since_last < DELAY_BETWEEN_API_CALLS[0]:
                    await asyncio.sleep(DELAY_BETWEEN_API_CALLS[0] - time_since_last + random.uniform(0, 1))

                # å…ˆè·å–ç”¨æˆ·ä¿¡æ¯
                user = await self.client.get_user_by_screen_name(username)
                self.last_request_time = time.time()
                self.request_count += 1

                if not user:
                    print(f"   [twikit] User not found: @{username}")
                    return tweets

                # çŸ­æš‚å»¶è¿Ÿåè·å–æ¨æ–‡
                await asyncio.sleep(random.uniform(*DELAY_BETWEEN_API_CALLS))

                # è·å–ç”¨æˆ·æ¨æ–‡
                user_tweets = await self.client.get_user_tweets(user.id, 'Tweets', count=count)
                self.last_request_time = time.time()
                self.request_count += 1

                for tweet in user_tweets:
                    try:
                        # ä¼˜å…ˆä½¿ç”¨ full_text è·å–é•¿æ¨æ–‡ (note_tweet) çš„å®Œæ•´å†…å®¹
                        tweet_text = ""
                        try:
                            tweet_text = tweet.full_text or tweet.text or ""
                        except Exception:
                            tweet_text = tweet.text or ""

                        tweet_data = {
                            "id": tweet.id,
                            "text": tweet_text,
                            "username": username,
                            "url": f"https://x.com/{username}/status/{tweet.id}",
                            "likes": tweet.favorite_count or 0,
                            "retweets": tweet.retweet_count or 0,
                            "views": tweet.view_count or 0,
                            "created_at": str(tweet.created_at) if tweet.created_at else None,
                            "images": [],
                        }

                        # æå–å›¾ç‰‡
                        if tweet.media:
                            for media in tweet.media:
                                if hasattr(media, 'media_url') and media.media_url:
                                    img_url = media.media_url
                                    if img_url.startswith('http://'):
                                        img_url = img_url.replace('http://', 'https://')
                                    tweet_data["images"].append(img_url)
                                elif hasattr(media, 'media_url_https') and media.media_url_https:
                                    tweet_data["images"].append(media.media_url_https)

                        tweets.append(tweet_data)

                    except Exception as e:
                        print(f"   [Warning] Failed to parse tweet: {e}")
                        continue

                print(f"   [twikit] Got {len(tweets)} tweets (requests: {self.request_count})")
                return tweets

            except Exception as e:
                error_str = str(e)

                # å¤„ç† cookie å†²çª
                if 'Multiple cookies exist' in error_str or '__cf_bm' in error_str:
                    print(f"   [twikit] Cookie conflict detected, clearing and retrying...")
                    self._clear_cf_cookies()
                    retry_count += 1
                    await asyncio.sleep(random.uniform(2, 5))
                    continue

                # å¤„ç†é™æµ
                if await self._handle_rate_limit(e, retry_count):
                    retry_count += 1
                    continue

                print(f"   [twikit] Error getting tweets: {e}")
                break

        # å¦‚æœ twikit å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ FxTwitter å¤‡ç”¨æ–¹æ¡ˆ
        print(f"   [Fallback] Trying Syndication API...")
        syn_tweets = fetch_user_timeline_syndication(username, count)
        if syn_tweets:
            for st in syn_tweets:
                # æ·»åŠ å»¶è¿Ÿé¿å… Syndication API ä¹Ÿè¢«é™æµ
                await asyncio.sleep(random.uniform(0.5, 1.5))
                details = fetch_tweet_details(st["id"], username)
                if details:
                    tweets.append(details)
                if len(tweets) >= count:
                    break

        return tweets


# ========== ä¸»å¤„ç†é€»è¾‘ ==========

async def process_tweet(db: Database, tweet: Dict, state: Dict,
                        viral_only: bool = False, dry_run: bool = False) -> bool:
    """å¤„ç†å•æ¡æ¨æ–‡ - ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°

    Args:
        db: æ•°æ®åº“è¿æ¥
        tweet: æ¨æ–‡æ•°æ®
        state: å¤„ç†çŠ¶æ€
        viral_only: æ˜¯å¦åªå¤„ç†çˆ†æ¬¾æ¨æ–‡ (ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸å†ä½¿ç”¨)
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“
    """
    tweet_id = tweet["id"]
    tweet_url = tweet["url"]
    text = tweet["text"]
    images = tweet["images"]
    username = tweet["username"]
    likes = tweet.get("likes", 0)
    retweets = tweet.get("retweets", 0)
    views = tweet.get("views", 0)

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if is_tweet_processed(state, tweet_id):
        return False

    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡
    if not images:
        mark_tweet_processed(state, tweet_id)
        return False

    # å°è¯•ç”¨ FxTwitter è·å–æ›´å®Œæ•´çš„æ–‡æœ¬ï¼ˆå±•å¼€çŸ­é“¾æ¥ã€è·å–é•¿æ¨æ–‡ï¼‰
    try:
        fx_data = fetch_with_fxtwitter(tweet_id, username)
        fx_result = parse_fxtwitter_result(fx_data)
        if fx_result:
            fx_text = fx_result.get("text", "")
            if fx_text and len(fx_text) > len(text):
                print(f"   [FxTwitter] Got longer text: {len(text)} -> {len(fx_text)} chars")
                text = fx_text
            # å¦‚æœ FxTwitter æœ‰æ›´å¤šå›¾ç‰‡ï¼Œè¡¥å……
            if fx_result.get("images") and len(fx_result["images"]) > len(images):
                images = fx_result["images"]
    except Exception as e:
        print(f"   [FxTwitter] Failed to get full text: {e}")

    # æ˜¾ç¤ºæ¨æ–‡ä¿¡æ¯
    print(f"\n   [Tweet] @{username} - {tweet_id}")
    print(f"   Text: {text[:100]}...")
    print(f"   Stats: â¤ï¸ {likes:,} | ğŸ” {retweets:,} | ğŸ‘ï¸ {views:,}")
    print(f"   Images: {len(images)}")

    # ä½¿ç”¨ç»Ÿä¸€å¤„ç†å‡½æ•°
    result = process_tweet_for_import(
        db=db,
        tweet_url=tweet_url,
        raw_text=text,
        raw_images=images,
        author=username,
        import_source="x-monitor",
        ai_model=AI_MODEL,
        dry_run=dry_run,
        skip_twitter_fetch=True  # å·²æœ‰ Twitter å›¾ç‰‡
    )

    mark_tweet_processed(state, tweet_id)

    if result["success"]:
        return True
    else:
        error = result.get("error", "")
        if error and error != "Already exists":
            print(f"   [Skip] {error}")
        return False


async def monitor_accounts(
    accounts: List[str],
    tweets_per_account: int = 10,
    viral_only: bool = False,
    dry_run: bool = False
) -> Dict:
    """ç›‘å¬è´¦å·åˆ—è¡¨

    Args:
        accounts: è¦ç›‘å¬çš„è´¦å·åˆ—è¡¨
        tweets_per_account: æ¯ä¸ªè´¦å·è·å–çš„æ¨æ–‡æ•°é‡
        viral_only: æ˜¯å¦åªå¤„ç†çˆ†æ¬¾æ¨æ–‡
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“
    """

    print("=" * 60)
    print("X/Twitter AI Art Monitor")
    print("=" * 60)
    print(f"Accounts: {len(accounts)}")
    print(f"Viral Only: {viral_only}")
    print(f"Dry Run: {dry_run}")
    print(f"AI Model: {AI_MODEL}")
    print(f"Rate Limit: {DELAY_BETWEEN_ACCOUNTS[0]}-{DELAY_BETWEEN_ACCOUNTS[1]}s between accounts")
    print("=" * 60)

    # æ£€æŸ¥æ•°æ®åº“
    if not DATABASE_URL:
        print("[Error] DATABASE_URL not set")
        return {"error": "DATABASE_URL not set"}

    # åˆå§‹åŒ–
    db = Database(DATABASE_URL)
    monitor = XMonitor()
    state = load_state()

    stats = {
        "accounts_checked": 0,
        "tweets_found": 0,
        "filtered_stage1": 0,  # ç¬¬ä¸€é˜¶æ®µè¿‡æ»¤ (ç‰¹å¾åŒ¹é…)
        "filtered_stage2": 0,  # ç¬¬äºŒé˜¶æ®µè¿‡æ»¤ (AI æå–å¤±è´¥)
        "advertisement": 0,    # å¹¿å‘Šå†…å®¹
        "prompt_in_reply": 0,  # Prompt åœ¨è¯„è®ºä¸­
        "prompts_saved": 0,
        "errors": 0,
    }

    try:
        db.connect()
        print("[DB] Connected")

        await monitor.init_client()
        print("[X] Client ready")

        # éå†è´¦å·
        for i, username in enumerate(accounts, 1):
            print(f"\n[{i}/{len(accounts)}] Checking @{username}...")

            try:
                tweets = await monitor.get_user_tweets(username, tweets_per_account)
                stats["accounts_checked"] += 1
                stats["tweets_found"] += len(tweets)

                print(f"   Found {len(tweets)} tweets")

                for tweet in tweets:
                    result = await process_tweet(db, tweet, state, viral_only=viral_only, dry_run=dry_run)
                    if result == "filtered_stage1":
                        stats["filtered_stage1"] += 1
                    elif result == "filtered_stage2":
                        stats["filtered_stage2"] += 1
                    elif result == "advertisement":
                        stats["advertisement"] += 1
                    elif result == "prompt_in_reply":
                        stats["prompt_in_reply"] += 1
                    elif result is True:
                        stats["prompts_saved"] += 1

                    # å¤„ç†æ¨æ–‡é—´å»¶è¿Ÿ (é¿å… AI API é™æµ)
                    await asyncio.sleep(random.uniform(*DELAY_BETWEEN_TWEETS))

                # è´¦å·é—´å»¶è¿Ÿ (é¿å… Twitter é™æµ)
                delay = random.uniform(*DELAY_BETWEEN_ACCOUNTS)
                print(f"   [Delay] Next account in {delay:.1f}s...")
                await asyncio.sleep(delay)

            except Exception as e:
                error_str = str(e)
                print(f"   [Error] {e}")
                stats["errors"] += 1

                # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                if 'TooManyRequests' in str(type(e).__name__) or '429' in error_str:
                    wait_time = DELAY_ON_RATE_LIMIT + random.uniform(0, 30)
                    print(f"   [Rate Limit] Waiting {wait_time:.0f}s before next account...")
                    await asyncio.sleep(wait_time)

        # æ›´æ–°çŠ¶æ€
        state["last_check"] = datetime.now(timezone.utc).isoformat()
        save_state(state)

    finally:
        db.close()

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    print(f"Accounts checked: {stats['accounts_checked']}")
    print(f"Tweets found: {stats['tweets_found']}")
    print(f"  â”œâ”€ Stage 1 filtered (no keywords): {stats['filtered_stage1']}")
    print(f"  â”œâ”€ Stage 2 filtered (AI no prompt): {stats['filtered_stage2']}")
    print(f"  â”œâ”€ Advertisement (skipped): {stats['advertisement']}")
    print(f"  â”œâ”€ Prompt in reply (skipped): {stats['prompt_in_reply']}")
    print(f"  â””â”€ Prompts saved: {stats['prompts_saved']}")
    print(f"Errors: {stats['errors']}")
    print("=" * 60)

    return stats


async def run_continuous(
    accounts: List[str],
    interval_minutes: int = 30,
    viral_only: bool = False,
    dry_run: bool = False
):
    """æŒç»­ç›‘å¬æ¨¡å¼"""
    print(f"Starting continuous monitor (interval: {interval_minutes} min)")
    print(f"Viral Only: {viral_only}")
    print(f"Dry Run: {dry_run}")
    print("Press Ctrl+C to stop\n")

    while True:
        try:
            await monitor_accounts(accounts, viral_only=viral_only, dry_run=dry_run)

            print(f"\nNext check in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)

        except KeyboardInterrupt:
            print("\nStopped by user")
            break
        except Exception as e:
            print(f"\n[Error] {e}")
            print(f"Retrying in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)


# ========== CLI ==========

def main():
    parser = argparse.ArgumentParser(
        description="X/Twitter AI Art Account Monitor (ä½¿ç”¨ twikit + cookies è®¤è¯)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # List top authors from database
  python fetch_x_accounts.py --list-authors

  # Run once with default accounts
  python fetch_x_accounts.py

  # Monitor top 20 authors from database
  python fetch_x_accounts.py --top 20

  # Dry run (don't save to database)
  python fetch_x_accounts.py --top 10 --dry-run

  # Continuous monitoring top 30 authors (every 30 min)
  python fetch_x_accounts.py --top 30 --interval 30

  # Only process viral tweets (high engagement)
  python fetch_x_accounts.py --top 30 --viral-only

  # Continuous viral-only monitoring
  python fetch_x_accounts.py --top 30 --interval 30 --viral-only

  # Monitor specific accounts
  python fetch_x_accounts.py --accounts midjourney,openai
        """
    )

    parser.add_argument("--accounts", "-a", type=str,
                        help="Comma-separated account list")
    parser.add_argument("--top", "-t", type=int, default=0,
                        help="Use top N authors from database (e.g., --top 20)")
    parser.add_argument("--list-authors", action="store_true",
                        help="List top authors from database and exit")
    parser.add_argument("--interval", "-i", type=int, default=0,
                        help="Continuous mode interval in minutes (0=run once)")
    parser.add_argument("--count", "-c", type=int, default=10,
                        help="Tweets per account (default: 10)")
    parser.add_argument("--viral-only", "-v", action="store_true",
                        help="Only process viral tweets (likes>=1000, retweets>=500, views>=100k)")
    parser.add_argument("--dry-run", "-d", action="store_true",
                        help="Dry run mode - fetch and process but don't save to database")

    args = parser.parse_args()

    # åˆ—å‡ºé«˜é¢‘ä½œè€…
    if args.list_authors:
        if not DATABASE_URL:
            print("Error: DATABASE_URL not set")
            return
        db = Database(DATABASE_URL)
        db.connect()
        authors = db.get_top_authors(50)
        db.close()
        print("Top 50 Authors (from database):")
        print("=" * 50)
        for i, row in enumerate(authors, 1):
            print(f"{i:3}. @{row['author']:<25} {row['count']:>5} prompts")
        return

    # è§£æè´¦å·åˆ—è¡¨
    if args.accounts:
        # æŒ‡å®šè´¦å·æ¨¡å¼ï¼šåªä½¿ç”¨æŒ‡å®šçš„è´¦å·
        accounts = [a.strip() for a in args.accounts.split(",") if a.strip()]
    else:
        # åˆå¹¶æ¨¡å¼ï¼šæ•°æ®åº“é«˜é¢‘ä½œè€… + é»˜è®¤è´¦å·åˆ—è¡¨
        db_authors = []
        if DATABASE_URL:
            try:
                db = Database(DATABASE_URL)
                db.connect()
                top_count = args.top if args.top > 0 else 50  # é»˜è®¤å– top 50
                authors = db.get_top_authors(top_count)
                db.close()
                db_authors = [row['author'] for row in authors]
                print(f"[DB] Got {len(db_authors)} authors from database")
            except Exception as e:
                print(f"[DB] Failed to get authors: {e}")

        # åˆå¹¶å¹¶å»é‡ï¼ˆä¿æŒé¡ºåºï¼šæ•°æ®åº“ä¼˜å…ˆï¼Œç„¶åæ˜¯é»˜è®¤åˆ—è¡¨ä¸­çš„æ–°è´¦å·ï¼‰
        seen = set()
        accounts = []
        for author in db_authors + DEFAULT_ACCOUNTS:
            author_lower = author.lower()  # å¿½ç•¥å¤§å°å†™å»é‡
            if author_lower not in seen:
                seen.add(author_lower)
                accounts.append(author)

        print(f"[Accounts] Total {len(accounts)} unique accounts (DB: {len(db_authors)}, Default: {len(DEFAULT_ACCOUNTS)})")

    # è¿è¡Œ
    if args.interval > 0:
        asyncio.run(run_continuous(
            accounts,
            interval_minutes=args.interval,
            viral_only=args.viral_only,
            dry_run=args.dry_run
        ))
    else:
        asyncio.run(monitor_accounts(
            accounts,
            tweets_per_account=args.count,
            viral_only=args.viral_only,
            dry_run=args.dry_run
        ))


if __name__ == "__main__":
    main()
