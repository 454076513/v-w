#!/usr/bin/env python3
"""
X/Twitter AI Art Account Monitor
ç›‘å¬ AI è‰ºæœ¯è´¦å·ï¼Œè‡ªåŠ¨æå–æç¤ºè¯å¹¶å…¥åº“

æŠ€æœ¯æ–¹æ¡ˆ:
- Twitter Syndication API: è·å–ç”¨æˆ·æ—¶é—´çº¿ (æ— éœ€è®¤è¯)
- FxTwitter/VxTwitter API: è·å–æ¨æ–‡è¯¦æƒ…å’Œäº’åŠ¨æ•°æ®

ä½¿ç”¨æ–¹æ³•:
    # è¿è¡Œç›‘å¬ (å•æ¬¡)
    python fetch_x_accounts.py

    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„é«˜é¢‘ä½œè€…
    python fetch_x_accounts.py --top 20

    # æŒç»­ç›‘å¬ (æ¯ N åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡)
    python fetch_x_accounts.py --interval 30

    # åªå¤„ç†çˆ†æ¬¾æ¨æ–‡
    python fetch_x_accounts.py --viral-only

    # é¢„è§ˆæ¨¡å¼ (ä¸å†™å…¥æ•°æ®åº“)
    python fetch_x_accounts.py --dry-run

    # ç›‘å¬ç‰¹å®šè´¦å·
    python fetch_x_accounts.py --accounts midjourney,openai

ç¯å¢ƒå˜é‡:
    DATABASE_URL        - PostgreSQL è¿æ¥å­—ç¬¦ä¸² (å¿…éœ€)
    AI_MODEL            - AI æ¨¡å‹ (é»˜è®¤: openai)
"""

import os
import sys
import asyncio
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    root_dir = Path(__file__).parent.parent
    env_local = root_dir / ".env.local"
    env_file = root_dir / ".env"

    if env_local.exists():
        load_dotenv(env_local)
        print(f"[env] Loaded: {env_local}")
    elif env_file.exists():
        load_dotenv(env_file)
        print(f"[env] Loaded: {env_file}")
except ImportError:
    pass

# twikit å·²å¼ƒç”¨ï¼Œä½¿ç”¨ Syndication API + FxTwitter æ›¿ä»£
HAS_TWIKIT = False

# HTTP è¯·æ±‚
import requests
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

# å¯¼å…¥ç°æœ‰çš„ AI å¤„ç†å‡½æ•°
from fetch_twitter_content import (
    extract_prompt_with_ai,
    classify_prompt_with_ai,
    DEFAULT_MODEL,
    fetch_with_fxtwitter,
    fetch_with_vxtwitter,
    parse_fxtwitter_result,
    parse_vxtwitter_result,
)

# æ•°æ®åº“
try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("Please install psycopg2: pip install psycopg2-binary")
    sys.exit(1)

# ========== é…ç½® ==========

DATABASE_URL = os.environ.get("DATABASE_URL", "")
AI_MODEL = os.environ.get("AI_MODEL", DEFAULT_MODEL)

# çŠ¶æ€æ–‡ä»¶ (è®°å½•å·²å¤„ç†çš„æ¨æ–‡ ID)
STATE_FILE = Path(__file__).parent / "x_monitor_state.json"

# é»˜è®¤ç›‘å¬çš„ AI è‰ºæœ¯è´¦å· (åŸºäºæ•°æ®åº“é«˜é¢‘ç»Ÿè®¡)
DEFAULT_ACCOUNTS = [
    # Top 20 é«˜é¢‘è´¦å·
    "songguoxiansen",    # #1 - 144 prompts
    "Gdgtify",           # #2 - 123 prompts
    "Ankit_patel211",    # #3 - 99 prompts
    "dotey",             # #4 - 96 prompts
    "azed_ai",           # #5 - 85 prompts
    "lexx_aura",         # #6 - 79 prompts
    "YaseenK7212",       # #7 - 79 prompts
    "saniaspeaks_",      # #8 - 78 prompts
    "ZaraIrahh",         # #9 - 75 prompts
    "Just_sharon7",      # #10 - 74 prompts
    "xmliisu",           # #11 - 71 prompts
    "Vivekhy",           # #12 - 64 prompts
    "astronomerozge1",   # #13 - 64 prompts
    "siennalovesai",     # #14 - 61 prompts
    "Strength04_X",      # #15 - 60 prompts
    "aleenaamiir",       # #16 - 57 prompts
    "SimplyAnnisa",      # #17 - 54 prompts
    "umesh_ai",          # #18 - 53 prompts
    "oggii_0",           # #19 - 44 prompts
    "xmiiru_",           # #20 - 43 prompts
]


# ========== æ•°æ®åº“æ“ä½œ ==========

class Database:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.conn = None

    def connect(self):
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(self.connection_string)
        return self.conn

    def close(self):
        if self.conn and not self.conn.closed:
            self.conn.close()

    def execute_write(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            conn.commit()
            if cur.description:
                result = cur.fetchone()
                return dict(result) if result else None
            return None

    def execute_one(self, query: str, params: tuple = None) -> Optional[Dict]:
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query, params)
            result = cur.fetchone()
            return dict(result) if result else None

    def prompt_exists(self, source_link: str) -> bool:
        result = self.execute_one(
            "SELECT id FROM prompts WHERE source_link = %s",
            (source_link,)
        )
        return result is not None

    def save_prompt(self, title: str, prompt: str, category: str,
                    tags: List[str], images: List[str], source_link: str,
                    author: str = None, import_source: str = None) -> Optional[Dict]:
        return self.execute_write(
            """
            INSERT INTO prompts (title, prompt, category, tags, images, source_link, author, import_source)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING *
            """,
            (title, prompt, category, tags or [], images or [], source_link, author, import_source)
        )

    def get_top_authors(self, limit: int = 30) -> List[Dict]:
        """è·å–é«˜é¢‘ä½œè€…åˆ—è¡¨"""
        conn = self.connect()
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute("""
                SELECT author, COUNT(*) as count
                FROM prompts
                WHERE author IS NOT NULL
                  AND author != ''
                  AND deleted_at IS NULL
                GROUP BY author
                ORDER BY count DESC
                LIMIT %s
            """, (limit,))
            return [dict(row) for row in cur.fetchall()]


# ========== çŠ¶æ€ç®¡ç† ==========

def load_state() -> Dict:
    """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, 'r') as f:
                return json.load(f)
        except:
            pass
    return {"processed_tweets": [], "last_check": None}


def save_state(state: Dict):
    """ä¿å­˜çŠ¶æ€æ–‡ä»¶"""
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)


def is_tweet_processed(state: Dict, tweet_id: str) -> bool:
    """æ£€æŸ¥æ¨æ–‡æ˜¯å¦å·²å¤„ç†"""
    return tweet_id in state.get("processed_tweets", [])


def mark_tweet_processed(state: Dict, tweet_id: str):
    """æ ‡è®°æ¨æ–‡ä¸ºå·²å¤„ç†"""
    if "processed_tweets" not in state:
        state["processed_tweets"] = []

    state["processed_tweets"].append(tweet_id)

    # åªä¿ç•™æœ€è¿‘ 10000 æ¡è®°å½•
    if len(state["processed_tweets"]) > 10000:
        state["processed_tweets"] = state["processed_tweets"][-5000:]

    save_state(state)


# ========== æç¤ºè¯ç‰¹å¾åŒ¹é… ==========

# Nano Banana ç›¸å…³å…³é”®è¯
PROMPT_KEYWORDS = [
    # äº§å“åç§°
    "nano banana", "nanobanana", "å°é¦™è•‰", "é¦™è•‰",
    "nano banana pro", "gemini", "gemini 2.5", "gemini 3",
    "gemini image", "gemini pro",

    # å…¶ä»– AI å›¾åƒå·¥å…·
    "midjourney", "mj", "stable diffusion", "sd", "dall-e", "dalle",
    "flux", "comfyui", "leonardo", "ideogram", "runway",
    "å¯çµ", "kling", "å³æ¢¦", "é€šä¹‰ä¸‡ç›¸", "æ–‡å¿ƒä¸€æ ¼",

    # æç¤ºè¯æ ‡è¯†
    "æç¤ºè¯", "å’’è¯­", "prompt", "prompts",

    # å¸¸è§åŠ¨ä½œå¼€å¤´ (è‡ªç„¶è¯­è¨€æè¿°é£æ ¼)
    "åˆ›å»ºä¸€ä¸ª", "ç”Ÿæˆä¸€ä¸ª", "è®¾è®¡ä¸€ä¸ª", "åˆ¶ä½œä¸€ä¸ª", "ç”»ä¸€ä¸ª",
    "create a", "generate a", "design a", "make a", "draw a",

    # Midjourney ç‰¹æœ‰å‚æ•°
    "--ar", "--v ", "--style", "--s ", "--c ", "--q ",

    # SD/ComfyUI ç‰¹æœ‰
    "(masterpiece", "best quality", "8k uhd", "highly detailed",
]

# éœ€è¦åŒ…å«å›¾ç‰‡çš„æ¨æ–‡æ‰è€ƒè™‘
MIN_IMAGES = 1

# æ–‡æœ¬æœ€å°é•¿åº¦ (è¿‡æ»¤å¤ªçŸ­çš„æ¨æ–‡)
MIN_TEXT_LENGTH = 30


def is_likely_prompt_tweet(tweet: Dict) -> tuple[bool, str]:
    """
    ç¬¬ä¸€é˜¶æ®µè¿‡æ»¤: åŸºäºå…³é”®è¯å’Œç‰¹å¾åˆ¤æ–­æ˜¯å¦å¯èƒ½åŒ…å«æç¤ºè¯

    Args:
        tweet: æ¨æ–‡æ•°æ®

    Returns:
        (is_likely, reason): æ˜¯å¦å¯èƒ½æ˜¯æç¤ºè¯æ¨æ–‡åŠåŸå› 
    """
    text = tweet.get("text", "").lower()
    images = tweet.get("images", [])

    # å¿…é¡»æœ‰å›¾ç‰‡
    if len(images) < MIN_IMAGES:
        return False, "no_images"

    # æ–‡æœ¬å¤ªçŸ­
    if len(text) < MIN_TEXT_LENGTH:
        return False, "text_too_short"

    # æ£€æŸ¥å…³é”®è¯
    matched_keywords = []
    for keyword in PROMPT_KEYWORDS:
        if keyword.lower() in text:
            matched_keywords.append(keyword)

    if matched_keywords:
        return True, f"keywords: {', '.join(matched_keywords[:3])}"

    # æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ®µè½æè¿° (Nano Banana é£æ ¼çš„è‡ªç„¶è¯­è¨€æç¤ºè¯)
    # é€šå¸¸æç¤ºè¯ä¼šæœ‰è¾ƒé•¿çš„è¿ç»­æè¿°
    if len(text) > 200:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æè¿°æ€§å†…å®¹
        chinese_descriptors = ["é£æ ¼", "åœºæ™¯", "èƒŒæ™¯", "äººç‰©", "é¢œè‰²", "å…‰çº¿", "æ°›å›´", "æ„å›¾"]
        for desc in chinese_descriptors:
            if desc in text:
                return True, f"descriptive: {desc}"

    return False, "no_match"


# ========== çˆ†æ¬¾å®šä¹‰ ==========

# çˆ†æ¬¾é˜ˆå€¼é…ç½®
VIRAL_THRESHOLDS = {
    "likes_min": 1000,        # ç‚¹èµ >= 1000
    "retweets_min": 500,      # è½¬å‘ >= 500
    "views_min": 100000,      # æµè§ˆé‡ >= 100k
    "likes_small_account": 500,  # å°è´¦å·ï¼ˆ<10kç²‰ï¼‰ç‚¹èµé˜ˆå€¼
    "engagement_rate_min": 0.01,  # äº’åŠ¨ç‡ >= 1%
}


def is_viral_tweet(tweet: Dict, follower_count: int = 0) -> tuple[bool, str]:
    """
    åˆ¤æ–­æ¨æ–‡æ˜¯å¦ä¸ºçˆ†æ¬¾

    Args:
        tweet: æ¨æ–‡æ•°æ®å­—å…¸
        follower_count: å‘å¸ƒè€…ç²‰ä¸æ•°ï¼ˆç”¨äºè®¡ç®—äº’åŠ¨ç‡ï¼‰

    Returns:
        (is_viral, reason): æ˜¯å¦çˆ†æ¬¾åŠåŸå› 
    """
    likes = tweet.get("likes", 0) or 0
    retweets = tweet.get("retweets", 0) or 0
    views = tweet.get("views", 0) or 0

    reasons = []

    # ç»å¯¹æ•°å€¼åˆ¤æ–­
    if likes >= VIRAL_THRESHOLDS["likes_min"]:
        reasons.append(f"likes={likes}")

    if retweets >= VIRAL_THRESHOLDS["retweets_min"]:
        reasons.append(f"retweets={retweets}")

    if views >= VIRAL_THRESHOLDS["views_min"]:
        reasons.append(f"views={views}")

    # å°è´¦å·åˆ¤æ–­ï¼ˆç²‰ä¸æ•° < 10kï¼‰
    if follower_count > 0 and follower_count < 10000:
        if likes >= VIRAL_THRESHOLDS["likes_small_account"]:
            reasons.append(f"small_account_viral(likes={likes})")

    # äº’åŠ¨ç‡åˆ¤æ–­ï¼ˆå¦‚æœæœ‰ç²‰ä¸æ•°ï¼‰
    if follower_count > 0:
        engagement = (likes + retweets) / follower_count
        if engagement >= VIRAL_THRESHOLDS["engagement_rate_min"]:
            reasons.append(f"engagement_rate={engagement:.1%}")

    is_viral = len(reasons) > 0
    reason = ", ".join(reasons) if reasons else "not_viral"

    return is_viral, reason


def get_viral_score(tweet: Dict) -> int:
    """
    è®¡ç®—æ¨æ–‡çš„çˆ†æ¬¾è¯„åˆ†ï¼ˆç”¨äºæ’åºï¼‰

    åŸºäº X ç®—æ³•æƒé‡ï¼š
    - ç‚¹èµ: +30 åˆ†/ä¸ª
    - è½¬å‘: +20 åˆ†/ä¸ª
    - æµè§ˆé‡: +0.001 åˆ†/ä¸ª

    Returns:
        çˆ†æ¬¾è¯„åˆ†
    """
    likes = tweet.get("likes", 0) or 0
    retweets = tweet.get("retweets", 0) or 0
    views = tweet.get("views", 0) or 0

    score = (likes * 30) + (retweets * 20) + (views * 0.001)
    return int(score)


# ========== åˆ†ç±»æ˜ å°„ ==========

CATEGORY_MAP = {
    "Portrait": "Portrait",
    "Landscape/Nature": "Landscape",
    "Landscape": "Landscape",
    "Nature": "Nature",
    "Animals": "Nature",
    "Architecture/Urban": "Architecture",
    "Architecture": "Architecture",
    "Abstract Art": "Abstract",
    "Abstract": "Abstract",
    "Sci-Fi/Futuristic": "Sci-Fi",
    "Sci-Fi": "Sci-Fi",
    "Fantasy/Magic": "Fantasy",
    "Fantasy": "Fantasy",
    "Anime/Cartoon": "Anime",
    "Anime": "Anime",
    "Realistic Photography": "Photography",
    "Photography": "Photography",
    "Illustration/Painting": "Illustration",
    "Illustration": "Illustration",
    "Fashion/Clothing": "Fashion",
    "Fashion": "Fashion",
    "Food": "Food",
    "Product/Commercial": "Product",
    "Product": "Product",
    "Cinematic": "Cinematic",
    "Horror/Dark": "Cinematic",
    "Cute/Kawaii": "Clay / Felt",
    "Vintage/Retro": "Retro / Vintage",
    "Minimalist": "Minimalist",
    "Surreal": "Abstract",
    "Other": "Other",
}


def map_category(classification: Dict) -> str:
    """å°† AI åˆ†ç±»æ˜ å°„åˆ°ç³»ç»Ÿåˆ†ç±»"""
    raw_category = classification.get("category", "Other")

    if raw_category in CATEGORY_MAP:
        return CATEGORY_MAP[raw_category]

    raw_lower = raw_category.lower()
    for key, value in CATEGORY_MAP.items():
        if key.lower() in raw_lower or raw_lower in key.lower():
            return value

    return "Photography"


# ========== Nitter/RSS æ–¹å¼è·å–æ—¶é—´çº¿ ==========

# Nitter å®ä¾‹åˆ—è¡¨ (å…¬å¼€å¯ç”¨)
NITTER_INSTANCES = [
    "https://nitter.poast.org",
    "https://nitter.privacydev.net",
    "https://nitter.1d4.us",
    "https://nitter.net",
    "https://nitter.cz",
]

# RSSHub å®ä¾‹ (å¦ä¸€ç§è·å– Twitter Timeline çš„æ–¹å¼)
RSSHUB_INSTANCES = [
    "https://rsshub.app",
    "https://rsshub.rssforever.com",
]


def fetch_user_timeline_nitter(username: str, count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ Nitter RSS è·å–ç”¨æˆ·æ—¶é—´çº¿

    Args:
        username: Twitter ç”¨æˆ·å (ä¸å« @)
        count: è·å–æ•°é‡

    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    tweets = []

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml',
    }

    for instance in NITTER_INSTANCES:
        try:
            rss_url = f"{instance}/{username}/rss"
            response = requests.get(rss_url, headers=headers, timeout=15)

            if response.status_code == 200:
                # è§£æ RSS
                tweets = parse_nitter_rss(response.text, username, count)
                if tweets:
                    print(f"   [Nitter] Got {len(tweets)} tweets from {instance}")
                    return tweets
        except Exception as e:
            print(f"   [Nitter] {instance} failed: {e}")
            continue

    return tweets


def parse_nitter_rss(xml_content: str, username: str, count: int = 20) -> List[Dict]:
    """è§£æ Nitter RSS å†…å®¹"""
    import re

    tweets = []

    if not HAS_BS4:
        print("   [Nitter] BeautifulSoup not available")
        return tweets

    soup = BeautifulSoup(xml_content, 'xml')
    items = soup.find_all('item')[:count]

    for item in items:
        try:
            # æå–é“¾æ¥å’Œæ¨æ–‡ ID
            link = item.find('link')
            if not link:
                continue
            link_text = link.get_text()

            # ä»é“¾æ¥æå– tweet_id
            # æ ¼å¼: https://nitter.xxx/username/status/123456789#m
            match = re.search(r'/status/(\d+)', link_text)
            if not match:
                continue
            tweet_id = match.group(1)

            # æå–æ­£æ–‡
            description = item.find('description')
            text = ""
            images = []

            if description:
                desc_html = description.get_text()
                desc_soup = BeautifulSoup(desc_html, 'html.parser')

                # æå–æ–‡æœ¬ (å»é™¤å›¾ç‰‡æè¿°)
                for img in desc_soup.find_all('img'):
                    img_src = img.get('src', '')
                    if 'pbs.twimg.com' in img_src or 'twimg.com' in img_src:
                        images.append(img_src)
                    img.decompose()

                text = desc_soup.get_text(separator=' ').strip()

            # æå–å‘å¸ƒæ—¶é—´
            pub_date = item.find('pubDate')
            created_at = pub_date.get_text() if pub_date else None

            tweets.append({
                "id": tweet_id,
                "text": text,
                "created_at": created_at,
                "username": username,
                "url": f"https://x.com/{username}/status/{tweet_id}",
                "images": images,
                "likes": 0,  # RSS ä¸æä¾›è¿™äº›æ•°æ®
                "retweets": 0,
                "views": 0,
            })
        except Exception as e:
            continue

    return tweets


def fetch_user_timeline_syndication(username: str, count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ Twitter Syndication API è·å–ç”¨æˆ·æ—¶é—´çº¿
    è¿™æ˜¯ Twitter å®˜æ–¹çš„åµŒå…¥ APIï¼Œä¸éœ€è¦è®¤è¯
    """
    import re

    tweets = []

    # Twitter Syndication Timeline API
    url = f"https://syndication.twitter.com/srv/timeline-profile/screen-name/{username}"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml',
    }

    try:
        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code == 200:
            # ä» JSON æ•°æ®ä¸­æå– tweet IDs
            tweet_ids = re.findall(r'"id_str":"(\d+)"', response.text)

            seen_ids = set()
            for tweet_id in tweet_ids:
                if tweet_id not in seen_ids:
                    seen_ids.add(tweet_id)
                    tweets.append({
                        "id": tweet_id,
                        "username": username,
                        "url": f"https://x.com/{username}/status/{tweet_id}",
                    })
                    if len(tweets) >= count:
                        break

            if tweets:
                print(f"   [Syndication] Got {len(tweets)} tweet IDs")
            else:
                # æ£€æŸ¥æ˜¯å¦è´¦å·ä¸å­˜åœ¨æˆ–è¢«é™åˆ¶
                if "UserUnavailable" in response.text or "This account doesn" in response.text:
                    print(f"   [Syndication] Account unavailable or suspended")
                elif len(response.text) < 1000:
                    print(f"   [Syndication] Empty or minimal response")
        else:
            print(f"   [Syndication] HTTP {response.status_code}")

    except Exception as e:
        print(f"   [Syndication] Error: {e}")

    return tweets


def fetch_user_timeline_rsshub(username: str, count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ RSSHub è·å–ç”¨æˆ·æ—¶é—´çº¿

    Args:
        username: Twitter ç”¨æˆ·å (ä¸å« @)
        count: è·å–æ•°é‡

    Returns:
        æ¨æ–‡åˆ—è¡¨ (åªåŒ…å« tweet_idï¼Œéœ€è¦åç»­è·å–è¯¦æƒ…)
    """
    import re

    tweets = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml',
    }

    for instance in RSSHUB_INSTANCES:
        try:
            # RSSHub Twitter è·¯ç”±: /twitter/user/:id
            rss_url = f"{instance}/twitter/user/{username}"
            response = requests.get(rss_url, headers=headers, timeout=15)

            if response.status_code == 200:
                # è§£æ RSS æå– tweet IDs
                if HAS_BS4:
                    soup = BeautifulSoup(response.text, 'xml')
                    items = soup.find_all('item')[:count]

                    for item in items:
                        link = item.find('link')
                        if link:
                            link_text = link.get_text()
                            # æå– tweet_id
                            match = re.search(r'/status/(\d+)', link_text)
                            if match:
                                tweet_id = match.group(1)
                                tweets.append({
                                    "id": tweet_id,
                                    "username": username,
                                    "url": f"https://x.com/{username}/status/{tweet_id}",
                                })

                if tweets:
                    print(f"   [RSSHub] Got {len(tweets)} tweet IDs from {instance}")
                    return tweets
        except Exception as e:
            print(f"   [RSSHub] {instance} failed: {e}")
            continue

    return tweets


def fetch_tweet_details(tweet_id: str, username: str) -> Optional[Dict]:
    """
    è·å–å•æ¡æ¨æ–‡çš„è¯¦ç»†ä¿¡æ¯ (åŒ…æ‹¬å›¾ç‰‡ã€äº’åŠ¨æ•°æ®)
    ä½¿ç”¨ FxTwitter/VxTwitter API
    """
    try:
        data = fetch_with_fxtwitter(tweet_id, username)
        result = parse_fxtwitter_result(data)
        if result and result.get("text"):
            stats = result.get("stats", {})
            return {
                "id": tweet_id,
                "text": result.get("text", ""),
                "images": result.get("images", []),
                "username": username,
                "url": f"https://x.com/{username}/status/{tweet_id}",
                "likes": stats.get("likes", 0),
                "retweets": stats.get("retweets", 0),
                "views": stats.get("views", 0),
                "created_at": result.get("created_at"),
            }
    except Exception:
        pass

    try:
        data = fetch_with_vxtwitter(tweet_id, username)
        result = parse_vxtwitter_result(data)
        if result and result.get("text"):
            stats = result.get("stats", {})
            return {
                "id": tweet_id,
                "text": result.get("text", ""),
                "images": result.get("images", []),
                "username": username,
                "url": f"https://x.com/{username}/status/{tweet_id}",
                "likes": stats.get("likes", 0),
                "retweets": stats.get("retweets", 0),
                "views": stats.get("views", 0),
                "created_at": result.get("created_at"),
            }
    except Exception:
        pass

    return None


# ========== X/Twitter å®¢æˆ·ç«¯ ==========

class XMonitor:
    """X/Twitter ç›‘æ§å™¨ - ä½¿ç”¨ Syndication API + FxTwitter"""

    def __init__(self, use_guest: bool = False):
        # use_guest å‚æ•°ä¿ç•™ä»¥å…¼å®¹ CLIï¼Œä½†ä¸å†ä½¿ç”¨
        pass

    async def init_client(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯ - ä½¿ç”¨æ— éœ€è®¤è¯çš„ API"""
        print("[X] Using Syndication API + FxTwitter (no auth required)")

    async def get_user_tweets(self, username: str, count: int = 20) -> List[Dict]:
        """è·å–ç”¨æˆ·æœ€æ–°æ¨æ–‡"""
        tweets = []

        # æ–¹æ³• 1: Twitter Syndication API (å®˜æ–¹åµŒå…¥ APIï¼Œæœ€å¯é )
        print(f"   [1] Trying Syndication API...")
        syn_tweets = fetch_user_timeline_syndication(username, count)
        if syn_tweets:
            for st in syn_tweets:
                details = fetch_tweet_details(st["id"], username)
                if details:
                    tweets.append(details)
                if len(tweets) >= count:
                    break
            if tweets:
                return tweets

        # æ–¹æ³• 2: Nitter RSS (å¤‡ç”¨)
        print(f"   [2] Trying Nitter RSS...")
        nitter_tweets = fetch_user_timeline_nitter(username, count)
        if nitter_tweets:
            for nt in nitter_tweets:
                details = fetch_tweet_details(nt["id"], username)
                if details:
                    tweets.append(details)
                else:
                    tweets.append(nt)
                if len(tweets) >= count:
                    break

        return tweets


# ========== ä¸»å¤„ç†é€»è¾‘ ==========

async def process_tweet(db: Database, tweet: Dict, state: Dict,
                        viral_only: bool = False, dry_run: bool = False) -> bool:
    """å¤„ç†å•æ¡æ¨æ–‡

    Args:
        db: æ•°æ®åº“è¿æ¥
        tweet: æ¨æ–‡æ•°æ®
        state: å¤„ç†çŠ¶æ€
        viral_only: æ˜¯å¦åªå¤„ç†çˆ†æ¬¾æ¨æ–‡
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“
    """
    tweet_id = tweet["id"]
    tweet_url = tweet["url"]
    text = tweet["text"]
    images = tweet["images"]
    username = tweet["username"]
    likes = tweet.get("likes", 0)
    retweets = tweet.get("retweets", 0)
    views = tweet.get("views", 0)

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if is_tweet_processed(state, tweet_id):
        return False

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
    if db.prompt_exists(tweet_url):
        mark_tweet_processed(state, tweet_id)
        return False

    # ========== ç¬¬ä¸€é˜¶æ®µ: ç‰¹å¾åŒ¹é…è¿‡æ»¤ ==========
    is_likely, likely_reason = is_likely_prompt_tweet(tweet)
    if not is_likely:
        # é™é»˜è·³è¿‡ä¸å¤ªå¯èƒ½æ˜¯æç¤ºè¯çš„æ¨æ–‡
        mark_tweet_processed(state, tweet_id)
        # è¿”å›ç‰¹æ®Šå€¼è¡¨ç¤ºè¢«ç¬¬ä¸€é˜¶æ®µè¿‡æ»¤
        return "filtered_stage1"

    # çˆ†æ¬¾åˆ¤æ–­
    is_viral, viral_reason = is_viral_tweet(tweet)
    viral_score = get_viral_score(tweet)

    # å¦‚æœåªè¦çˆ†æ¬¾ï¼Œè·³è¿‡éçˆ†æ¬¾
    if viral_only and not is_viral:
        mark_tweet_processed(state, tweet_id)
        return False

    # æ˜¾ç¤ºæ¨æ–‡ä¿¡æ¯ (é€šè¿‡ç¬¬ä¸€é˜¶æ®µç­›é€‰çš„)
    viral_badge = "ğŸ”¥ VIRAL" if is_viral else ""
    print(f"\n   [Tweet] @{username} - {tweet_id} {viral_badge}")
    print(f"   Match: {likely_reason}")
    print(f"   Text: {text[:100]}...")
    print(f"   Stats: â¤ï¸ {likes:,} | ğŸ” {retweets:,} | ğŸ‘ï¸ {views:,} | Score: {viral_score:,}")
    print(f"   Images: {len(images)}")
    if is_viral:
        print(f"   Viral: {viral_reason}")

    # ========== ç¬¬äºŒé˜¶æ®µ: AI æå–å’ŒéªŒè¯ ==========
    try:
        print(f"   [AI] Extracting prompt...")
        extracted_prompt = extract_prompt_with_ai(text, model=AI_MODEL)

        if not extracted_prompt or extracted_prompt == "No prompt found":
            print(f"   [Skip] AI found no prompt")
            mark_tweet_processed(state, tweet_id)
            # è¿”å›ç‰¹æ®Šå€¼è¡¨ç¤ºè¢«ç¬¬äºŒé˜¶æ®µ AI è¿‡æ»¤
            return "filtered_stage2"

        # AI åˆ†ç±»
        print(f"   [AI] Classifying...")
        classification = classify_prompt_with_ai(extracted_prompt, model=AI_MODEL)

        # å‡†å¤‡æ•°æ®
        title = classification.get("title", "").strip()
        if not title or title == "Untitled Prompt":
            title = f"@{username} #{tweet_id[-6:]}"

        category = map_category(classification)
        tags = classification.get("sub_categories", [])
        if not isinstance(tags, list):
            tags = []
        tags = [str(t).strip() for t in tags if t][:5]

        # Dry Run æ¨¡å¼
        if dry_run:
            print(f"   [DRY RUN] Would save: {title}")
            print(f"      Category: {category}")
            print(f"      Tags: {tags}")
            print(f"      Images: {len(images)}")
            print(f"      Prompt: {extracted_prompt[:100]}...")
            mark_tweet_processed(state, tweet_id)
            return True

        # ä¿å­˜åˆ°æ•°æ®åº“
        print(f"   [DB] Saving: {title}")
        prompt_record = db.save_prompt(
            title=title,
            prompt=extracted_prompt,
            category=category,
            tags=tags,
            images=images[:5],
            source_link=tweet_url,
            author=username,
            import_source=f"x-monitor"
        )

        if prompt_record:
            print(f"   [OK] Saved: {title}")
            mark_tweet_processed(state, tweet_id)
            return True
        else:
            print(f"   [Error] Failed to save")
            return False

    except Exception as e:
        print(f"   [Error] {e}")
        mark_tweet_processed(state, tweet_id)
        return False


async def monitor_accounts(
    accounts: List[str],
    tweets_per_account: int = 10,
    viral_only: bool = False,
    dry_run: bool = False
) -> Dict:
    """ç›‘å¬è´¦å·åˆ—è¡¨

    Args:
        accounts: è¦ç›‘å¬çš„è´¦å·åˆ—è¡¨
        tweets_per_account: æ¯ä¸ªè´¦å·è·å–çš„æ¨æ–‡æ•°é‡
        viral_only: æ˜¯å¦åªå¤„ç†çˆ†æ¬¾æ¨æ–‡
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“
    """

    print("=" * 60)
    print("X/Twitter AI Art Monitor")
    print("=" * 60)
    print(f"Accounts: {len(accounts)}")
    print(f"Viral Only: {viral_only}")
    print(f"Dry Run: {dry_run}")
    print(f"AI Model: {AI_MODEL}")
    print("=" * 60)

    # æ£€æŸ¥æ•°æ®åº“
    if not DATABASE_URL:
        print("[Error] DATABASE_URL not set")
        return {"error": "DATABASE_URL not set"}

    # åˆå§‹åŒ–
    db = Database(DATABASE_URL)
    monitor = XMonitor()
    state = load_state()

    stats = {
        "accounts_checked": 0,
        "tweets_found": 0,
        "filtered_stage1": 0,  # ç¬¬ä¸€é˜¶æ®µè¿‡æ»¤ (ç‰¹å¾åŒ¹é…)
        "filtered_stage2": 0,  # ç¬¬äºŒé˜¶æ®µè¿‡æ»¤ (AI æå–å¤±è´¥)
        "prompts_saved": 0,
        "errors": 0,
    }

    try:
        db.connect()
        print("[DB] Connected")

        await monitor.init_client()
        print("[X] Client ready")

        # éå†è´¦å·
        for i, username in enumerate(accounts, 1):
            print(f"\n[{i}/{len(accounts)}] Checking @{username}...")

            try:
                tweets = await monitor.get_user_tweets(username, tweets_per_account)
                stats["accounts_checked"] += 1
                stats["tweets_found"] += len(tweets)

                print(f"   Found {len(tweets)} tweets")

                for tweet in tweets:
                    result = await process_tweet(db, tweet, state, viral_only=viral_only, dry_run=dry_run)
                    if result == "filtered_stage1":
                        stats["filtered_stage1"] += 1
                    elif result == "filtered_stage2":
                        stats["filtered_stage2"] += 1
                    elif result is True:
                        stats["prompts_saved"] += 1

                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(2)

            except Exception as e:
                print(f"   [Error] {e}")
                stats["errors"] += 1

        # æ›´æ–°çŠ¶æ€
        state["last_check"] = datetime.now(timezone.utc).isoformat()
        save_state(state)

    finally:
        db.close()

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    print(f"Accounts checked: {stats['accounts_checked']}")
    print(f"Tweets found: {stats['tweets_found']}")
    print(f"  â”œâ”€ Stage 1 filtered (no keywords): {stats['filtered_stage1']}")
    print(f"  â”œâ”€ Stage 2 filtered (AI no prompt): {stats['filtered_stage2']}")
    print(f"  â””â”€ Prompts saved: {stats['prompts_saved']}")
    print(f"Errors: {stats['errors']}")
    print("=" * 60)

    return stats


async def run_continuous(
    accounts: List[str],
    interval_minutes: int = 30,
    viral_only: bool = False,
    dry_run: bool = False
):
    """æŒç»­ç›‘å¬æ¨¡å¼"""
    print(f"Starting continuous monitor (interval: {interval_minutes} min)")
    print(f"Viral Only: {viral_only}")
    print(f"Dry Run: {dry_run}")
    print("Press Ctrl+C to stop\n")

    while True:
        try:
            await monitor_accounts(accounts, viral_only=viral_only, dry_run=dry_run)

            print(f"\nNext check in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)

        except KeyboardInterrupt:
            print("\nStopped by user")
            break
        except Exception as e:
            print(f"\n[Error] {e}")
            print(f"Retrying in {interval_minutes} minutes...")
            await asyncio.sleep(interval_minutes * 60)


# ========== CLI ==========

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="X/Twitter AI Art Account Monitor (ä½¿ç”¨ Syndication API + FxTwitterï¼Œæ— éœ€è®¤è¯)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # List top authors from database
  python fetch_x_accounts.py --list-authors

  # Run once with default accounts
  python fetch_x_accounts.py

  # Monitor top 20 authors from database
  python fetch_x_accounts.py --top 20

  # Dry run (don't save to database)
  python fetch_x_accounts.py --top 10 --dry-run

  # Continuous monitoring top 30 authors (every 30 min)
  python fetch_x_accounts.py --top 30 --interval 30

  # Only process viral tweets (high engagement)
  python fetch_x_accounts.py --top 30 --viral-only

  # Continuous viral-only monitoring
  python fetch_x_accounts.py --top 30 --interval 30 --viral-only

  # Monitor specific accounts
  python fetch_x_accounts.py --accounts midjourney,openai
        """
    )

    parser.add_argument("--accounts", "-a", type=str,
                        help="Comma-separated account list")
    parser.add_argument("--top", "-t", type=int, default=0,
                        help="Use top N authors from database (e.g., --top 20)")
    parser.add_argument("--list-authors", action="store_true",
                        help="List top authors from database and exit")
    parser.add_argument("--interval", "-i", type=int, default=0,
                        help="Continuous mode interval in minutes (0=run once)")
    parser.add_argument("--count", "-c", type=int, default=10,
                        help="Tweets per account (default: 10)")
    parser.add_argument("--viral-only", "-v", action="store_true",
                        help="Only process viral tweets (likes>=1000, retweets>=500, views>=100k)")
    parser.add_argument("--dry-run", "-d", action="store_true",
                        help="Dry run mode - fetch and process but don't save to database")

    args = parser.parse_args()

    # åˆ—å‡ºé«˜é¢‘ä½œè€…
    if args.list_authors:
        if not DATABASE_URL:
            print("Error: DATABASE_URL not set")
            return
        db = Database(DATABASE_URL)
        db.connect()
        authors = db.get_top_authors(50)
        db.close()
        print("Top 50 Authors (from database):")
        print("=" * 50)
        for i, row in enumerate(authors, 1):
            print(f"{i:3}. @{row['author']:<25} {row['count']:>5} prompts")
        return

    # è§£æè´¦å·åˆ—è¡¨
    if args.accounts:
        accounts = [a.strip() for a in args.accounts.split(",") if a.strip()]
    elif args.top > 0:
        # ä»æ•°æ®åº“è·å–é«˜é¢‘ä½œè€…
        if not DATABASE_URL:
            print("Error: DATABASE_URL not set")
            return
        db = Database(DATABASE_URL)
        db.connect()
        authors = db.get_top_authors(args.top)
        db.close()
        accounts = [row['author'] for row in authors]
        print(f"Using top {len(accounts)} authors from database")
    else:
        accounts = DEFAULT_ACCOUNTS

    # è¿è¡Œ
    if args.interval > 0:
        asyncio.run(run_continuous(
            accounts,
            interval_minutes=args.interval,
            viral_only=args.viral_only,
            dry_run=args.dry_run
        ))
    else:
        asyncio.run(monitor_accounts(
            accounts,
            tweets_per_account=args.count,
            viral_only=args.viral_only,
            dry_run=args.dry_run
        ))


if __name__ == "__main__":
    main()
